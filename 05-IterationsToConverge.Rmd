# Time to complete

In this section, we will consider the Cox's Proportional Hazard model for analyzing the time to converge to a a solution (in number of iterations).

* **RQ4-a:**  What is the average number of function evaluations taken by an algorithm to converge to a solution at a precision of $\epsilon=0.1$, with a budget of 100,000 function evaluations per dimension?
* **RQ4-b:** What is the impact of noise in the number of function evaluations taken by an algorithm to converge to a solution at a precision of $\epsilon=0.1$, with a budget of 100,000 function evaluations per dimension?

## RQ4 Data preparation

We start importing the dataset

```{r}
dataset <- readr::read_csv('./data/statscomp.csv')
```

Filtering the data that we want and applying some transformations. The Event variable will indicate if it was censored or not.
```{r}
d <- dataset %>% 
  dplyr::filter(OptimizationSuccessful==TRUE & MaxFevalPerDimensions==100000 & (Algorithm=="PSO"|Algorithm=="CMAES"|Algorithm=="DifferentialEvolution"|Algorithm=="RandomSearch2")) %>% 
  dplyr::select(Algorithm, CostFunction, Event="SolveAt1e-1", simNumber, Ndimensions, SD, SolvedAtIteration="SolveEarlierAt1e-1") %>% 
  dplyr::mutate(y=SolvedAtIteration/Ndimensions,
                Event=as.integer(Event),
                CostFunctionID=create_index(CostFunction),
                AlgorithmID=create_index(Algorithm)) %>% 
  dplyr::select(Algorithm, AlgorithmID, CostFunction, CostFunctionID, SD, Event, y,-simNumber,-SolvedAtIteration, -Ndimensions)

algorithms<-get_index_names_as_array(d$Algorithm)
bm <- get_index_names_as_array(d$CostFunction)
```

The data should look like this:
```{r}
kable(dplyr::sample_n(d,size=10),"html", booktabs=T, format.args = list(scientific = FALSE), digits = 3) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```


```{r include=F}
saveRDS(dplyr::sample_n(d,size=6), './statscomp-paper/tables/datafortables/timetoconvergedata.RDS')
```

## RQ4 Stan model

The Stan model is specified in the file: `'./stanmodels/timetoconverge.stan'`. Note that at the end of the model we commented the generated quantities. This block generates the predictive posterior y_rep and the log likelihood, log_lik. These values are useful in diagnosing and validating the model but the end file is extremely large (~1Gb for 2000 iterations) and make many of the following calculations slow. If the reader wants to see these values is just to uncomment and run the stan model again. Note also the the predictive posterior calculates for censored and non censored data. We can in r restrict and compare the predictive only to the non censored data or even censor it if the prediction is above the budget.

```{r}
print_stan_code('./stanmodels/timetoconverge.stan')
```

Let's compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations `"./data/timetoconverge-data.RDS"`

Note that stan does not support NA in the data, so we have two options... We either replace NA for a value and add conditionals in the model (note that this value will not be used). Or we separate the data frame in two parts, censored and not not-censored. We will do the first approach replacing the NA by 0.

```{r}
dstan<-d %>% 
  dplyr::mutate(y=replace_na(y,0))

standata <- list(
  N_total=nrow(dstan),
  y = dstan$y,
  event = dstan$Event,
  x_noise = d$SD,
  N_algorithm = length(algorithms),
  algorithm_id = dstan$AlgorithmID,
  N_bm = length(bm),
  bm_id = d$CostFunctionID
  )
saveRDS(standata, file = "./data/timetoconverge-data.RDS")
```

For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately

```{r echo=T, eval=F}
standata<-readRDS("./data/timetoconverge-data.RDS")
timetoconverge_fit <- stan(file = './stanmodels/timetoconverge.stan',
                     data=standata,
                     chains = 4,
                     warmup = 200,
                     iter = 3000)
saveRDS(timetoconverge_fit, file = "./data/timetoconverge-fit.RDS")
```

```{r echo=F, include=F, eval=T}
timetoconverge_fit <-readRDS("./data/timetoconverge-fit.RDS")
```


## RQ4 Diagnosis

```{r}
a_alg <- c("a_alg[1]",
           "a_alg[2]",
           "a_alg[3]",
           "a_alg[4]")

b_noise <- c("b_noise[1]",
           "b_noise[2]",
           "b_noise[3]",
           "b_noise[4]")
rstan::traceplot(timetoconverge_fit, pars=a_alg)
rstan::traceplot(timetoconverge_fit, pars=b_noise)
rstan::traceplot(timetoconverge_fit, pars='s')
```


Another diagnosis is to look at the Rhat. If Rhat is greater than 1.05 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling.
```{r}
kable(summary(timetoconverge_fit)$summary) %>% 
  kable_styling(bootstrap_options = c('striped',"hover", "condensed" ))
```

## RQ4 Results and Plots

### RQ4 Parameters and plots

First lets get the HPDI of every parameter. 

Then we restrict to the algorithms, them to the slopes, then to the parameter s
```{r}
hpdi <- get_HPDI_from_stanfit(timetoconverge_fit)

hpdi_algorithm <- hpdi %>% 
      dplyr::filter(str_detect(Parameter, "a_alg\\[")) %>%
      dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels

hpdi_noise<- hpdi %>% 
      dplyr::filter(str_detect(Parameter, "b_noise\\[")) %>%
      dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels

hpdi_s <- hpdi %>% 
      dplyr::filter(Parameter=='s')


p_alg<-ggplot(data=hpdi_algorithm, aes(x=Parameter))+
  geom_pointrange(aes(
    ymin=HPDI.lower, 
    ymax=HPDI.higher, 
    y=Mean))+
  labs(y="a_alg", x="Algorithm")+
  coord_flip()
p_alg + plot_annotation(title = 'HPDI interval for the algorithms')

p_noise<-ggplot(data=hpdi_noise, aes(x=Parameter))+
  geom_pointrange(aes(
    ymin=HPDI.lower, 
    ymax=HPDI.higher, 
    y=Mean))+
  labs(y="b_noise", x="Algorithm")+
  coord_flip()
p_noise + plot_annotation(title = 'HPDI interval for noise coefficient')

p_s <- ggplot(data=hpdi_s, aes(x=Parameter))+
  geom_pointrange(aes(
    ymin=HPDI.lower, 
    ymax=HPDI.higher, 
    y=Mean))+
  labs(y="Estimate of s", x="Parameter")+
  coord_flip()
p_s + plot_annotation(title = 'HPDI interval std of the benchmarks')
```


```{r echo=F, include=F, eval=T}
#figure for the paper
p<- (p_alg | p_noise | p_s) + plot_annotation(title = 'HPDI interval of the parameters')
save_fig(p, 'timetoconverge.pdf')
```

### Hazard ratio

```{r}
hr_table <- tibble(
  "Algorithms" = algorithms,
  "Baseline HR" = exp(hpdi_algorithm$Mean),
  "Noise HR" = exp(hpdi_noise$Mean))

kable(hr_table, booktabs=T, format.args = list(scientific = FALSE), digits = 3) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

```{r include=F}
saveRDS(hr_table, './statscomp-paper/tables/datafortables/hr_table.RDS')
```



### Iterations to Converge

To obtain the average iteration to converge we first extract samples from the posterior distribution

Looking at the average value of the data regardless of the benchmarks
```{r}
d_no_na <- d %>% drop_na(y)
d_no_na %>% group_by(Algorithm) %>% summarise(Mean=mean(y))
```


```{r}
posterior <- rstan::extract(timetoconverge_fit)
a <- as_tibble(posterior$a_alg)
colnames(a) <- algorithms

lambda <- exp(a)
mu <- 1/lambda


#Creating a HPD table
timetoconverge_table<-as_tibble(HDInterval::hdi(mu,credMass=0.95), rownames = "Metric") %>%
  tibble::add_row(Metric="Mean",CMAES=mean(mu$CMAES), DifferentialEvolution=mean(mu$DifferentialEvolution), PSO=mean(mu$PSO), RandomSearch2=mean(mu$RandomSearch2)) %>% 
  tidyr::pivot_longer(cols=-Metric,names_to = 'Algorithms', values_to='values') %>% 
  tidyr::pivot_wider(names_from =Metric , values_from=values) %>% 
  dplyr::rename(Mean=Mean,
                'HPD low' = lower,
                'HPD high' = upper) %>% 
  dplyr::relocate(Algorithms, Mean)

saveRDS(timetoconverge_table, './statscomp-paper/tables/datafortables/averagetimetoconverge.RDS')
```

### Merging hazards and time to converge table

```{r include=F}

# join with the hazards table
ft <- dplyr::inner_join(timetoconverge_table,hr_table, by='Algorithms')
colnames(ft)<-c("Parameter", "Mean", "HPD low", "HPD high","Baseline", "Noise")

saveRDS(ft, './statscomp-paper/tables/datafortables/averagetimetoconverge_hr_table.RDS')

```

### Parameter table

```{r}
rename_pars <- c(paste(rep('a_',length(algorithms)),algorithms, sep = ""), paste(rep('b_',length(algorithms)),algorithms, sep = ""),'s')

t<-create_table_model(timetoconverge_fit, c(a_alg, b_noise, 's'), rename_pars)

colnames(t)<-c("Parameter", "Mean", "HPD low", "HPD high")
saveRDS(t,'./statscomp-paper/tables/datafortables/timetoconverge-hr-par-table.RDS')
```
