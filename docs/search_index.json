[
["index.html", "Online Appendix for: Statistical Models for the Analysis of Optimization Algorithms with Benchmark Functions Preface Pre-requisites Source code Compiling this document Software environment", " Online Appendix for: Statistical Models for the Analysis of Optimization Algorithms with Benchmark Functions David Issa Mattos, Jan Bosch, Helena Holmström Olsson 06 October, 2020 Preface This document is an online appendix to the paper “Statistical Models for the Analysis of Optimization Algorithms with Benchmark Functions” by David Issa Mattos, Jan Bosch and Helena Holmström Olsson. It shows the step-by-step process to analyze the data, including data preparation, modeling and plotting for all the models described on the paper. Pre-requisites To follow the code, we assume that the reader has some familiarity with the R environment including packages included of the tidyverse, such as dplyr and ggplot2. The code presented is described and fairly commented to help readers follow the modeling process. Other programming languages such as Python with numpy, pandas, matplotlib etc are capable of performing the same steps, but this is out of the scope of this document. For the Bayesian models, we try to minimize dependency on a specific R package such as brms or rstanarm, and therefore we discuss the model in Stan only, since it has bindings for multiple programming languages. The reader familiar with other languages might be interested in adapting these models and plots to the desired language, Source code The full source code is available in the repository https://github.com/davidissamattos/statscomp. The dataset and the final data for each model (after the described data transformation) is also available for download in the ./data folder. The Stan models are available in the ./stanmodels folder. The utils folder contains some helper functions. The environment was defined and based on the renv package. The renv package logs all the packages in the renv.lock file and manages installation for a specific project. For more information see !!!!!!! To replicate this environment, after downloading this repository, type: renv::hydrate() This command will download and install all the the packages use in this work. Note that it will install the packages only for this project. Compiling this document This document was created with the bookdown package. To compile it (and run every command to generate the models, figures and etc. ) type: bookdown::render_book(&#39;index.Rmd&#39;, &#39;all&#39;) or alternatively use the custom function from the utils.R file. This function besides compiling the book generate the tables for the paper. We cannot generate latex tables (with correct labels) while compiling to bookdown_site. So this function takes the saved tables and generate them separately compile_book() Software environment We suggest to use Stan installed in Linux. There appear to be several problems with Stan in Mac Os Catalina. The environment used is defined in the renv.lock file and session information used to compile this document is sessionInfo() R version 4.0.2 (2020-06-22) Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Catalina 10.15.6 Matrix products: default BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 attached base packages: [1] graphics grDevices datasets stats utils methods base other attached packages: [1] gtools_3.8.2 progress_1.2.2 ggthemr_1.1.0 [4] viridis_0.5.1 viridisLite_0.3.0 patchwork_1.0.1 [7] coda_0.19-3 rstan_2.21.2 StanHeaders_2.21.0-6 [10] glue_1.4.2 forcats_0.5.0 stringr_1.4.0 [13] dplyr_1.0.2 purrr_0.3.4 readr_1.3.1 [16] tidyr_1.1.2 tibble_3.0.3 ggplot2_3.3.2 [19] tidyverse_1.3.0 kableExtra_1.2.1 rmdformats_0.3.7 [22] knitr_1.29 loaded via a namespace (and not attached): [1] matrixStats_0.56.0 fs_1.5.0 lubridate_1.7.9 webshot_0.5.2 [5] httr_1.4.2 tools_4.0.2 backports_1.1.10 R6_2.4.1 [9] DBI_1.1.0 colorspace_1.4-1 withr_2.2.0 tidyselect_1.1.0 [13] gridExtra_2.3 prettyunits_1.1.1 processx_3.4.4 curl_4.3 [17] compiler_4.0.2 cli_2.0.2 rvest_0.3.6 xml2_1.3.2 [21] labeling_0.3 bookdown_0.20 scales_1.1.1 callr_3.4.4 [25] digest_0.6.25 rmarkdown_2.3 pkgconfig_2.0.3 htmltools_0.5.0 [29] highr_0.8 dbplyr_1.4.4 rlang_0.4.7 readxl_1.3.1 [33] rstudioapi_0.11 farver_2.0.3 generics_0.0.2 jsonlite_1.7.1 [37] inline_0.3.16 magrittr_1.5 loo_2.3.1 Rcpp_1.0.5 [41] munsell_0.5.0 fansi_0.4.1 lifecycle_0.2.0 stringi_1.5.3 [45] yaml_2.2.1 pkgbuild_1.1.0 plyr_1.8.6 grid_4.0.2 [49] blob_1.2.1 parallel_4.0.2 crayon_1.3.4 lattice_0.20-41 [53] haven_2.3.1 hms_0.5.3 ps_1.3.4 pillar_1.4.6 [57] codetools_0.2-16 stats4_4.0.2 reprex_0.3.0 evaluate_0.14 [61] V8_3.2.0 renv_0.10.0 RcppParallel_5.0.2 modelr_0.1.8 [65] vctrs_0.3.4 cellranger_1.1.0 gtable_0.3.0 assertthat_0.2.1 [69] xfun_0.17 broom_0.7.0 ellipsis_0.3.1 "],
["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction This document is based on a single dataset available at ./data/statscomp.csv. With this dataset we ask different research questions that motivates the statistical models developed on the paper. Explanations about the models and how the data was obtained are available in the paper. 1.0.1 Exploring the dataset This dataset follows the principle of tidy data as described in https://r4ds.had.co.nz/tidy-data.html. The key idea is that every variables has its own column, and every observation has its own unique row. Throughout this document, to facilitate our modeling approach, we will modify this dataset in different ways, often resulting in non-tidy data. However every model will start from the same base tidy dataset. This approach will hopefully make it easier for the reader to understand from where we are starting and adopt similar strategies in their own models. Additionally, we recommend, if the reader has the opportunity to influence the data collection process, the choice of tidy data. It is often ideal for exploratory analysis, plotting, is the basis for most models, and easy to transform to be used in different models. d &lt;- readr::read_csv(&#39;./data/statscomp.csv&#39;) Here we are excluding a few columns to simplify our view kable(head(dplyr::select(d, -BestArm, -Continuous, -Differentiability, -Separability, -Scalability, -Modality,-BBOB,-BaseClass, -MaxFeval, -FevalPerDimensions), n=10)) Algorithm CostFunction NumberFunctionEval EuclideanDistance TrueRewardDifference CumulativeRegret TimeToComplete Ndimensions OptimizationSuccessful SD MaxFevalPerDimensions SolveAt1 SolveAt1e-1 SolveAt1e-3 SolveAt1e-6 SolveEarlierAt1 SolveEarlierAt1e-1 SolveEarlierAt1e-3 SolveEarlierAt1e-6 simNumber NelderMead BentCigarN6 600 8.9123708 7.364249e+07 4.926652e+10 0.0300207 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 PSO BentCigarN6 600 0.5605997 1.559497e+05 7.938082e+10 0.0394440 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 SimulatedAnnealing BentCigarN6 600 9.7499527 1.086834e+08 1.208830e+12 0.0424774 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 CuckooSearch BentCigarN6 600 8.0025211 1.200314e+07 1.017438e+13 0.0317579 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 DifferentialEvolution BentCigarN6 600 5.3888603 4.634518e+06 2.399718e+12 0.1168543 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 RandomSearch1 BentCigarN6 600 1.5702536 1.919896e+06 1.983989e+13 0.0399160 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 RandomSearch2 BentCigarN6 599 1.5702536 1.655484e+06 1.929796e+13 0.0356977 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 CMAES BentCigarN6 604 0.5744144 3.357865e-01 2.589247e+08 0.1810286 6 TRUE 0 100 TRUE FALSE FALSE FALSE 543 NA NA NA 0 NelderMead BentCigarN6 600 7.9123493 2.152945e+07 2.041479e+11 0.0423668 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 1 PSO BentCigarN6 600 1.0818350 1.853063e+05 7.224343e+10 0.0397996 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 1 1.0.2 Column definitions of the dataset Algorithms: string Algorithm used in the optimization CostFunction: string Specific cost function used. If the cost function can be instantiated in more than one dimension this name also includes the number of dimensions, e.g. SphereN10 is has the base class Sphere and the N=10 for dimensions. BestArm: string represents the xalgo obtained at the end of the optimization NumberFunctionEval: numeric number of times the functon was evaluated in total EuclideanDistance: numeric ||xalgo - xoptimal||2 TrueRewardDifference: numeric falgo - foptimal CumulativeRegret: numeric total regret TimeToComplete: numeric time taken to complete the optimization Continuous: string function properties from the Jamil and Yang survey 2013 Differentiability: string function properties from the Jamil and Yang survey 2013 Separability: string function properties from the Jamil and Yang survey 2013 Scalability: string function properties from the Jamil and Yang survey 2013 Modality: string function properties from the Jamil and Yang survey 2013 Ndimension: numeric number of dimensions OptimizationSuccessful\" BBOB: boolean is part of the BBOB 2009 functions? BaseClass: string the benchmark function used. E.g. SphereN10 has the base class Sphere SD: numeric gaussian noise added to the benchmark function MaxFeval: numeric maximum number of function evaluations in total MaxFevalPerDimensions: numeric maximum number of function evaluations allowed per dimensions FevalPerDimensions: numeric number of times the benchmark function was evaluated per dimensions (some algorithms might evaluate a bit less than the maximum) SolveAt1: boolean was the problem solved at precision 1 SolveAt1e-1\" boolean was the problem solved at precision 1e-1 SolveAt1e-3\" boolean was the problem solved at precision 1e-3 SolveAt1e-6: boolean was the problem solved at precision 1e-6 SolveEarlierAt1: numeric iteration number where converged to the result at precision 1 SolveEarlierAt1e-1: numeric iteration number where converged to the result at precision 1e-1 SolveEarlierAt1e-3: numeric iteration number where converged to the result at precision 1e-3 SolveEarlierAt1e-6: numeric iteration number where converged to the result at precision 1e-6 simNumber: numeric number of the repeated measures, in the dataset, every algorithm was evaluated 10 times in each benchmark function in each condition, in this case the number goes from 0 to 9 "],
["probability-of-success-model.html", "Chapter 2 Probability of success model 2.1 RQ1 Data preparation 2.2 RQ1 Stan model 2.3 RQ1 Diagnosis 2.4 RQ1 Results and Plots", " Chapter 2 Probability of success model Our first model can be used to address problems such as: RQ1-a: What is the probability of each algorithm solving a problem at precision \\(\\epsilon \\leq 0.1\\)? RQ1-b: What is the impact of noise in the probability of success of each algorithm at precision \\(\\epsilon \\leq 0.1\\)? 2.1 RQ1 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&#39;./data/statscomp.csv&#39;) Let’s select only the columns that interests us. Note we use \"\" to select some of the columns because they have “-” in the column name dataset&lt;-dplyr::select(dataset, Algorithm, CostFunction, SD, MaxFevalPerDimensions, simNumber, SolveAt1, &quot;SolveAt1e-1&quot;,&quot;SolveAt1e-3&quot;,&quot;SolveAt1e-6&quot;, OptimizationSuccessful) Let’s do some basic transformation 1 - We select only the cases where the optimization completed 2 - We convert True to 1 and 0 to false 3 - We group by the algorithms, functions, SD, and budget so we can summarize and create aggregated data 4 - We create an index of each algorithm and the cost functions. This is basically creating a map of NelderMead=1, PSO=2 etc… This makes things easier to work in Stan. For that we use the function create_index from the utils.R file 5 - We drop the columns we wont use 6 - Get an array with the names of the benchmark functions and the algorithms (to create nicer plots later with lengend) Since we are only looking at 1e-1 for the precision we comment the other lines d &lt;- dataset %&gt;% dplyr::filter(OptimizationSuccessful==TRUE) %&gt;% dplyr::mutate( solvedAt1e1=as.integer(dataset$&quot;SolveAt1e-1&quot;), budget=MaxFevalPerDimensions) %&gt;% dplyr::group_by(Algorithm, CostFunction, SD, budget) %&gt;% dplyr::summarize( solvedAt1e1=sum(solvedAt1e1), N=n()) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(AlgorithmID=create_index(Algorithm), CostFunctionID=create_index(CostFunction)) %&gt;% dplyr::select(Algorithm,AlgorithmID, CostFunction, CostFunctionID, SD, budget, N, y=solvedAt1e1, ) #List of algorithms bm &lt;- get_index_names_as_array(d$CostFunction) algorithms &lt;- get_index_names_as_array(d$Algorithm) Lets preview a sample of the data set kable(dplyr::sample_n(d,size=10),&quot;html&quot;, booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithm AlgorithmID CostFunction CostFunctionID SD budget N y RandomSearch1 6 ChenBird 2 3 10000 10 0 CMAES 1 BentCigarN6 1 3 10000 10 0 PSO 5 SphereN6 22 0 20 10 0 CMAES 1 ExponentialN2 7 3 100000 10 4 SimulatedAnnealing 8 ThreeHumpCamelBack 24 3 100 10 0 PSO 5 LunacekBiRastriginN6 9 0 10000 10 0 CMAES 1 QingN2 13 3 1000 10 1 NelderMead 4 Trefethen 25 0 10000 10 0 DifferentialEvolution 3 StrechedVSineWave2N 23 3 20 10 0 RandomSearch2 7 Schwefel2d4N6 20 0 10000 10 0 2.2 RQ1 Stan model The Stan model is specified in the file: './stanmodels/probsuccess.stan'. Note that at the end of the model we commented the generated quantities. This block generates the predictive posterior y_rep and the log likelihood, log_lik. These values are useful in diagnosing and validating the model but the end file is extremely large (~1Gb for 2000 iterations) and make many of the following calculations slow. If the reader wants to see these values is just to uncomment and run the stan model again print_stan_code(&#39;./stanmodels/probsuccess.stan&#39;) // Probability of success model // Author: David Issa Mattos // Date: 16 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size int y[N_total]; // Result of the binomial int N_draw[N_total]; // Number of draws in the binomial real x_noise[N_total];//predictor for noise //To model each algorithm independently int &lt;lower=1&gt; N_algorithm; // Number of algorithms int algorithm_id[N_total]; //vector that has the id of each algorithm //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { //Fixed effect real a_alg[N_algorithm];//the mean effect given by the algorithms real b_noise[N_algorithm];//slope for the noise // //Random effect. The effect of the benchmarks real a_bm_norm[N_bm];//the mean effect given by the base class type real&lt;lower=0&gt; s;//std for the random effects } model { real p[N_total]; //Fixed effect a_alg ~ normal(0,5); b_noise ~ normal(0,5); // //Random effects s ~ exponential(0.1); a_bm_norm ~ normal(0,1); for (i in 1:N_total) { p[i] = a_alg[algorithm_id[i]]+ a_bm_norm[bm_id[i]]*s + b_noise[algorithm_id[i]] * x_noise[i]; } //Equivalent to: y~binomial(N, inverse_logit(a+bx=alpha)) y ~ binomial_logit(N_draw,p); } //Uncoment this part to get the posterior predictives and the log likelihood //But note that it takes a lot of space in the final model // generated quantities{ // vector [N_total] y_rep; // vector[N_total] log_lik; // // for(i in 1:N_total){ // real p; // p = a_alg[algorithm_id[i]]+ a_bm_norm[bm_id[i]]*s + b_noise[algorithm_id[i]] * x_noise[i]; // // y_rep[i] = binomial_rng(N_draw[i], inv_logit(p)); // // //Log likelihood // log_lik[i] = binomial_lpmf(y[i] | N_draw[i], inv_logit(p)); // } // } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/probsuccsess-data.RDS\" standata &lt;- list( N_total = nrow(d), y = d$y, N_draw = d$N, x_noise = d$SD, N_algorithm = length(algorithms), algorithm_id =d$AlgorithmID, N_bm = length(bm), bm_id = d$CostFunctionID) saveRDS(standata, file = &quot;./data/probsuccsess-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object in the data folder. However, when we change our model or the data we can just run this chunk separately. standata&lt;-readRDS(&quot;./data/probsuccsess-data.RDS&quot;) probsuccess.fit &lt;- stan(file = &#39;./stanmodels/probsuccess.stan&#39;, data=standata, chains = 4, warmup = 200, iter = 3000) saveRDS(probsuccess.fit, file = &quot;./data/probsuccsess-fit.RDS&quot;) 2.3 RQ1 Diagnosis 2.3.1 RQ1 Chains convergence The first step is to evaluate the convergence of the chains. We will look now only for the slopes, algorithms intercept and the standard deviation of the random effects (and not each intercept of the random effects) a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;, &quot;a_alg[5]&quot;, &quot;a_alg[6]&quot;, &quot;a_alg[7]&quot;, &quot;a_alg[8]&quot;) b_noise &lt;- c(&quot;b_noise[1]&quot;, &quot;b_noise[2]&quot;, &quot;b_noise[3]&quot;, &quot;b_noise[4]&quot;, &quot;b_noise[5]&quot;, &quot;b_noise[6]&quot;, &quot;b_noise[7]&quot;, &quot;b_noise[8]&quot;) rstan::traceplot(probsuccess.fit, pars=a_alg) rstan::traceplot(probsuccess.fit, pars=b_noise) rstan::traceplot(probsuccess.fit, pars=c(&#39;s&#39;)) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.05 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(probsuccess.fit)$summary) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] -0.1016770 0.0141859 0.4511590 -0.9986581 -0.3944800 -0.1079857 0.1956272 0.7748758 1011.456 1.0007354 a_alg[2] -2.5516066 0.0142459 0.4563417 -3.4560617 -2.8523292 -2.5527132 -2.2471997 -1.6730424 1026.124 1.0009415 a_alg[3] -0.2139745 0.0142481 0.4513760 -1.1136926 -0.5092907 -0.2159365 0.0889257 0.6489137 1003.611 1.0007939 a_alg[4] -4.3498335 0.0142878 0.4667423 -5.2926288 -4.6576410 -4.3486295 -4.0362012 -3.4514857 1067.149 1.0007534 a_alg[5] -0.3943603 0.0142332 0.4506022 -1.2874160 -0.6869869 -0.3994464 -0.0884889 0.4724644 1002.264 1.0009414 a_alg[6] -2.0070362 0.0142076 0.4539332 -2.9122092 -2.3013030 -2.0129352 -1.7030975 -1.1352872 1020.806 1.0008781 a_alg[7] -2.2163631 0.0142042 0.4537016 -3.1183583 -2.5133972 -2.2204326 -1.9159819 -1.3441906 1020.248 1.0009555 a_alg[8] -2.5625822 0.0142418 0.4562100 -3.4753740 -2.8617582 -2.5668217 -2.2559160 -1.6908949 1026.123 1.0008100 b_noise[1] -1.2728595 0.0004634 0.0464911 -1.3649835 -1.3042495 -1.2727833 -1.2419386 -1.1808717 10064.607 1.0003304 b_noise[2] -0.8111614 0.0005341 0.0594260 -0.9269817 -0.8512298 -0.8108461 -0.7706404 -0.6960778 12378.857 0.9998278 b_noise[3] -1.3531582 0.0004555 0.0498441 -1.4530193 -1.3862144 -1.3528896 -1.3200088 -1.2545062 11973.466 1.0003042 b_noise[4] -0.3851807 0.0006594 0.0708802 -0.5254143 -0.4334606 -0.3844210 -0.3357693 -0.2486244 11553.895 0.9998575 b_noise[5] -1.1483855 0.0004288 0.0471090 -1.2413071 -1.1803177 -1.1475632 -1.1166922 -1.0569722 12068.645 0.9999617 b_noise[6] -0.9653479 0.0005154 0.0567297 -1.0794613 -1.0029451 -0.9644473 -0.9267742 -0.8573317 12113.240 0.9999536 b_noise[7] -0.7231598 0.0005147 0.0521852 -0.8278536 -0.7579466 -0.7223159 -0.6875584 -0.6217345 10280.685 0.9998671 b_noise[8] -0.7934309 0.0006002 0.0579600 -0.9074668 -0.8320577 -0.7937229 -0.7548121 -0.6780034 9326.707 1.0000572 a_bm_norm[1] -0.5515980 0.0059809 0.2113896 -0.9712815 -0.6922152 -0.5481803 -0.4065674 -0.1432695 1249.219 1.0005773 a_bm_norm[2] -1.3857790 0.0072688 0.3110012 -2.0315335 -1.5876596 -1.3696647 -1.1724448 -0.8177882 1830.642 1.0004815 a_bm_norm[3] -0.6135924 0.0059932 0.2138498 -1.0370821 -0.7578332 -0.6099309 -0.4669450 -0.1978411 1273.226 1.0007982 a_bm_norm[4] 0.6996375 0.0065183 0.2143535 0.2882285 0.5533039 0.6956576 0.8438217 1.1273589 1081.413 1.0011373 a_bm_norm[5] -0.9190532 0.0063871 0.2451850 -1.4221268 -1.0826093 -0.9142283 -0.7500079 -0.4543208 1473.602 1.0004336 a_bm_norm[6] -0.1266989 0.0057661 0.1905645 -0.5002489 -0.2576990 -0.1251113 0.0010672 0.2486333 1092.226 1.0005111 a_bm_norm[7] 2.0929329 0.0098459 0.3562187 1.4022113 1.8483216 2.0869635 2.3308668 2.8104401 1308.946 1.0019514 a_bm_norm[8] 1.4163381 0.0079819 0.2778883 0.8720145 1.2260972 1.4104389 1.6037712 1.9676857 1212.061 1.0016224 a_bm_norm[9] -2.2875170 0.0080868 0.5070058 -3.3784938 -2.6042096 -2.2496205 -1.9305845 -1.4020841 3930.757 1.0003584 a_bm_norm[10] 0.0376740 0.0057984 0.1890303 -0.3267560 -0.0909227 0.0381280 0.1632858 0.4111242 1062.770 1.0008355 a_bm_norm[11] -0.4683589 0.0058892 0.2054966 -0.8814239 -0.6047223 -0.4628744 -0.3299558 -0.0697175 1217.572 1.0004397 a_bm_norm[12] 0.9705429 0.0072309 0.2352270 0.5211149 0.8117004 0.9658386 1.1296289 1.4391891 1058.256 1.0014069 a_bm_norm[13] 0.1111258 0.0058348 0.1895685 -0.2518913 -0.0206515 0.1106819 0.2360902 0.4897085 1055.563 1.0006293 a_bm_norm[14] -0.4185043 0.0058612 0.2020018 -0.8150537 -0.5543865 -0.4170919 -0.2820809 -0.0200522 1187.764 1.0004502 a_bm_norm[15] -0.2120039 0.0057779 0.1931704 -0.5875451 -0.3435861 -0.2108279 -0.0835340 0.1704687 1117.724 1.0004975 a_bm_norm[16] 0.0582553 0.0057948 0.1899707 -0.3051919 -0.0713240 0.0577533 0.1855058 0.4365644 1074.708 1.0007318 a_bm_norm[17] -0.0945615 0.0058098 0.1905650 -0.4616333 -0.2244295 -0.0938981 0.0338356 0.2816415 1075.888 1.0006509 a_bm_norm[18] 0.3553886 0.0060492 0.1962915 -0.0150112 0.2217934 0.3541406 0.4875904 0.7474879 1052.943 1.0007216 a_bm_norm[19] -1.3873544 0.0071574 0.3144653 -2.0341171 -1.5904221 -1.3696656 -1.1671590 -0.8205984 1930.342 1.0006691 a_bm_norm[20] -0.2103035 0.0057874 0.1925856 -0.5825628 -0.3399080 -0.2079499 -0.0814456 0.1685257 1107.335 1.0007190 a_bm_norm[21] 0.0374586 0.0057911 0.1888960 -0.3244013 -0.0918735 0.0378206 0.1633785 0.4096106 1063.953 1.0007178 a_bm_norm[22] 0.4262087 0.0061486 0.1989057 0.0477541 0.2899258 0.4231280 0.5578995 0.8254036 1046.516 1.0011291 a_bm_norm[23] -0.2208615 0.0057839 0.1927261 -0.5929206 -0.3516246 -0.2213745 -0.0911591 0.1589538 1110.306 1.0006586 a_bm_norm[24] 0.9181966 0.0069786 0.2314096 0.4725721 0.7597149 0.9153235 1.0732483 1.3825348 1099.586 1.0014348 a_bm_norm[25] -0.4800747 0.0058778 0.2060577 -0.8903264 -0.6193752 -0.4793265 -0.3426119 -0.0764864 1228.989 1.0007175 a_bm_norm[26] 1.8127547 0.0090502 0.3223532 1.1907146 1.5925470 1.8072705 2.0297038 2.4651831 1268.663 1.0017888 a_bm_norm[27] -0.3950165 0.0058627 0.2010451 -0.7896719 -0.5304012 -0.3915179 -0.2564086 -0.0006536 1175.971 1.0004427 a_bm_norm[28] -1.5823227 0.0078727 0.3510577 -2.3267529 -1.8102101 -1.5614566 -1.3363711 -0.9473444 1988.408 1.0008761 a_bm_norm[29] 0.4734666 0.0062045 0.2008774 0.0930725 0.3343172 0.4734256 0.6092113 0.8729607 1048.224 1.0011553 a_bm_norm[30] 0.7791124 0.0067026 0.2195527 0.3600034 0.6284312 0.7741367 0.9296758 1.2156303 1072.990 1.0013464 s 2.4396697 0.0091963 0.3666749 1.8524855 2.1797734 2.3975453 2.6487166 3.2807386 1589.761 1.0026073 lp__ -5600.1110570 0.1389985 6.2045095 -5613.2707018 -5604.0278224 -5599.7723066 -5595.7819986 -5588.9259626 1992.483 1.0021429 2.4 RQ1 Results and Plots First lets get the HPDI of every parameter. We do this with the helper function from utils.R. But the function is quite simple. It just converts the stanmodel object to an object that the coda package can read (and do some renaming). Alternatively we can use the HDInterval package. Then we restrict to the algorithms, them to the slopes, then to the other parameters. We create different data frames that we use to plot with ggplot pointrange hpdi &lt;- get_HPDI_from_stanfit(probsuccess.fit) hpdi_oddsratio &lt;- hpdi hpdi_oddsratio$Mean &lt;- exp(hpdi$Mean) hpdi_oddsratio$HPDI.lower &lt;- exp(hpdi$HPDI.lower) hpdi_oddsratio$HPDI.higher &lt;- exp(hpdi$HPDI.higher) hpdi_oddsratio_algorithm &lt;- hpdi_oddsratio %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_oddsratio_b_noise &lt;- hpdi_oddsratio %&gt;% dplyr::filter(str_detect(Parameter, &quot;b_noise\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_s &lt;- hpdi %&gt;% dplyr::filter(Parameter==&#39;s&#39;) p_alg&lt;-ggplot(data=hpdi_oddsratio_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Odds ratio for intercept&quot;, x=&quot;Algorithm&quot;)+ coord_flip() p_alg + plot_annotation(title = &#39;HPDI interval for the algorithms odd ratio&#39;) p_noise &lt;- ggplot(data=hpdi_oddsratio_b_noise, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs( y=&quot;Odds ratio for b_noise&quot;, x=&quot;&quot;)+ coord_flip()+ theme(axis.text.y=element_blank()) p_noise + plot_annotation(title = &#39;HPDI interval for the noise coefficients odd ratio&#39;) p_s &lt;- ggplot(data=hpdi_s, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;s&quot;, x=&quot;Parameter&quot;)+ coord_flip() p_s + plot_annotation(title = &#39;HPDI interval for s&#39;) Creating an output table algreduced &lt;- c(&quot;CMAES&quot;, &quot;Cuckoo&quot;, &quot;DiffEvol.&quot;, &quot;NelderM.&quot;, &quot;PSO&quot;, &quot;RandomS1&quot;,&quot;RandomS2&quot;, &quot;SimAnneal&quot;) rename_pars &lt;- c( paste(rep(&#39;a_&#39;,length(algorithms)),algreduced, sep = &quot;&quot;), paste(rep(&#39;b_&#39;,length(algorithms)),algreduced, sep = &quot;&quot;), &#39;s&#39;) t&lt;-create_table_model(probsuccess.fit, pars = c(a_alg, b_noise, &#39;s&#39;), renamepars = rename_pars) t&lt;- t %&gt;% mutate(&#39;OR Mean&#39; = exp(Mean), &#39;OR HPD low&#39; = exp(HPDI.lower), &#39;OR HPD high&#39; = exp(HPDI.higher)) colnames(t)&lt;-c(&quot;Parameter&quot;, &quot;Mean&quot;, &quot;HPD low&quot;, &quot;HPD high&quot;,&#39;OR Mean&#39;,&#39;OR HPD low&#39;,&#39;OR HPD high&#39;) saveRDS(t,&#39;./statscomp-paper/tables/datafortables/probsuccess-par-table.RDS&#39;) "],
["relative-improvement.html", "Chapter 3 Relative improvement 3.1 RQ2 Data preparation 3.2 RQ2 Stan model 3.3 RQ2 Diagnosis 3.4 RQ2 Results and Plots", " Chapter 3 Relative improvement Our next, model deals with relative improvement of the algorithms over a baseline in noiseless functions. This model is based on a normal linear regression. RQ2 What is the expected improvement of these algorithms against the Random Search in noiseless benchmark functions in terms of approaching a global minima based in the Euclidean distance to the location of the closest global minima? 3.1 RQ2 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&#39;./data/statscomp.csv&#39;) Let’s select only the columns that interests us, in this case the Euclidean distance d&lt;- dataset %&gt;% dplyr::select(Algorithm, CostFunction, SD, Budget=MaxFevalPerDimensions, simNumber, EuclideanDistance, OptimizationSuccessful) %&gt;% dplyr::filter(OptimizationSuccessful &amp; SD==0) %&gt;% dplyr::select(-SD, -OptimizationSuccessful) Let’s first make this a wide data set based on the algorithm to make it easier to compute the relative improvement over the Random Search. We are also dropping the RandomSearch2 since there is no noise in the benchmark functions There are several ways that can be used to compute a relative improvement (and they will affect the result). The way we are using is to compare against the mean of distance of the 10 samples of the Random Search in each cost function for a specific budget. The way we are comparing is we divide the distance of each algorithm by the average distance of the random search. If this ratio is greater than 1 then random search is better, if smaller than 1 then the algorithm is better relativeImprovement &lt;- function(x, rs){ #x is the column #rs is the random search column ri &lt;- (rs-x)/rs ri&lt;-ifelse(ri &lt; -1, -1, ri) ri&lt;-ifelse(ri &gt; 1, 1, ri) return(ri) } d_wide &lt;- d %&gt;% tidyr::pivot_wider( names_from = Algorithm, values_from = EuclideanDistance) %&gt;% dplyr::select(-RandomSearch2) %&gt;% dplyr::group_by(CostFunction, Budget) %&gt;% dplyr::mutate(avgRandomSearch=mean(RandomSearch1)) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate_at(c(&quot;NelderMead&quot;, &quot;PSO&quot;, &quot;SimulatedAnnealing&quot;,&quot;CuckooSearch&quot;, &quot;DifferentialEvolution&quot;, &quot;CMAES&quot;), ~relativeImprovement(.x,rs=avgRandomSearch)) After we compute our metric we drop the Random Search column and we pivot_longer again to make the inference d_final &lt;- d_wide %&gt;% dplyr::select(-RandomSearch1, -avgRandomSearch) %&gt;% tidyr::pivot_longer( cols = c(&quot;NelderMead&quot;, &quot;PSO&quot;, &quot;SimulatedAnnealing&quot;,&quot;CuckooSearch&quot;, &quot;DifferentialEvolution&quot;, &quot;CMAES&quot;), names_to = &quot;Algorithm&quot;, values_to = &quot;y&quot;) %&gt;% dplyr::select(-simNumber) %&gt;% dplyr::mutate(AlgorithmID=create_index(Algorithm), CostFunctionID=create_index(CostFunction)) %&gt;% dplyr::select(Algorithm, AlgorithmID, CostFunction, CostFunctionID, Budget, y) #checking if there is any na -&gt; stan does not accept that find.na &lt;- d_final %&gt;% dplyr::filter(is.na(y)) bm&lt;-get_index_names_as_array(d_final$CostFunction) saveRDS(bm, &#39;./data/relativeimprovement_bm.RDS&#39;) algorithms &lt;- get_index_names_as_array(d_final$Algorithm) saveRDS(algorithms, &#39;./data/relativeimprovement_algorithms.RDS&#39;) Now we have our final dataset to use with Stan. Lets preview a sample of the data set kable(dplyr::sample_n(d_final,size=10), &quot;html&quot;,booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithm AlgorithmID CostFunction CostFunctionID Budget y DifferentialEvolution 3 PinterN6 11 1000 0.812 NelderMead 4 ZakharovN2 30 20 -1.000 DifferentialEvolution 3 PinterN6 11 100000 1.000 CuckooSearch 2 Shubert 21 10000 -1.000 PSO 5 Shubert 21 100000 0.943 NelderMead 4 WhitleyN6 28 1000 -1.000 CuckooSearch 2 WhitleyN6 28 20 -0.557 CMAES 1 ChungReynoldsN2 4 20 -1.000 PSO 5 ExponentialN2 7 1000 -0.889 CuckooSearch 2 ChenBird 2 100 0.420 3.2 RQ2 Stan model The Stan model is specified in the file: './stanmodels/relativeimprovement.stan'. Note that at the end of the model we commented the generated quantities. This block generates the predictive posterior y_rep and the log likelihood, log_lik. These values are useful in diagnosing and validating the model but the end file is extremely large (~1Gb for 2000 iterations) and make many of the following calculations slow. If the reader wants to see these values is just to uncomment and run the stan model again print_stan_code(&#39;./stanmodels/relativeimprovement.stan&#39;) // Relative improvement model // Author: David Issa Mattos // Date: 17 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size real y[N_total]; // relative improvement variable //To model each algorithm independently int &lt;lower=1&gt; N_algorithm; // Number of algorithms int algorithm_id[N_total]; //vector that has the id of each algorithm //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { real &lt;lower=0&gt; sigma;//std for the normal //Fixed effect real a_alg[N_algorithm];//the mean effect given by the algorithms // //Random effect. The effect of the benchmarks real a_bm_norm[N_bm];//the mean effect given by the base class type real&lt;lower=0&gt; s;//std for the random effects } model { real mu[N_total]; sigma ~ exponential(1); //Fixed effect a_alg ~ normal(0,1); // //Random effects s ~ exponential(0.1); a_bm_norm ~ normal(0,1); for (i in 1:N_total) { mu[i] = a_alg[algorithm_id[i]] + a_bm_norm[bm_id[i]]*s; } y ~ normal(mu, sigma); } //Uncoment this part to get the posterior predictives and the log likelihood //But note that it takes a lot of space in the final model // generated quantities{ // vector [N_total] y_rep; // vector[N_total] log_lik; // for(i in 1:N_total){ // real mu; // mu = a_alg[algorithm_id[i]] + a_bm_norm[bm_id[i]]*s; // y_rep[i]= normal_rng(mu, sigma); // log_lik[i] = normal_lpdf(y[i] | mu, sigma ); // } // } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/relativeimprovement-data.RDS\" standata &lt;- list( N_total=nrow(d_final), y = d_final$y, N_algorithm = length(algorithms), algorithm_id = d_final$AlgorithmID, N_bm = length(bm), bm_id = d_final$CostFunctionID) saveRDS(standata, file = &quot;./data/relativeimprovement-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately standata&lt;-readRDS(&quot;./data/relativeimprovement-data.RDS&quot;) relativeimprovement.fit &lt;- stan(file = &#39;./stanmodels/relativeimprovement.stan&#39;, data=standata, chains = 4, warmup = 200, iter = 2000) saveRDS(relativeimprovement.fit, file = &quot;./data/relativeimprovement-fit.RDS&quot;) 3.3 RQ2 Diagnosis a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;, &quot;a_alg[5]&quot;, &quot;a_alg[6]&quot;) rstan::traceplot(relativeimprovement.fit, pars=a_alg) rstan::traceplot(relativeimprovement.fit, pars=c(&#39;s&#39;,&#39;sigma&#39;)) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.05 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(relativeimprovement.fit)$summary) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat sigma 0.6366100 0.0000428 0.0046356 0.6279357 0.6334462 0.6365653 0.6396818 0.6457408 11737.9589 0.9998479 a_alg[1] 0.1504431 0.0010669 0.0315679 0.0890274 0.1289946 0.1503498 0.1710892 0.2130964 875.5536 1.0020870 a_alg[2] -0.3782017 0.0010608 0.0312129 -0.4384189 -0.3990024 -0.3785456 -0.3569890 -0.3180648 865.8437 1.0022883 a_alg[3] 0.3016706 0.0010386 0.0318547 0.2394596 0.2803897 0.3016050 0.3229036 0.3637406 940.7541 1.0017231 a_alg[4] -0.6416444 0.0010506 0.0316451 -0.7035318 -0.6628445 -0.6415439 -0.6206061 -0.5804023 907.2309 1.0016704 a_alg[5] 0.3197972 0.0010933 0.0316366 0.2567743 0.2984940 0.3203481 0.3410073 0.3804611 837.4044 1.0015712 a_alg[6] -0.5712294 0.0010744 0.0318686 -0.6329672 -0.5926750 -0.5713187 -0.5504153 -0.5080394 879.8954 1.0023406 a_bm_norm[1] -0.5324372 0.0072268 0.3110141 -1.1446108 -0.7392397 -0.5307194 -0.3175314 0.0609326 1852.0990 1.0003273 a_bm_norm[2] -2.1565188 0.0112839 0.4114065 -3.0139485 -2.4225173 -2.1345244 -1.8665108 -1.4204484 1329.3022 1.0009460 a_bm_norm[3] 0.8557922 0.0073074 0.3220833 0.2453745 0.6347019 0.8489550 1.0702666 1.5008964 1942.7468 1.0012470 a_bm_norm[4] 0.1502905 0.0066763 0.3072522 -0.4326836 -0.0593567 0.1441169 0.3552582 0.7645143 2117.9527 1.0002763 a_bm_norm[5] -1.1326181 0.0085411 0.3420580 -1.8346108 -1.3509023 -1.1248777 -0.8996870 -0.4881811 1603.8856 1.0003229 a_bm_norm[6] 0.8558640 0.0072952 0.3236103 0.2370131 0.6320748 0.8505455 1.0739499 1.4909068 1967.7769 1.0011601 a_bm_norm[7] -0.0413356 0.0071274 0.3039265 -0.6351911 -0.2450705 -0.0405354 0.1630906 0.5561242 1818.3338 1.0004550 a_bm_norm[8] 1.1243779 0.0076636 0.3352797 0.4993539 0.8898000 1.1171430 1.3455862 1.8050221 1914.0470 1.0014555 a_bm_norm[9] 0.4787389 0.0070748 0.3140171 -0.1179473 0.2695584 0.4710521 0.6866548 1.1086989 1970.0285 1.0005173 a_bm_norm[10] 0.3282321 0.0071282 0.3096721 -0.2771011 0.1231111 0.3274623 0.5365737 0.9280143 1887.3190 1.0010798 a_bm_norm[11] 0.8736090 0.0080123 0.3259996 0.2503156 0.6479220 0.8653393 1.0896311 1.5221133 1655.4586 1.0009389 a_bm_norm[12] 0.6707052 0.0072055 0.3167687 0.0749724 0.4535551 0.6644335 0.8764376 1.3154337 1932.6408 1.0013575 a_bm_norm[13] -1.7386842 0.0098444 0.3781142 -2.5231052 -1.9816060 -1.7269537 -1.4711017 -1.0392991 1475.2544 1.0006415 a_bm_norm[14] 0.8892515 0.0074731 0.3241643 0.2790440 0.6662089 0.8769090 1.1051927 1.5387143 1881.5965 1.0009292 a_bm_norm[15] -1.0726736 0.0084882 0.3314618 -1.7423574 -1.2824677 -1.0642625 -0.8519186 -0.4486825 1524.8706 1.0007155 a_bm_norm[16] 0.1074685 0.0070940 0.3051650 -0.4907630 -0.0949131 0.1019742 0.3119469 0.7071876 1850.4962 1.0010560 a_bm_norm[17] 0.8224704 0.0072394 0.3262720 0.2010029 0.5977758 0.8164466 1.0357533 1.4659379 2031.1884 1.0005378 a_bm_norm[18] 0.7667547 0.0075036 0.3201146 0.1585328 0.5478409 0.7561602 0.9779430 1.3995464 1819.9792 1.0015047 a_bm_norm[19] -0.5677922 0.0072741 0.3134924 -1.2106486 -0.7726446 -0.5606116 -0.3540789 0.0217070 1857.3671 1.0002155 a_bm_norm[20] 0.0879460 0.0068471 0.3058517 -0.5149716 -0.1169447 0.0908125 0.2943910 0.6794939 1995.2988 1.0007190 a_bm_norm[21] -0.5632527 0.0073789 0.3167070 -1.2067040 -0.7682321 -0.5597471 -0.3499427 0.0516084 1842.1702 1.0005945 a_bm_norm[22] 1.1724562 0.0077659 0.3403050 0.5270203 0.9331018 1.1642364 1.4008483 1.8550582 1920.2270 1.0014713 a_bm_norm[23] -1.2555759 0.0087782 0.3440404 -1.9785188 -1.4762408 -1.2379681 -1.0227795 -0.6103550 1536.0428 1.0003350 a_bm_norm[24] 0.2698611 0.0068884 0.3067795 -0.3418640 0.0682229 0.2669627 0.4770308 0.8782026 1983.4420 1.0004909 a_bm_norm[25] -0.7249007 0.0075140 0.3210720 -1.3799455 -0.9392957 -0.7174527 -0.5078526 -0.1048101 1825.8476 1.0005200 a_bm_norm[26] 1.4013859 0.0081970 0.3558229 0.7488342 1.1587726 1.3878845 1.6302152 2.1367271 1884.3446 1.0012795 a_bm_norm[27] -1.0722893 0.0083682 0.3332402 -1.7443855 -1.2929783 -1.0652907 -0.8431567 -0.4500125 1585.8196 1.0003164 a_bm_norm[28] -0.3734760 0.0069790 0.3079066 -1.0152636 -0.5677904 -0.3693947 -0.1721274 0.2091721 1946.4702 1.0004438 a_bm_norm[29] -0.5220881 0.0076369 0.3137439 -1.1577763 -0.7261937 -0.5122625 -0.3076597 0.0602610 1687.7736 1.0007462 a_bm_norm[30] 1.0909753 0.0079913 0.3376618 0.4643900 0.8590997 1.0825904 1.3123522 1.7709307 1785.3890 1.0013195 s 0.1462747 0.0006600 0.0215428 0.1102501 0.1309906 0.1443111 0.1596464 0.1940269 1065.5445 1.0017016 lp__ -453.8280738 0.1664049 5.9138935 -466.3807087 -457.4939235 -453.3995258 -449.6758589 -443.5918651 1263.0328 1.0020458 3.4 RQ2 Results and Plots First lets get the HPDI of every parameter. Then we restrict to the algorithms, them to the slopes, then to the hpdi &lt;- get_HPDI_from_stanfit(relativeimprovement.fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_other_parameters &lt;- hpdi %&gt;% dplyr::filter(Parameter==&#39;s&#39; | Parameter==&#39;sigma&#39;) p_alg&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate of intercept&quot;, x=&quot;Algorithm&quot;)+ coord_flip() p_alg + plot_annotation(title = &#39;HPDI interval for the algorithms&#39;) p_others &lt;- ggplot(data=hpdi_other_parameters, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate of s and sigma&quot;, x=&quot;Parameter&quot;)+ coord_flip() p_others + plot_annotation(title = &#39;HPDI interval&#39;) Creating an output table rename_pars &lt;- c(paste(rep(&#39;a_&#39;,length(algorithms)),algorithms, sep = &quot;&quot;),&#39;s&#39;,&#39;sigma&#39;) t&lt;-create_table_model(relativeimprovement.fit, c(a_alg, &#39;s&#39;, &#39;sigma&#39;),rename_pars) colnames(t)&lt;-c(&quot;Parameter&quot;, &quot;Mean&quot;, &quot;HPD low&quot;, &quot;HPD high&quot;) saveRDS(t,&#39;./statscomp-paper/tables/datafortables/relativeimprovement-par-table.RDS&#39;) "],
["ranking.html", "Chapter 4 Ranking 4.1 RQ3 Data preparation 4.2 RQ3 Stan model 4.3 RQ3 Diagnosis 4.4 RQ3 Results and Plots", " Chapter 4 Ranking In this section, we will consider the Bradley-Terry Model for ranking algorithms in the fixed budget of 10,000 function evaluations per dimension and controlling for noise and the effect of benchmark functions RQ3: How can we rank algorithm different optimization algorithms given a budget of 10,000 evaluations per dimension in noisy benchmarks? 4.1 RQ3 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&#39;./data/statscomp.csv&#39;) The BT model formulation that we use has a specific data format, where we have one column with algo_0 (with index of each algorithm) another column with algo_1 and a third column with who won (algo 0 or algo 1), First lets select only the data that we are interested and create ranking by the each run in each group (by the simNumber). To avoid ties (dealing with those on next session) we will rank ties randomly d1 &lt;- dataset %&gt;% dplyr::select(Algorithm, CostFunction, SD, Budget=MaxFevalPerDimensions, simNumber, TrueRewardDifference, OptimizationSuccessful) %&gt;% dplyr::filter(OptimizationSuccessful &amp; Budget==10000 &amp; SD==3.0) %&gt;% dplyr::select(-Budget, -OptimizationSuccessful, -SD) %&gt;% dplyr::group_by(CostFunction, simNumber) %&gt;% dplyr::mutate(rankReward=rank(TrueRewardDifference, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-TrueRewardDifference) kable(dplyr::sample_n(d1,size=10), booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithm CostFunction simNumber rankReward NelderMead WhitleyN6 1 8 NelderMead DiscusN2 5 7 DifferentialEvolution LunacekBiRastriginN6 5 3 NelderMead Shubert 9 8 NelderMead Shubert 1 8 PSO Price1 8 3 CMAES DiscusN2 6 6 RandomSearch2 PinterN6 7 5 CuckooSearch Trefethen 5 3 SimulatedAnnealing Schwefel2d26N6 6 8 Now to compare the ranks we need to pivot wider the data frame and based on that we will expand to the dataset in the appropriated format d1_wide &lt;- d1 %&gt;% tidyr::pivot_wider(names_from = Algorithm, values_from=rankReward) kable(dplyr::sample_n(d1_wide,size=10), booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) CostFunction simNumber NelderMead PSO SimulatedAnnealing CuckooSearch DifferentialEvolution RandomSearch1 RandomSearch2 CMAES Tripod 2 8 3 7 5 1 4 2 6 ThreeHumpCamelBack 2 8 5 7 6 2 1 4 3 ZakharovN2 0 8 4 7 5 3 2 1 6 XinSheYang2N2 5 8 4 7 1 5 2 6 3 LunacekBiRastriginN6 0 8 1 5 7 2 6 4 3 DiscusN2 0 7 2 4 6 1 5 3 8 Trigonometric1N6 1 7 2 4 8 5 6 3 1 ChenV 5 7 1 3 4 8 2 6 5 Schwefel2d20N2 8 7 1 8 2 5 4 3 6 Schwefel2d4N6 8 8 3 7 6 2 4 5 1 Now we need to modify this data set and expand it so we have the pairwise comparisons First let’s get the number of algorithms and create combination of all possible 2 by 2 comparisons without repeating algorithms &lt;- get_index_names_as_array(d1$Algorithm) n_algorithms &lt;- length(algorithms) comb &lt;- gtools::combinations(n=n_algorithms, r=2, v=seq(1:n_algorithms), repeats.allowed = F) The pairs combinations looks like this (for algo_0 and algo_1): kable(comb, booktabs=T) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) 1 2 1 3 1 4 1 5 1 6 1 7 1 8 2 3 2 4 2 5 2 6 2 7 2 8 3 4 3 5 3 6 3 7 3 8 4 5 4 6 4 7 4 8 5 6 5 7 5 8 6 7 6 8 7 8 Note that each row of d_wide will be expanded into 28 rows. Giving a dataset with a total of 8400 rows. The following code can a bit slow to run due to the double for loops (there is probably a way to vectorize this and make it run faster), but for building this appendix we will not run, instead we will run it once, save this data, and load it when needed. It takes a couple of minutes but if you have a lot of data and algorithms it can easily go for hours We will use a progress bar to follow the data frame creation. 1- We initialize a tibble data frame 2- First we loop through the wide data frame d1_wide row by row 3- For each row we will loop through the different combinations in the comb variable to create the rows of the data frame. We add each row to the initial dataframe pb &lt;- progress::progress_bar$new(format = &quot;[:bar] :current/:total (:percent)&quot;, total = nrow(d1_wide)) df_out &lt;- dplyr::tribble(~algo0_name, ~algo0, ~algo1_name, ~algo1, ~y, ~simNumber, ~CostFunction) for(i in 1:nrow(d1_wide)) { current_row &lt;- d1_wide[i,] for(j in 1:nrow(comb)){ comb_row &lt;- comb[j,] algo0_name &lt;- algorithms[comb_row[1]] algo0 &lt;- comb_row[1] algo0_rank &lt;- current_row[[1,algo0_name]] algo1_name &lt;- algorithms[comb_row[2]] algo1 &lt;- comb_row[2] algo1_rank &lt;- current_row[[1,algo1_name]] diff_rank &lt;- algo1_rank - algo0_rank y &lt;- ifelse(diff_rank&lt;0, 1, 0) df_out &lt;- add_row(df_out, algo0_name=algo0_name, algo0=algo0, algo1_name=algo1_name, algo1=algo1, y=y, simNumber=current_row$simNumber, CostFunction=current_row$CostFunction) } pb$tick() } saveRDS(df_out, file=&quot;./data/ranking.RDS&quot;) Adding index for the benchmarks df_out$CostFunctionId &lt;- create_index(df_out$CostFunction) benchmarks &lt;- get_index_names_as_array(df_out$CostFunction) Visualizing how the data frame looks like df_out &lt;- readRDS(&quot;./data/ranking.RDS&quot;) kable(dplyr::sample_n(df_out,size=10), &quot;html&quot;, booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) algo0_name algo0 algo1_name algo1 y simNumber CostFunction CuckooSearch 2 RandomSearch2 7 1 5 PinterN6 CMAES 1 RandomSearch2 7 0 1 Schwefel2d26N6 NelderMead 4 RandomSearch2 7 1 5 DiscusN2 CuckooSearch 2 NelderMead 4 0 8 Mishra7N6 NelderMead 4 RandomSearch2 7 1 1 StrechedVSineWave2N RandomSearch2 7 SimulatedAnnealing 8 0 4 Trigonometric1N6 NelderMead 4 SimulatedAnnealing 8 0 7 Schwefel2d23N6 DifferentialEvolution 3 PSO 5 0 8 DiscusN2 CuckooSearch 2 PSO 5 1 6 ChungReynoldsN2 NelderMead 4 RandomSearch2 7 1 1 Schwefel2d20N2 4.2 RQ3 Stan model The Stan model is specified in the file: './stanmodels/rankingmodel_withcluster.stan'. print_stan_code(&#39;./stanmodels/rankingmodel_withcluster.stan&#39;) // Ranking model with cluster data // Author: David Issa Mattos // Date: 1 Oct 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size int y[N_total]; //variable that indicates which one wins algo0 oor algo 1 int &lt;lower=1&gt; N_algorithm; // Number of algorithms int &lt;lower=1&gt; algo0[N_total]; int &lt;lower=1&gt; algo1[N_total]; // //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { real a_alg[N_algorithm]; //Latent variable that represents the strength value of each algorithm real&lt;lower=0&gt; s;//std for the random effects matrix[N_algorithm, N_bm] Uij; //parameters of the random effects for cluster } model { real p[N_total]; a_alg ~ normal(0,2); s ~ exponential(0.1); for (i in 1:N_algorithm) { for(j in 1:N_bm){ Uij[i, j] ~ normal(0, 1); } } for (i in 1:N_total) { p[i] = (a_alg[algo1[i]] + s*Uij[algo1[i], bm_id[i]]) - (a_alg[algo0[i]] + s*Uij[algo0[i], bm_id[i]] ) ; } y ~ bernoulli_logit(p); } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/rankingmodel-withcluster-data.RDS\" For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately standata &lt;- list( N_total=nrow(df_out), y = as.integer(df_out$y), N_algorithm = length(algorithms), algo0=df_out$algo0, algo1=df_out$algo1, bm_id=df_out$CostFunctionId, N_bm=length(benchmarks) ) saveRDS(standata, file = &quot;./data/rankingmodel-withcluster-data.RDS&quot;) standata&lt;-readRDS(&quot;./data/rankingmodel-withcluster-data.RDS&quot;) ranking.fit &lt;- stan(file = &#39;./stanmodels/rankingmodel_withcluster.stan&#39;, data=standata, chains = 4, warmup = 200, iter = 2000) saveRDS(ranking.fit, file = &quot;./data/ranking-with-cluster-fit.RDS&quot;) ranking.fit &lt;-readRDS(&quot;./data/ranking-with-cluster-fit.RDS&quot;) a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;, &quot;a_alg[5]&quot;, &quot;a_alg[6]&quot;, &quot;a_alg[7]&quot;, &quot;a_alg[8]&quot;) 4.3 RQ3 Diagnosis rstan::traceplot(ranking.fit, pars=c(a_alg,&#39;s&#39;)) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.05 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the posteriors. Note that we have several random effects parameter estimates… kable(summary(ranking.fit)$summary) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] 0.3696071 0.0274756 0.7032806 -1.0712309 -0.0930206 0.3837413 0.8380181 1.7819029 655.1837 1.003792 a_alg[2] -0.2156327 0.0274038 0.7025901 -1.6603802 -0.6768489 -0.2051252 0.2527180 1.1842839 657.3298 1.003796 a_alg[3] 0.9697466 0.0274442 0.7026793 -0.4771415 0.5074117 0.9861502 1.4364222 2.3771677 655.5615 1.003786 a_alg[4] -1.5743322 0.0274385 0.7040243 -3.0075916 -2.0374430 -1.5609753 -1.1089352 -0.1566811 658.3457 1.003665 a_alg[5] 0.8663112 0.0274661 0.7031670 -0.5753947 0.4059510 0.8811888 1.3301487 2.2838648 655.4274 1.003823 a_alg[6] 0.2095375 0.0274898 0.7027110 -1.2202323 -0.2522482 0.2229654 0.6764983 1.6093695 653.4481 1.003767 a_alg[7] 0.2327811 0.0274277 0.7031632 -1.2068769 -0.2288522 0.2501319 0.7012086 1.6358498 657.2567 1.003844 a_alg[8] -0.7405542 0.0274459 0.7038703 -2.1842576 -1.2029190 -0.7252634 -0.2739076 0.6674933 657.7034 1.003796 lp__ -4761.6122712 0.0383782 2.0180572 -4766.4097965 -4762.7381715 -4761.2851077 -4760.1370666 -4758.6808222 2765.0207 1.002815 4.4 RQ3 Results and Plots First let’s get the HPDI interval for the “strength” parameters. Then we will sample the posterior and rank them and present the ranks with their respective posteriors. hpdi &lt;- get_HPDI_from_stanfit(ranking.fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels p_alg&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate&quot;, x=&quot;Algorithm&quot;, title = &quot;HPDI interval of the strength of the algorithms&quot;)+ coord_flip() p_alg #+ plot_annotation(title = &#39;HPDI interval for the algorithms strength&#39;) Computing the ranks posterior &lt;- rstan::extract(ranking.fit) a_alg &lt;- as_tibble(posterior$a_alg) colnames(a_alg) &lt;- algorithms #sampling from the posterior s &lt;- dplyr::sample_n(a_alg, size = 1000, replace=T) s &lt;- dplyr::mutate(s, rown = row_number()) wide_s &lt;- tidyr::pivot_longer(s, cols=all_of(algorithms), names_to = &quot;Algorithm&quot;, values_to = &quot;a_alg&quot;) rank_df &lt;- wide_s %&gt;% dplyr::group_by(rown) %&gt;% dplyr::mutate(Rank = rank(-a_alg, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-a_alg) %&gt;% dplyr::group_by(Algorithm) %&gt;% dplyr::summarise(MedianRank = median(Rank), VarianceRank = var(Rank)) %&gt;% dplyr::arrange(MedianRank) Probability of CMAES to beat Random Search and probability of Differential Evolution beating random search inv_logit &lt;- function(x){ y&lt;-exp(x)/(1+exp(x)) return(y) } p_cmaes_beat_rs &lt;- as.data.frame(inv_logit(s$CMAES-s$RandomSearch1)) colnames(p_cmaes_beat_rs) &lt;- c(&#39;x&#39;) quantile(p_cmaes_beat_rs$x, 0.05) 5% 0.5148706 quantile(p_cmaes_beat_rs$x, 0.95) 95% 0.5640326 quantile(p_cmaes_beat_rs$x, 0.5) 50% 0.539845 #raw data draw &lt;- df_out %&gt;% dplyr::filter(algo0_name==&#39;CMAES&#39; &amp; algo1_name==&#39;RandomSearch1&#39;) (nrow(draw)-sum(draw$y))/nrow(draw) #average of the data [1] 0.5833333 # # p_de_beat_rs &lt;- as.data.frame(inv_logit(s$DifferentialEvolution-s$RandomSearch1)) # colnames(p_de_beat_rs) &lt;- c(&#39;x&#39;) # quantile(p_de_beat_rs$x, 0.05) # quantile(p_de_beat_rs$x, 0.95) # quantile(p_de_beat_rs$x, 0.5) we can see that in this case the probability of CMAES beating RS is between 0.50-0.82 with average of 0.67 rank_df_table &lt;- rank_df colnames(rank_df_table) &lt;- c(&quot;Algorithm&quot;,&quot;Median Rank&quot;, &quot;Variance of the Rank&quot;) kable(rank_df_table, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) Algorithm Median Rank Variance of the Rank DifferentialEvolution 1 0.0511351 PSO 2 0.0511351 CMAES 3 0.0215375 RandomSearch2 4 0.2620370 RandomSearch1 5 0.2514825 CuckooSearch 6 0.0000000 SimulatedAnnealing 7 0.0000000 NelderMead 8 0.0000000 a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;, &quot;a_alg[5]&quot;, &quot;a_alg[6]&quot;, &quot;a_alg[7]&quot;, &quot;a_alg[8]&quot;) rename_pars &lt;- c(paste(rep(&#39;a_&#39;,length(algorithms)),algorithms, sep = &quot;&quot;),&#39;s&#39;) t&lt;-create_table_model(ranking.fit, pars = c(a_alg, &#39;s&#39;), renamepars = rename_pars) colnames(t)&lt;-c(&quot;Parameter&quot;, &quot;Mean&quot;, &quot;HPD low&quot;, &quot;HPD high&quot;) saveRDS(t,&#39;./statscomp-paper/tables/datafortables/ranking-par-table.RDS&#39;) "],
["time-to-complete.html", "Chapter 5 Time to complete 5.1 RQ4 Data preparation 5.2 RQ4 Stan model 5.3 RQ4 Diagnosis 5.4 RQ4 Results and Plots", " Chapter 5 Time to complete In this section, we will consider the Cox’s Proportional Hazard model for analyzing the time to converge to a a solution (in number of iterations). RQ4-a: What is the average number of function evaluations taken by an algorithm to converge to a solution at a precision of \\(\\epsilon=0.1\\), with a budget of 100,000 function evaluations per dimension? RQ4-b: What is the impact of noise in the number of function evaluations taken by an algorithm to converge to a solution at a precision of \\(\\epsilon=0.1\\), with a budget of 100,000 function evaluations per dimension? 5.1 RQ4 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&#39;./data/statscomp.csv&#39;) Filtering the data that we want and applying some transformations. The Event variable will indicate if it was censored or not. d &lt;- dataset %&gt;% dplyr::filter(OptimizationSuccessful==TRUE &amp; MaxFevalPerDimensions==100000 &amp; (Algorithm==&quot;PSO&quot;|Algorithm==&quot;CMAES&quot;|Algorithm==&quot;DifferentialEvolution&quot;|Algorithm==&quot;RandomSearch2&quot;)) %&gt;% dplyr::select(Algorithm, CostFunction, Event=&quot;SolveAt1e-1&quot;, simNumber, Ndimensions, SD, SolvedAtIteration=&quot;SolveEarlierAt1e-1&quot;) %&gt;% dplyr::mutate(y=SolvedAtIteration/Ndimensions, Event=as.integer(Event), CostFunctionID=create_index(CostFunction), AlgorithmID=create_index(Algorithm)) %&gt;% dplyr::select(Algorithm, AlgorithmID, CostFunction, CostFunctionID, SD, Event, y,-simNumber,-SolvedAtIteration, -Ndimensions) algorithms&lt;-get_index_names_as_array(d$Algorithm) bm &lt;- get_index_names_as_array(d$CostFunction) The data should look like this: kable(dplyr::sample_n(d,size=10),&quot;html&quot;, booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithm AlgorithmID CostFunction CostFunctionID SD Event y CMAES 1 Trigonometric1N6 26 0 1 0.167 DifferentialEvolution 2 XinSheYang2N2 29 0 1 367.500 DifferentialEvolution 2 SalomonN2 15 3 0 2717.500 DifferentialEvolution 2 Damavandi 5 0 0 NA RandomSearch2 4 ChenV 3 0 0 NA RandomSearch2 4 Shubert 21 0 0 NA CMAES 1 RosenbrockRotatedN6 14 0 1 284.500 CMAES 1 Giunta 8 3 0 3.000 DifferentialEvolution 2 LunacekBiRastriginN6 9 0 0 NA CMAES 1 Shubert 21 3 0 NA 5.2 RQ4 Stan model The Stan model is specified in the file: './stanmodels/timetoconverge.stan'. Note that at the end of the model we commented the generated quantities. This block generates the predictive posterior y_rep and the log likelihood, log_lik. These values are useful in diagnosing and validating the model but the end file is extremely large (~1Gb for 2000 iterations) and make many of the following calculations slow. If the reader wants to see these values is just to uncomment and run the stan model again. Note also the the predictive posterior calculates for censored and non censored data. We can in r restrict and compare the predictive only to the non censored data or even censor it if the prediction is above the budget. print_stan_code(&#39;./stanmodels/timetoconverge.stan&#39;) // Time to converge, Cox regression model // Author: David Issa Mattos // Date: 23 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size real y[N_total]; // iteration where it was solved int event[N_total]; // Indicates if the event occured or not //To model each algorithm independently int &lt;lower=1&gt; N_algorithm; // Number of algorithms int algorithm_id[N_total]; //vector that has the id of each algorithm //To model the influence of the noise real x_noise[N_total]; //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { //Fixed effect real a_alg[N_algorithm];//the mean effect given by the algorithms real b_noise[N_algorithm];//effect of noise // //Random effect. The effect of the benchmarks real a_bm_norm[N_bm];//the mean effect given by the base class type real&lt;lower=0&gt; s;//std for the random effects } model { //Fixed effect a_alg ~ normal(0,10); // //Random effects s ~ exponential(0.1); a_bm_norm ~ normal(0,1); b_noise ~ normal(0,2); for (i in 1:N_total) { //uncensored data if(event[i]==1) target += exponential_lpdf(y[i] | exp(a_alg[algorithm_id[i]] + s*a_bm_norm[bm_id[i]] + b_noise[algorithm_id[i]]*x_noise[i])); //censored data if(event[i]==0) target += exponential_lccdf(y[i] | exp(a_alg[algorithm_id[i]] + s*a_bm_norm[bm_id[i]] + b_noise[algorithm_id[i]]*x_noise[i])); } } //Uncoment this part to get the posterior predictives and the log likelihood //But note that it takes a lot of space in the final model // //Here we suppose that the predictive data will not be censored. // //But if it is above the budget we can censor it later // generated quantities{ // vector [N_total] y_rep; // vector[N_total] log_lik; // // for(i in 1:N_total){ // real mu; // mu = a_alg[algorithm_id[i]] + s*a_bm_norm[bm_id[i]] + b_noise[algorithm_id[i]]*x_noise[i]; // y_rep[i]= exponential_rng(mu); // // //uncensored data // if(event[i]==1) log_lik[i]= exponential_lpdf(y[i] | exp(a_alg[algorithm_id[i]] + s*a_bm_norm[bm_id[i]] + b_noise[algorithm_id[i]]*x_noise[i])); // //censored data // if(event[i]==0) log_lik[i]= exponential_lccdf(y[i] | exp(a_alg[algorithm_id[i]] + s*a_bm_norm[bm_id[i]] + b_noise[algorithm_id[i]]*x_noise[i])); // } // } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/timetoconverge-data.RDS\" Note that stan does not support NA in the data, so we have two options… We either replace NA for a value and add conditionals in the model (note that this value will not be used). Or we separate the data frame in two parts, censored and not not-censored. We will do the first approach replacing the NA by 0. dstan&lt;-d %&gt;% dplyr::mutate(y=replace_na(y,0)) standata &lt;- list( N_total=nrow(dstan), y = dstan$y, event = dstan$Event, x_noise = d$SD, N_algorithm = length(algorithms), algorithm_id = dstan$AlgorithmID, N_bm = length(bm), bm_id = d$CostFunctionID ) saveRDS(standata, file = &quot;./data/timetoconverge-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately standata&lt;-readRDS(&quot;./data/timetoconverge-data.RDS&quot;) timetoconverge_fit &lt;- stan(file = &#39;./stanmodels/timetoconverge.stan&#39;, data=standata, chains = 4, warmup = 200, iter = 3000) saveRDS(timetoconverge_fit, file = &quot;./data/timetoconverge-fit.RDS&quot;) 5.3 RQ4 Diagnosis a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;) b_noise &lt;- c(&quot;b_noise[1]&quot;, &quot;b_noise[2]&quot;, &quot;b_noise[3]&quot;, &quot;b_noise[4]&quot;) rstan::traceplot(timetoconverge_fit, pars=a_alg) rstan::traceplot(timetoconverge_fit, pars=b_noise) rstan::traceplot(timetoconverge_fit, pars=&#39;s&#39;) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.05 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(timetoconverge_fit)$summary) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] -5.0876436 0.0163450 0.4749516 -6.1009042 -5.3879902 -5.0723454 -4.7666194 -4.1983810 844.3647 1.0029185 a_alg[2] -6.3679581 0.0164399 0.4729020 -7.3803595 -6.6721314 -6.3496253 -6.0462865 -5.4849112 827.4495 1.0028510 a_alg[3] -6.3007230 0.0163282 0.4719901 -7.2941107 -6.6024358 -6.2839196 -5.9799940 -5.4202570 835.5808 1.0028738 a_alg[4] -8.9508434 0.0163599 0.4804615 -9.9725014 -9.2629771 -8.9396770 -8.6226019 -8.0575120 862.4938 1.0026451 b_noise[1] -0.7873706 0.0007192 0.0690586 -0.9271111 -0.8327514 -0.7860082 -0.7407884 -0.6560111 9220.2337 0.9999166 b_noise[2] -0.9589087 0.0007239 0.0650844 -1.0877326 -1.0024880 -0.9586410 -0.9135975 -0.8360297 8083.6928 0.9996969 b_noise[3] -0.6783821 0.0006466 0.0620621 -0.8025203 -0.7202606 -0.6769984 -0.6373789 -0.5589440 9212.0256 1.0003974 b_noise[4] -0.4101144 0.0007183 0.0699979 -0.5541200 -0.4556815 -0.4092248 -0.3627118 -0.2753365 9497.4239 1.0001411 a_bm_norm[1] -0.7041758 0.0074050 0.2440880 -1.1894131 -0.8657688 -0.7055417 -0.5392809 -0.2269247 1086.5259 1.0020074 a_bm_norm[2] -0.5818895 0.0073780 0.4270480 -1.5191613 -0.8418573 -0.5479411 -0.2833611 0.1484277 3350.2487 1.0006340 a_bm_norm[3] -0.6352680 0.0072651 0.2320169 -1.0878214 -0.7884735 -0.6381949 -0.4805968 -0.1731862 1019.9047 1.0019516 a_bm_norm[4] 0.0368664 0.0067509 0.1986395 -0.3358272 -0.1013126 0.0323551 0.1679413 0.4523768 865.7858 1.0024907 a_bm_norm[5] -0.7091761 0.0073280 0.2940958 -1.3154960 -0.8957398 -0.6995976 -0.5110043 -0.1537985 1610.6609 1.0011429 a_bm_norm[6] -0.9853524 0.0078730 0.2463757 -1.4710094 -1.1489153 -0.9852357 -0.8255646 -0.4912610 979.2990 1.0026001 a_bm_norm[7] 2.7705694 0.0107276 0.4115945 1.9929088 2.4867064 2.7579232 3.0485064 3.5979627 1472.0983 1.0012395 a_bm_norm[8] 1.7905731 0.0084627 0.3045386 1.2072707 1.5803690 1.7814323 1.9964897 2.3985149 1294.9769 1.0017889 a_bm_norm[9] 0.0020454 0.0111730 1.0044252 -1.9297168 -0.6878567 -0.0082880 0.6849401 1.9679818 8081.5821 0.9998849 a_bm_norm[10] -0.4393476 0.0071129 0.2114526 -0.8364776 -0.5832773 -0.4466005 -0.2994468 -0.0016197 883.7470 1.0028639 a_bm_norm[11] -0.5384238 0.0071635 0.2390311 -1.0120373 -0.6926342 -0.5387878 -0.3808905 -0.0640232 1113.4031 1.0021580 a_bm_norm[12] 0.9383982 0.0071315 0.2320267 0.4989064 0.7778547 0.9321981 1.0941525 1.4067974 1058.5545 1.0023799 a_bm_norm[13] -0.1434221 0.0068078 0.2063970 -0.5319042 -0.2860761 -0.1480169 -0.0075512 0.2832601 919.1634 1.0029467 a_bm_norm[14] -0.5726918 0.0072433 0.2264003 -1.0093800 -0.7224609 -0.5778708 -0.4215351 -0.1147686 976.9795 1.0022757 a_bm_norm[15] -0.5973401 0.0071526 0.2234751 -1.0297154 -0.7474412 -0.6036431 -0.4493756 -0.1442324 976.1780 1.0021256 a_bm_norm[16] -0.1502131 0.0067989 0.2036326 -0.5376026 -0.2904790 -0.1557584 -0.0157417 0.2663880 897.0410 1.0028733 a_bm_norm[17] -0.8350260 0.0076364 0.2334681 -1.2874667 -0.9917736 -0.8379554 -0.6810848 -0.3605677 934.7052 1.0024115 a_bm_norm[18] 0.2081051 0.0066556 0.2005683 -0.1665740 0.0669625 0.2033960 0.3401691 0.6170056 908.1384 1.0030978 a_bm_norm[19] -0.5037260 0.0075471 0.3074640 -1.1386899 -0.7012929 -0.4932808 -0.2909143 0.0670852 1659.6929 1.0021561 a_bm_norm[20] -0.4077283 0.0070541 0.2170846 -0.8231301 -0.5541845 -0.4122319 -0.2635899 0.0353661 947.0569 1.0032213 a_bm_norm[21] -0.5961393 0.0071996 0.2185068 -1.0155647 -0.7439766 -0.5974407 -0.4536867 -0.1544017 921.1054 1.0023679 a_bm_norm[22] 0.8260845 0.0069766 0.2283002 0.3954488 0.6672539 0.8183983 0.9790294 1.2938929 1070.8364 1.0025092 a_bm_norm[23] -0.5486623 0.0071808 0.2228906 -0.9827855 -0.6989598 -0.5537703 -0.3993232 -0.0914536 963.4607 1.0023567 a_bm_norm[24] 0.8638508 0.0070587 0.2284265 0.4350897 0.7051429 0.8585957 1.0158818 1.3288395 1047.2288 1.0021541 a_bm_norm[25] -1.0517292 0.0078211 0.2523898 -1.5458927 -1.2180663 -1.0537688 -0.8854221 -0.5519734 1041.3668 1.0017377 a_bm_norm[26] 2.1795035 0.0093341 0.3456007 1.5244836 1.9441663 2.1689410 2.4116865 2.8699751 1370.9012 1.0015402 a_bm_norm[27] -0.3155189 0.0070174 0.2250988 -0.7440751 -0.4691298 -0.3204429 -0.1644169 0.1370003 1028.9402 1.0024098 a_bm_norm[28] -0.8538050 0.0074970 0.4198744 -1.7782993 -1.1090358 -0.8195746 -0.5620716 -0.1229214 3136.6107 0.9999658 a_bm_norm[29] 0.3213962 0.0067173 0.2081710 -0.0685415 0.1780737 0.3146147 0.4584381 0.7477982 960.3955 1.0022376 a_bm_norm[30] 0.8008738 0.0069773 0.2245934 0.3818617 0.6456976 0.7942260 0.9504745 1.2578285 1036.1382 1.0025364 s 2.4428738 0.0081690 0.3319133 1.8958297 2.2045872 2.4095331 2.6429476 3.1826092 1650.8714 1.0009461 lp__ -5467.9684851 0.1284895 5.5912387 -5479.8300452 -5471.5617575 -5467.6665614 -5464.0459719 -5457.9444212 1893.5662 1.0007111 5.4 RQ4 Results and Plots 5.4.1 RQ4 Parameters and plots First lets get the HPDI of every parameter. Then we restrict to the algorithms, them to the slopes, then to the parameter s hpdi &lt;- get_HPDI_from_stanfit(timetoconverge_fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_noise&lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;b_noise\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_s &lt;- hpdi %&gt;% dplyr::filter(Parameter==&#39;s&#39;) p_alg&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;a_alg&quot;, x=&quot;Algorithm&quot;)+ coord_flip() p_alg + plot_annotation(title = &#39;HPDI interval for the algorithms&#39;) p_noise&lt;-ggplot(data=hpdi_noise, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;b_noise&quot;, x=&quot;Algorithm&quot;)+ coord_flip() p_noise + plot_annotation(title = &#39;HPDI interval for noise coefficient&#39;) p_s &lt;- ggplot(data=hpdi_s, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate of s&quot;, x=&quot;Parameter&quot;)+ coord_flip() p_s + plot_annotation(title = &#39;HPDI interval std of the benchmarks&#39;) 5.4.2 Hazard ratio hr_table &lt;- tibble( &quot;Algorithms&quot; = algorithms, &quot;Baseline HR&quot; = exp(hpdi_algorithm$Mean), &quot;Noise HR&quot; = exp(hpdi_noise$Mean)) kable(hr_table, booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithms Baseline HR Noise HR CMAES 0.006 0.455 DifferentialEvolution 0.002 0.383 PSO 0.002 0.507 RandomSearch2 0.000 0.664 5.4.3 Iterations to Converge To obtain the average iteration to converge we first extract samples from the posterior distribution Looking at the average value of the data regardless of the benchmarks d_no_na &lt;- d %&gt;% drop_na(y) d_no_na %&gt;% group_by(Algorithm) %&gt;% summarise(Mean=mean(y)) # A tibble: 4 x 2 Algorithm Mean &lt;chr&gt; &lt;dbl&gt; 1 CMAES 115. 2 DifferentialEvolution 3340. 3 PSO 2406. 4 RandomSearch2 12129. posterior &lt;- rstan::extract(timetoconverge_fit) a &lt;- as_tibble(posterior$a_alg) colnames(a) &lt;- algorithms lambda &lt;- exp(a) mu &lt;- 1/lambda #Creating a HPD table timetoconverge_table&lt;-as_tibble(HDInterval::hdi(mu,credMass=0.95), rownames = &quot;Metric&quot;) %&gt;% tibble::add_row(Metric=&quot;Mean&quot;,CMAES=mean(mu$CMAES), DifferentialEvolution=mean(mu$DifferentialEvolution), PSO=mean(mu$PSO), RandomSearch2=mean(mu$RandomSearch2)) %&gt;% tidyr::pivot_longer(cols=-Metric,names_to = &#39;Algorithms&#39;, values_to=&#39;values&#39;) %&gt;% tidyr::pivot_wider(names_from =Metric , values_from=values) %&gt;% dplyr::rename(Mean=Mean, &#39;HPD low&#39; = lower, &#39;HPD high&#39; = upper) %&gt;% dplyr::relocate(Algorithms, Mean) saveRDS(timetoconverge_table, &#39;./statscomp-paper/tables/datafortables/averagetimetoconverge.RDS&#39;) 5.4.4 Merging hazards and time to converge table 5.4.5 Parameter table rename_pars &lt;- c(paste(rep(&#39;a_&#39;,length(algorithms)),algorithms, sep = &quot;&quot;), paste(rep(&#39;b_&#39;,length(algorithms)),algorithms, sep = &quot;&quot;),&#39;s&#39;) t&lt;-create_table_model(timetoconverge_fit, c(a_alg, b_noise, &#39;s&#39;), rename_pars) colnames(t)&lt;-c(&quot;Parameter&quot;, &quot;Mean&quot;, &quot;HPD low&quot;, &quot;HPD high&quot;) saveRDS(t,&#39;./statscomp-paper/tables/datafortables/timetoconverge-hr-par-table.RDS&#39;) "],
["multiple-group-comparison.html", "Chapter 6 Multiple-group comparison 6.1 RQ5 Data preparation 6.2 RQ5 Stan model 6.3 RQ5 Diagnosis 6.4 RQ5 Results and Plots", " Chapter 6 Multiple-group comparison We present here the Stan version of the BEST (Bayesian Estimation Supersedes the t Test) from John K. Kruschke. We will consider the following research question RQ5: Is there a difference in the time taken per function evaluation between the PSO, the RandomSearch1 and the Differential Evolution algorithms? 6.1 RQ5 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&#39;./data/statscomp.csv&#39;) Filtering the data that we want and applying some transformations d &lt;- dataset %&gt;% dplyr::filter( OptimizationSuccessful==TRUE &amp; (Algorithm==&quot;PSO&quot; | Algorithm==&quot;RandomSearch1&quot; | Algorithm==&quot;DifferentialEvolution&quot;)) %&gt;% dplyr::select(Algorithm, CostFunction, TimeToComplete, simNumber, MaxFeval) %&gt;% dplyr::mutate(y=10000*TimeToComplete/MaxFeval, CostFunctionID=create_index(CostFunction), AlgorithmID=create_index(Algorithm)) %&gt;% dplyr::select(Algorithm, AlgorithmID, CostFunction, CostFunctionID, y,-simNumber, -MaxFeval) algorithms&lt;-get_index_names_as_array(d$Algorithm) bm &lt;- get_index_names_as_array(d$CostFunction) The data should look like this: kable(dplyr::sample_n(d,size=10), &quot;html&quot;,booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithm AlgorithmID CostFunction CostFunctionID y DifferentialEvolution 1 ThreeHumpCamelBack 24 1.456 DifferentialEvolution 1 Price1 12 1.472 DifferentialEvolution 1 Shubert 21 1.771 DifferentialEvolution 1 PinterN6 11 2.157 RandomSearch1 3 PinterN6 11 0.754 DifferentialEvolution 1 LunacekBiRastriginN6 9 2.607 PSO 2 Mishra7N6 10 0.461 DifferentialEvolution 1 WhitleyN6 28 3.009 RandomSearch1 3 Trefethen 25 0.297 DifferentialEvolution 1 ZakharovN2 30 1.670 Some initial visualizations in terms of box-plots p1&lt;-ggplot(d) + geom_boxplot(aes(x=Algorithm, y=y))+ labs(y=&quot;Time to complete x10,000 (s)&quot;) p1 + plot_annotation(title =&quot;Box-plot of the runtime per function evaluation&quot;) lmfit &lt;- lm(y~Algorithm, data=d) p2&lt;-ggplot()+ geom_qq(aes(sample=lmfit$residuals))+ geom_qq_line(aes(sample=lmfit$residuals))+ labs(x=&quot;Standard normal quantiles&quot;, y=&quot;Sample quantiles&quot;) p2 + plot_annotation(title = &quot;Q-Q plot for normality analysis&quot;) Verifying that the log of the runtime still present high tailed distributions d2&lt;-d d2$y &lt;- log(d2$y) lmfit2 &lt;- lm(y~Algorithm, data=d2) p3&lt;-ggplot()+ geom_qq(aes(sample=lmfit2$residuals))+ geom_qq_line(aes(sample=lmfit2$residuals))+ labs(x=&quot;Standard normal quantiles&quot;, y=&quot;Sample quantiles&quot;) p2 + plot_annotation(title = &quot;Q-Q plot with the log of runtime&quot;) 6.2 RQ5 Stan model The Stan model is specified in the file: './stanmodels/multiplegroups.stan'. Note that at the end of the model we commented the generated quantities. This block generates the predictive posterior y_rep and the log likelihood, log_lik. These values are useful in diagnosing and validating the model but the end file is extremely large (~1Gb for 2000 iterations) and make many of the following calculations slow. If the reader wants to see these values is just to uncomment and run the stan model again. print_stan_code(&#39;./stanmodels/multiplegroups.stan&#39;) // Multiple group comparison // Author: David Issa Mattos // Date: 23 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size real y[N_total]; // time to complete variable //To model each algorithm independently int &lt;lower=1&gt; N_algorithm; // Number of algorithms int algorithm_id[N_total]; //vector that has the id of each algorithm //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { //Fixed effect real a_alg[N_algorithm];//the mean effect given by the algorithms real &lt;lower=0&gt; sigma[N_algorithm];//std for the student t // //Random effect. The effect of the benchmarks real a_bm_norm[N_bm];//the mean effect given by the base class type real&lt;lower=0&gt; s;//std for the random effects real&lt;lower=0&gt; nu;//std for the random effects } model { real mu[N_total]; real sigma_i[N_total]; sigma ~ exponential(1); nu ~ exponential(1.0/30.0); //Fixed effect a_alg ~ normal(0,1); // //Random effects s ~ exponential(1); a_bm_norm ~ normal(0,1); for (i in 1:N_total) { mu[i] = a_alg[algorithm_id[i]] + a_bm_norm[bm_id[i]]*s; sigma_i[i] = sigma[algorithm_id[i]]; } y ~ student_t(nu, mu, sigma_i); } //Uncoment this part to get the posterior predictives and the log likelihood //But note that it takes a lot of space in the final model // generated quantities{ // vector [N_total] y_rep; // vector[N_total] log_lik; // for (i in 1:N_total){ // real mu; // real sigma_i; // mu = a_alg[algorithm_id[i]] + a_bm_norm[bm_id[i]]*s; // sigma_i = sigma[algorithm_id[i]]; // y_rep[i] = student_t_rng(nu,mu,sigma_i); // // //Log likelihood // log_lik[i] = student_t_lpdf(y[i] | nu,mu,sigma_i); // // } // } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/multiplegroup-data.RDS\" standata &lt;- list( N_total=nrow(d), y = d$y, N_algorithm = length(algorithms), algorithm_id = d$AlgorithmID, N_bm = length(bm), bm_id = d$CostFunctionID ) saveRDS(standata, file = &quot;./data/multiplegroups-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately. Here we increased the maxtreedepth and the number of iterations so we have a higher effective sample for inference. Both of these do not impact the validity of the chain just the computation efficiency. standata&lt;-readRDS(&quot;./data/multiplegroups-data.RDS&quot;) multiplegroup_fit &lt;- stan(file = &#39;./stanmodels/multiplegroups.stan&#39;, data=standata, chains = 4, warmup = 400, iter = 4000, control = list(max_treedepth = 15)) saveRDS(multiplegroup_fit, file = &quot;./data/multiplegroups-fit.RDS&quot;) 6.3 RQ5 Diagnosis a_alg_v &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;) sigma_v &lt;- c(&quot;sigma[1]&quot;, &quot;sigma[2]&quot;, &quot;sigma[3]&quot;) rstan::traceplot(multiplegroup_fit, pars=a_alg_v) rstan::traceplot(multiplegroup_fit, pars=sigma_v) rstan::traceplot(multiplegroup_fit, pars=c(&#39;s&#39;, &#39;nu&#39;)) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.05 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(multiplegroup_fit)$summary, &quot;html&quot;,) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] 1.7811268 0.0026230 0.0606078 1.6619826 1.7403893 1.7820330 1.8215807 1.8982458 533.8817 1.006126 a_alg[2] 0.5691139 0.0026209 0.0605588 0.4496716 0.5283954 0.5700587 0.6093643 0.6855433 533.8829 1.006094 a_alg[3] 0.4423529 0.0026208 0.0605582 0.3233317 0.4013482 0.4432400 0.4826083 0.5580626 533.9264 1.006091 sigma[1] 0.0877444 0.0000318 0.0017455 0.0843878 0.0865374 0.0877212 0.0889341 0.0911437 3011.1145 1.001623 sigma[2] 0.0694441 0.0000277 0.0015140 0.0664710 0.0684330 0.0694499 0.0704463 0.0724822 2976.8074 1.000836 sigma[3] 0.0368688 0.0000144 0.0008039 0.0352959 0.0363257 0.0368479 0.0374083 0.0384708 3121.5824 1.000719 a_bm_norm[1] 0.7169454 0.0087107 0.2135654 0.2988931 0.5710651 0.7142151 0.8674880 1.1328629 601.1139 1.008025 a_bm_norm[2] -0.1824269 0.0085491 0.1988784 -0.5714764 -0.3176881 -0.1827604 -0.0451221 0.1940826 541.1669 1.005750 a_bm_norm[3] -0.5830931 0.0091159 0.2163121 -1.0160424 -0.7275807 -0.5801526 -0.4404438 -0.1643768 563.0722 1.003927 a_bm_norm[4] -0.6091954 0.0091666 0.2178293 -1.0467759 -0.7534750 -0.6062762 -0.4656718 -0.1855039 564.6959 1.003882 a_bm_norm[5] -0.2485564 0.0086165 0.2008707 -0.6432752 -0.3856466 -0.2485221 -0.1111202 0.1316417 543.4600 1.005561 a_bm_norm[6] 0.4734111 0.0084824 0.2024756 0.0780666 0.3334823 0.4720095 0.6152515 0.8568556 569.7825 1.007887 a_bm_norm[7] -0.6423130 0.0092312 0.2198717 -1.0846795 -0.7887968 -0.6386941 -0.4972334 -0.2139691 567.3092 1.003799 a_bm_norm[8] -0.1644817 0.0085326 0.1985217 -0.5517190 -0.2996106 -0.1657384 -0.0275923 0.2129775 541.3154 1.005963 a_bm_norm[9] 2.1116300 0.0121812 0.3393819 1.4484849 1.8816608 2.1092589 2.3358436 2.7967160 776.2394 1.005745 a_bm_norm[10] -0.1812781 0.0085416 0.1989285 -0.5714155 -0.3176169 -0.1816472 -0.0444326 0.1975518 542.3993 1.005821 a_bm_norm[11] 0.9019238 0.0089863 0.2249558 0.4576177 0.7501431 0.8991386 1.0577316 1.3417398 626.6611 1.007809 a_bm_norm[12] -0.7818005 0.0095262 0.2292960 -1.2427092 -0.9348107 -0.7782093 -0.6287008 -0.3328971 579.3611 1.003212 a_bm_norm[13] -0.6775106 0.0092827 0.2220651 -1.1227791 -0.8255683 -0.6734795 -0.5296425 -0.2434466 572.2849 1.003527 a_bm_norm[14] 1.7137941 0.0109291 0.2967903 1.1337453 1.5123317 1.7108835 1.9123530 2.3102016 737.4474 1.006425 a_bm_norm[15] -0.2518458 0.0086224 0.2009569 -0.6477380 -0.3884656 -0.2520031 -0.1136934 0.1301903 543.1896 1.005519 a_bm_norm[16] -0.6890408 0.0093176 0.2228297 -1.1395233 -0.8382348 -0.6855899 -0.5409571 -0.2539657 571.9248 1.003543 a_bm_norm[17] -0.2997380 0.0086702 0.2027220 -0.6996320 -0.4386068 -0.2983614 -0.1626570 0.0871212 546.6910 1.005343 a_bm_norm[18] -0.2065718 0.0085791 0.1997227 -0.5978602 -0.3420286 -0.2057953 -0.0687583 0.1743771 541.9588 1.005684 a_bm_norm[19] -0.1781351 0.0085411 0.1988323 -0.5689114 -0.3141829 -0.1783365 -0.0407893 0.1991033 541.9402 1.005823 a_bm_norm[20] 0.0013479 0.0084299 0.1959243 -0.3801857 -0.1321950 -0.0011718 0.1384331 0.3748182 540.1669 1.006703 a_bm_norm[21] -0.0381568 0.0084334 0.1961037 -0.4215247 -0.1727083 -0.0400992 0.0998043 0.3345669 540.7160 1.006430 a_bm_norm[22] -0.2591012 0.0086473 0.2014404 -0.6537203 -0.3957154 -0.2589614 -0.1220667 0.1262415 542.6678 1.005472 a_bm_norm[23] -0.7358550 0.0094254 0.2260798 -1.1892931 -0.8858307 -0.7320868 -0.5846833 -0.2939862 575.3385 1.003330 a_bm_norm[24] -0.8254234 0.0096072 0.2324392 -1.2935835 -0.9805076 -0.8213645 -0.6689679 -0.3684101 585.3639 1.003067 a_bm_norm[25] -0.3366898 0.0087284 0.2039811 -0.7369234 -0.4748569 -0.3359966 -0.1992994 0.0536531 546.1521 1.005150 a_bm_norm[26] 0.3488281 0.0084028 0.1985740 -0.0350128 0.2120805 0.3487199 0.4879435 0.7238281 558.4606 1.007803 a_bm_norm[27] -0.7346498 0.0094058 0.2258222 -1.1905483 -0.8847851 -0.7302798 -0.5839647 -0.2927947 576.4249 1.003359 a_bm_norm[28] 3.6998131 0.0182556 0.5324392 2.6893552 3.3338793 3.6905820 4.0513884 4.7696181 850.6446 1.004008 a_bm_norm[29] -0.4143554 0.0088376 0.2074474 -0.8239239 -0.5542043 -0.4118008 -0.2764703 -0.0164877 550.9920 1.004714 a_bm_norm[30] -0.1770084 0.0085576 0.1989495 -0.5672901 -0.3128015 -0.1772755 -0.0397254 0.2004904 540.4818 1.005940 s 0.3072466 0.0014798 0.0441124 0.2352709 0.2768578 0.3021747 0.3322788 0.4069552 888.6323 1.001588 nu 2.7532026 0.0015421 0.0802559 2.6021136 2.6972495 2.7520588 2.8075206 2.9112477 2708.6351 1.001055 lp__ 14070.2753067 0.1671716 5.9172571 14057.6082046 14066.4863632 14070.6208015 14074.4422447 14080.7870357 1252.8982 1.000465 6.4 RQ5 Results and Plots First lets get the HPDI of every parameter. Then we restrict to the algorithms, them to the slopes, then to the parameter s hpdi &lt;- get_HPDI_from_stanfit(multiplegroup_fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_sigma&lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;sigma\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_s &lt;- hpdi %&gt;% dplyr::filter(Parameter==&#39;s&#39;) hpdi_nu &lt;- hpdi %&gt;% dplyr::filter(Parameter==&#39;nu&#39;) hpdi_nu_s &lt;- hpdi %&gt;% dplyr::filter(Parameter==&#39;nu&#39; | Parameter==&#39;s&#39;) p_alg&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;a_alg&quot;, x=&quot;Algorithm&quot;)+ theme(axis.title.x= element_blank())+ coord_flip() p_alg + plot_annotation(title = &#39;HPDI interval for the algorithms&#39;) p_sigma&lt;-ggplot(data=hpdi_sigma, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;sigma&quot;, x=&quot;Algorithm&quot;)+ theme(axis.title.x= element_blank())+ coord_flip() p_sigma + plot_annotation(title = &#39;HPDI interval for sigma&#39;) p_s &lt;- ggplot(data=hpdi_s, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;s&quot;, x=&quot;Parameter&quot;)+ coord_flip() p_s + plot_annotation(title = &#39;HPDI interval std of the benchmarks&#39;) p_nu &lt;- ggplot(data=hpdi_nu, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;nu&quot;, x=&quot;Parameter&quot;)+ coord_flip() p_nu + plot_annotation(title = &#39;HPDI interval of the degree of freedom&#39;) p_nu_s &lt;- ggplot(data=hpdi_nu_s, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate of s and nu&quot;, x=&quot;Parameter&quot;)+ theme(axis.title.x= element_blank())+ coord_flip() p_nu_s + plot_annotation(title = &#39;HPDI interval&#39;) Now lets get a posterior distribution of the difference posterior &lt;- rstan::extract(multiplegroup_fit) a_alg &lt;- as_tibble(posterior$a_alg) colnames(a_alg) &lt;- algorithms sample_a_alg &lt;- dplyr::sample_n(a_alg, size=1000, replace=T) %&gt;% dplyr::mutate(PSO_Random = PSO-RandomSearch1, DE_PSO= DifferentialEvolution-PSO, DE_Random = DifferentialEvolution-RandomSearch1) %&gt;% dplyr::select(-DifferentialEvolution,-PSO,-RandomSearch1) #Getting HPDI from a data frame and creating a table instead of plotting... hpdi_diff&lt;-HDInterval::hdi(sample_a_alg,credMass=0.95) hpdi_diff&lt;-hpdi_diff %&gt;% as_tibble(rownames = &quot;Metric&quot;) %&gt;% tibble::add_row(Metric=&quot;Mean&quot;, PSO_Random=mean(sample_a_alg$PSO_Random), DE_PSO=mean(sample_a_alg$DE_PSO), DE_Random=mean(sample_a_alg$DE_Random)) %&gt;% tidyr::pivot_longer(cols=-Metric, names_to=&quot;AlgorithmDifference&quot;, values_to=&#39;values&#39;) %&gt;% tidyr::pivot_wider(names_from =Metric , values_from=values) %&gt;% dplyr::mutate(Difference=c(&#39;PSO - RandomSearch&#39;, &#39;DiffEvolution - PSO&#39;, &#39;DiffEvolution - RandomSearch&#39;)) %&gt;% dplyr::select(Difference, Lower=lower, Mean, Upper=upper) kable(hpdi_diff, booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Difference Lower Mean Upper PSO - RandomSearch 0.123 0.127 0.130 DiffEvolution - PSO 1.207 1.212 1.217 DiffEvolution - RandomSearch 1.334 1.339 1.343 Creating an output table rename_pars &lt;- c( paste(rep(&#39;a_&#39;,length(algorithms)),algorithms, sep = &quot;&quot;), paste(rep(&#39;sigma_&#39;,length(algorithms)),algorithms, sep = &quot;&quot;), &#39;s&#39;, &#39;nu&#39;) t&lt;-create_table_model(multiplegroup_fit, pars=c(a_alg_v, sigma_v, &#39;s&#39;,&#39;nu&#39;),rename_pars) colnames(t)&lt;-c(&quot;Parameter&quot;, &quot;Mean&quot;, &quot;HPD low&quot;, &quot;HPD high&quot;) saveRDS(t,&#39;./statscomp-paper/tables/datafortables/multiplegroupsdifference-par-table.RDS&#39;) "],
["sensitivity-analysis-model-comparison-and-posterior-predictive.html", "Chapter 7 Sensitivity analysis, model comparison and posterior predictive 7.1 Compare models with and without clustering 7.2 Sensitivity analysis of priors 7.3 Posterior predictive plots", " Chapter 7 Sensitivity analysis, model comparison and posterior predictive In this chapter, we provide an example on how to do a sensitivity analysis and model comparison. For that we will use the Relative Improvement model of chapter 3. Note that it is important to have a model that calculates the log likelihood to compute the WAIC or the LOO-CV. Here we will show only how to use and interpret the WAIC. Reading the data for the (already prepared data) for the model standata&lt;-readRDS(&quot;./data/relativeimprovement-data.RDS&quot;) algorithms&lt;-readRDS(&quot;./data/relativeimprovement_algorithms.RDS&quot;) bm&lt;-readRDS(&quot;./data/relativeimprovement_bm.RDS&quot;) Here we consider 3 models for the relative improvement. The original model presented on chapter 3 and in the paper. A model without clustering information about the benchmarks (m1) and a model with a different set of priors (m2) In m2 we consider HalfNormal(0,5) for both the s and the sigma parameters (instead of the exponential) We are saving the files in a non-tracked folder because they are too big for github relativeimprovement_fit_original &lt;- stan(file = &#39;./stanmodels/relativeimprovement-original.stan&#39;, data=standata, chains = 4, warmup = 200, iter = 1000) saveRDS(relativeimprovement_fit_original, file = &quot;./data/gitignore/relativeimprovement_fit_original.RDS&quot;) standata_m1 = list( N_total= standata$N_total, y = standata$y, N_algorithm = standata$N_algorithm, algorithm_id = standata$algorithm_id ) relativeimprovement_fit_m1 &lt;- stan(file = &#39;./stanmodels/relativeimprovement-m1.stan&#39;, data=standata_m1, chains = 4, warmup = 200, iter = 1000) saveRDS(relativeimprovement_fit_m1, file = &quot;./data/gitignore/relativeimprovement_fit_m1.RDS&quot;) standata_m2 &lt;- standata relativeimprovement_fit_m2 &lt;- stan(file = &#39;./stanmodels/relativeimprovement-m2.stan&#39;, data=standata, chains = 4, warmup = 200, iter = 1000) saveRDS(relativeimprovement_fit_m2, file = &quot;./data/gitignore/relativeimprovement_fit_m2.RDS&quot;) 7.1 Compare models with and without clustering First we get the log likelihood log_lik_original &lt;- loo::extract_log_lik(relativeimprovement_fit_original, merge_chains = T) log_lik_m1 &lt;- loo::extract_log_lik(relativeimprovement_fit_m1 ,merge_chains = T) Then we compute the waic waic_original&lt;-loo::waic(log_lik_original) waic_m1&lt;-loo::waic(log_lik_m1) Now we use the compare function print(waic_original) Computed from 3200 by 9000 log-likelihood matrix Estimate SE elpd_waic -8723.2 53.7 p_waic 33.7 0.4 waic 17446.3 107.4 print(waic_m1) Computed from 3200 by 9000 log-likelihood matrix Estimate SE elpd_waic -8913.5 52.2 p_waic 6.6 0.1 waic 17827.0 104.4 loo::loo_compare(waic_original, waic_m1) elpd_diff se_diff model1 0.0 0.0 model2 -190.3 20.1 We can see that the WAIC original (with clustering) provides a big improvement over m1 (without clustering). 7.2 Sensitivity analysis of priors log_lik_m2 &lt;- loo::extract_log_lik(relativeimprovement_fit_m2, merge_chains = T) First let’s look at the WAIC waic_m2&lt;-loo::waic(log_lik_m2) print(waic_m2) Computed from 3200 by 9000 log-likelihood matrix Estimate SE elpd_waic -8723.6 53.7 p_waic 34.1 0.4 waic 17447.2 107.5 Comparing the models loo::loo_compare(waic_original, waic_m2) elpd_diff se_diff model1 0.0 0.0 model2 -0.4 0.1 We can see here that there is no significant difference between the models with the two priors. This already indicates some robustness in the estimation parameters regardless of the priors (which is expected since both are weakly informative priors). Comparing the estimates for the intercepts of the algorithms only. Note that since we have a very big stanfit the summary calculations might take a bit longer. a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;, &quot;a_alg[5]&quot;, &quot;a_alg[6]&quot;) df_original&lt;-summary(relativeimprovement_fit_original, pars = a_alg)$summary kable(df_original, &quot;html&quot;,booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] 0.154 0.001 0.032 0.090 0.132 0.154 0.175 0.215 470.933 1.007 a_alg[2] -0.374 0.002 0.032 -0.437 -0.396 -0.374 -0.353 -0.312 453.902 1.005 a_alg[3] 0.306 0.002 0.032 0.244 0.284 0.306 0.328 0.368 459.828 1.005 a_alg[4] -0.638 0.001 0.032 -0.700 -0.659 -0.638 -0.617 -0.575 465.164 1.004 a_alg[5] 0.323 0.001 0.032 0.258 0.301 0.324 0.345 0.385 472.724 1.004 a_alg[6] -0.567 0.001 0.032 -0.630 -0.589 -0.567 -0.545 -0.506 472.289 1.005 df_m2&lt;-summary(relativeimprovement_fit_m2, pars = a_alg)$summary kable(df_m2, &quot;html&quot;,booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] 0.148 0.002 0.031 0.083 0.129 0.148 0.169 0.209 423.994 1.016 a_alg[2] -0.380 0.001 0.031 -0.445 -0.400 -0.380 -0.359 -0.322 426.036 1.015 a_alg[3] 0.300 0.002 0.031 0.236 0.280 0.302 0.321 0.360 428.207 1.016 a_alg[4] -0.644 0.002 0.031 -0.709 -0.664 -0.643 -0.622 -0.585 431.340 1.012 a_alg[5] 0.318 0.002 0.031 0.253 0.298 0.319 0.339 0.378 422.176 1.015 a_alg[6] -0.573 0.002 0.031 -0.638 -0.593 -0.573 -0.552 -0.514 417.598 1.012 We can see from both tables that the estimates of the algorithms intercepts are very similar, which starts to indicate a certain robustness of the model in respect to the priors. 7.3 Posterior predictive plots To check for the posterior predictive we will use the original model. First we extract the posterior of the predictive values. We have in this posterior 3200 rows (800 iterations for every chain) and 9000 columns (1 for each point in the dataset). Lets start by resampling to get only 100 estimates for each observation. Then we will create a data frame that has a column for each type of observation. Then we will pivot longer so the 100 observations go to a single column. This will multiply the dataset number of rows by 100 . This will facilitate plotting with ggplot y_rep_posterior &lt;- as_tibble(rstan::extract(relativeimprovement_fit_original, pars=&#39;y_rep&#39;)$y_rep) y_rep &lt;- as_tibble(t(sample_n(y_rep_posterior, size=100))) y &lt;- as_tibble(standata$y) %&gt;% select(y_obs=value) algo&lt;-as_tibble(standata$algorithm_id) %&gt;% select(algo=value) algo$algo&lt;-dplyr::recode(algo$algo, &#39;1&#39;=algorithms[1], &#39;2&#39;=algorithms[2], &#39;3&#39;=algorithms[3], &#39;4&#39;=algorithms[4], &#39;5&#39;=algorithms[5], &#39;6&#39;=algorithms[6]) benchmark &lt;- as_tibble(standata$bm_id) %&gt;% select(benchmark=value) benchmark$benchmark&lt;-dplyr::recode(benchmark$benchmark, &#39;1&#39;=bm[1], &#39;2&#39;=bm[2], &#39;3&#39;=bm[3], &#39;4&#39;=bm[4], &#39;5&#39;=bm[5], &#39;6&#39;=bm[6], &#39;7&#39;=bm[7], &#39;8&#39;=bm[8], &#39;9&#39;=bm[9], &#39;10&#39;=bm[10], &#39;11&#39;=bm[11], &#39;12&#39;=bm[12], &#39;13&#39;=bm[13], &#39;14&#39;=bm[14], &#39;15&#39;=bm[15], &#39;16&#39;=bm[16], &#39;17&#39;=bm[17], &#39;18&#39;=bm[18], &#39;19&#39;=bm[19], &#39;20&#39;=bm[20], &#39;21&#39;=bm[21], &#39;22&#39;=bm[22], &#39;23&#39;=bm[23], &#39;24&#39;=bm[24], &#39;25&#39;=bm[25], &#39;26&#39;=bm[26], &#39;27&#39;=bm[27], &#39;28&#39;=bm[28], &#39;29&#39;=bm[29], &#39;30&#39;=bm[30] ) df &lt;- algo %&gt;% add_column(benchmark) %&gt;% add_column(y) %&gt;% add_column(y_rep) %&gt;% tidyr::pivot_longer(cols=4:103,names_to = &#39;sample&#39;, values_to=&#39;y_rep&#39;) There are multiple ways to plot predictive posterior. One of them is with a histogram plot of the predictions, or lines for th intercept etc.. Here we plot the histogram of each benchmark function for the PSO algorithm. Note that the model predicts better for some benchmark functions and not so well for others, but in average all the observed values are inside the histogram of the predictions ggplot(data=dplyr::filter(df, algo==&#39;PSO&#39;))+ geom_histogram(aes(x=y_rep), fill=&#39;black&#39;, alpha=0.8)+ geom_histogram(aes(x=y_obs), fill=&#39;blue&#39;, alpha=0.8)+ facet_wrap(~benchmark)+ labs(title=&#39;Predictive posterior for the PSO&#39;) "]
]
