[
["index.html", "Statistical Models for Benchmark Comparison Preface Pre-requisites Source code Compiling this document Software environment", " Statistical Models for Benchmark Comparison David Issa Mattos 25 June, 2020 Preface This document is an online appendix to the paper “Statistical Models for Benchmark Comparison” by David Issa Mattos, Jan Bosch and Helena Holmström Olsson. It shows the process to analyze the data, including data preparation, modeling and plotting for all the models described on the paper. Pre-requisites We assume that the reader has some familiarity with the R environment including packages included of the tidyverse, such as dplyr and ggplot2. The code presented is described and fairly commented to help readers follow the modeling process. Other programming languages such as Python with numpy, pandas, matplotlib etc are capable of performing the same steps, but this is out of the scope of this document. For the Bayesian models, we try to minimize dependency on a specific R package such as brms or rstanarm, and therefore we discuss the model in Stan only, since it has bindings for multiple programming languages. Source code The full source code is available in the repository. The raw data for each model (after the described data transformation) is also available for download in the ./data folder. The Stan models are available in the ./stanmodels folder. The utils folder contains some helper functions to help plotting, and generating figures etc… In the chapter utils we discuss each function in detail The environment was defined and based on the renv package. The renv package logs all the packages in the renv.lock file and manages installation for a specific project. For more information see !!!!!!! To replicate this environment, after downloading this repository, type: renv::hydrate() This command will download and install all the the packages use in this work. Note that it will install the packages only for this project. Compiling this document This document was created with the bookdown package. To compile it (and run every command to generate the models, figures and etc. ) type: rmarkdown::render_site(&quot;./index.Rmd&quot;, encoding = &quot;UTF-8&quot;) Software environment We suggest to use Stan installed in Linux. There appear to be several problems with Stan in Mac Os Catalina. The environment used is defined in the renv.lock file and session information used to compile this document is sessionInfo() R version 3.6.3 (2020-02-29) Platform: x86_64-apple-darwin15.6.0 (64-bit) Running under: macOS Catalina 10.15.4 Matrix products: default BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 attached base packages: [1] graphics grDevices datasets stats utils methods base other attached packages: [1] gtools_3.8.2 progress_1.2.2 ggthemr_1.1.0 viridis_0.5.1 [5] viridisLite_0.3.0 patchwork_1.0.0 coda_0.19-3 rstan_2.19.3 [9] StanHeaders_2.21.0-5 glue_1.4.1 forcats_0.5.0 stringr_1.4.0 [13] dplyr_1.0.0 purrr_0.3.4 readr_1.3.1 tidyr_1.1.0 [17] tibble_3.0.1 ggplot2_3.3.1 tidyverse_1.3.0 kableExtra_1.1.0 [21] rmdformats_0.3.7 knitr_1.28 loaded via a namespace (and not attached): [1] nlme_3.1-148 matrixStats_0.56.0 fs_1.4.1 lubridate_1.7.9 [5] webshot_0.5.2 httr_1.4.1 tools_3.6.3 backports_1.1.7 [9] R6_2.4.1 DBI_1.1.0 colorspace_1.4-1 withr_2.2.0 [13] tidyselect_1.1.0 gridExtra_2.3 prettyunits_1.1.1 processx_3.4.2 [17] compiler_3.6.3 cli_2.0.2 rvest_0.3.5 formatR_1.7 [21] HDInterval_0.2.2 xml2_1.3.2 labeling_0.3 bookdown_0.19 [25] scales_1.1.1 callr_3.4.3 digest_0.6.25 rmarkdown_2.2 [29] pkgconfig_2.0.3 htmltools_0.4.0 dbplyr_1.4.4 highr_0.8 [33] rlang_0.4.6 readxl_1.3.1 rstudioapi_0.11 farver_2.0.3 [37] generics_0.0.2 jsonlite_1.6.1 inline_0.3.15 magrittr_1.5 [41] loo_2.2.0 Rcpp_1.0.4.6 munsell_0.5.0 fansi_0.4.1 [45] lifecycle_0.2.0 stringi_1.4.6 yaml_2.2.1 plyr_1.8.6 [49] pkgbuild_1.0.8 grid_3.6.3 blob_1.2.1 parallel_3.6.3 [53] crayon_1.3.4 lattice_0.20-41 haven_2.3.1 hms_0.5.3 [57] ps_1.3.3 pillar_1.4.4 stats4_3.6.3 reprex_0.3.0 [61] evaluate_0.14 renv_0.10.0 RcppParallel_5.0.1 modelr_0.1.8 [65] vctrs_0.3.1 cellranger_1.1.0 gtable_0.3.0 assertthat_0.2.1 [69] xfun_0.14 broom_0.5.6 tinytex_0.23 ellipsis_0.3.1 "],
["introduction.html", "Chapter 1 Introduction 1.1 The simulation", " Chapter 1 Introduction This document is based on a single dataset. We will ask several possible research questions from this dataset in order to develop and motivate the statistical models of this paper. But first let’s explore the dataset and how this data was obtained. 1.1 The simulation 1.1.1 Exploring the dataset This dataset follows the principle of tidy data as described in !!!!!!!!!. The key idea is that every variables has its own column, and every observation has its own unique row. Throughout this document, to facilitate our modeling approach, we will modify this dataset in different ways, often resulting in non-tidy data. However every model will start from the same base tidy dataset. This approach will hopefully make it easier for the reader to understand from where we are starting and adopt similar strategies in their own models. Additionally, we recommend, if the reader has the opportunity to influence the data collection process, the choice of tidy data. It is often ideal for exploratory analysis, plotting, is the basis for most models, and easy to transform to be used in different models. d &lt;- readr::read_csv(&quot;./data/statscomp.csv&quot;) Here we are excluding a few columns to simplify our view kable(head(dplyr::select(d, -BestArm, -Continuous, -Differentiability, -Separability, -Scalability, -Modality, -BBOB, -BaseClass, -MaxFeval, -FevalPerDimensions), n = 10), &quot;html&quot;) Algorithm CostFunction NumberFunctionEval EuclideanDistance TrueRewardDifference CumulativeRegret TimeToComplete Ndimensions OptimizationSuccessful SD MaxFevalPerDimensions SolveAt1 SolveAt1e-1 SolveAt1e-3 SolveAt1e-6 SolveEarlierAt1 SolveEarlierAt1e-1 SolveEarlierAt1e-3 SolveEarlierAt1e-6 simNumber NelderMead BentCigarN6 600 8.9123708 7.364249e+07 4.926652e+10 0.0300207 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 PSO BentCigarN6 600 0.5605997 1.559497e+05 7.938082e+10 0.0394440 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 SimulatedAnnealing BentCigarN6 600 9.7499527 1.086834e+08 1.208830e+12 0.0424774 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 CuckooSearch BentCigarN6 600 8.0025211 1.200314e+07 1.017438e+13 0.0317579 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 DifferentialEvolution BentCigarN6 600 5.3888603 4.634518e+06 2.399718e+12 0.1168543 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 RandomSearch1 BentCigarN6 600 1.5702536 1.919896e+06 1.983989e+13 0.0399160 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 RandomSearch2 BentCigarN6 599 1.5702536 1.655484e+06 1.929796e+13 0.0356977 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 CMAES BentCigarN6 604 0.5744144 3.357865e-01 2.589247e+08 0.1810286 6 TRUE 0 100 TRUE FALSE FALSE FALSE 543 NA NA NA 0 NelderMead BentCigarN6 600 7.9123493 2.152945e+07 2.041479e+11 0.0423668 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 1 PSO BentCigarN6 600 1.0818350 1.853063e+05 7.224343e+10 0.0397996 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 1 1.1.2 Column definitions "],
["introduction-1.html", "Chapter 2 Introduction 2.1 The simulation", " Chapter 2 Introduction This document is based on a single dataset. We will ask several possible research questions from this dataset in order to develop and motivate the statistical models of this paper. But first let’s explore the dataset and how this data was obtained. 2.1 The simulation 2.1.1 Exploring the dataset This dataset follows the principle of tidy data as described in !!!!!!!!!. The key idea is that every variables has its own column, and every observation has its own unique row. Throughout this document, to facilitate our modeling approach, we will modify this dataset in different ways, often resulting in non-tidy data. However every model will start from the same base tidy dataset. This approach will hopefully make it easier for the reader to understand from where we are starting and adopt similar strategies in their own models. Additionally, we recommend, if the reader has the opportunity to influence the data collection process, the choice of tidy data. It is often ideal for exploratory analysis, plotting, is the basis for most models, and easy to transform to be used in different models. d &lt;- readr::read_csv(&quot;./data/statscomp.csv&quot;) Here we are excluding a few columns to simplify our view kable(head(dplyr::select(d, -BestArm, -Continuous, -Differentiability, -Separability, -Scalability, -Modality, -BBOB, -BaseClass, -MaxFeval, -FevalPerDimensions), n = 10)) Algorithm CostFunction NumberFunctionEval EuclideanDistance TrueRewardDifference CumulativeRegret TimeToComplete Ndimensions OptimizationSuccessful SD MaxFevalPerDimensions SolveAt1 SolveAt1e-1 SolveAt1e-3 SolveAt1e-6 SolveEarlierAt1 SolveEarlierAt1e-1 SolveEarlierAt1e-3 SolveEarlierAt1e-6 simNumber NelderMead BentCigarN6 600 8.9123708 7.364249e+07 4.926652e+10 0.0300207 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 PSO BentCigarN6 600 0.5605997 1.559497e+05 7.938082e+10 0.0394440 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 SimulatedAnnealing BentCigarN6 600 9.7499527 1.086834e+08 1.208830e+12 0.0424774 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 CuckooSearch BentCigarN6 600 8.0025211 1.200314e+07 1.017438e+13 0.0317579 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 DifferentialEvolution BentCigarN6 600 5.3888603 4.634518e+06 2.399718e+12 0.1168543 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 RandomSearch1 BentCigarN6 600 1.5702536 1.919896e+06 1.983989e+13 0.0399160 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 RandomSearch2 BentCigarN6 599 1.5702536 1.655484e+06 1.929796e+13 0.0356977 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 CMAES BentCigarN6 604 0.5744144 3.357865e-01 2.589247e+08 0.1810286 6 TRUE 0 100 TRUE FALSE FALSE FALSE 543 NA NA NA 0 NelderMead BentCigarN6 600 7.9123493 2.152945e+07 2.041479e+11 0.0423668 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 1 PSO BentCigarN6 600 1.0818350 1.853063e+05 7.224343e+10 0.0397996 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 1 2.1.2 Column definitions "],
["probability-of-success-model.html", "Chapter 3 Probability of success model 3.1 Data preparation 3.2 Stan model 3.3 Diagnosis 3.4 Results and Plots", " Chapter 3 Probability of success model Our first model can be used to address problems such as: RQ1 What is the probability of each solving a problem at precision \\(\\epsilon=0.1\\)? RQ2 What is the impact of noise in the probability of success of each algorithm? 3.1 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&quot;./data/statscomp.csv&quot;) Let’s select only the columns that interests us. Note that some of the columns have \"\" because of the “-” in the column name dataset &lt;- select(dataset, Algorithm, CostFunction, SD, MaxFevalPerDimensions, simNumber, SolveAt1, &quot;SolveAt1e-1&quot;, &quot;SolveAt1e-3&quot;, &quot;SolveAt1e-6&quot;, OptimizationSuccessful) Let’s do some basic transformation 1 - We select only the cases where the optimization completed 2 - We convert True to 1 and 0 to false 3 - We group by the algorithms, functions, SD, and budget so we can summarize and create aggregated data 4 - We create an index of each algorithm and the cost functions. This is basically creating a map of NelderMead=1, PSO=2 etc… This makes things easier to work in Stan. For that we use the function create_index from the utils.R file 5 - We drop the columns we wont use 6 - Get an array with the names of the benchmark functions and the algorithms (to create nicer plots later with lengend) Since we are only looking at 1e-1 for the precision we comment the other lines d &lt;- dataset %&gt;% dplyr::filter(OptimizationSuccessful == TRUE) %&gt;% dplyr::mutate(solvedAt1e1 = as.integer(dataset$&quot;SolveAt1e-1&quot;), budget = MaxFevalPerDimensions) %&gt;% dplyr::group_by(Algorithm, CostFunction, SD, budget) %&gt;% dplyr::summarize(solvedAt1e1 = sum(solvedAt1e1), N = n()) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(AlgorithmID = create_index(Algorithm), CostFunctionID = create_index(CostFunction)) %&gt;% dplyr::select(Algorithm, AlgorithmID, CostFunction, CostFunctionID, SD, budget, N, y = solvedAt1e1, ) # List of algorithms bm &lt;- get_index_names_as_array(d$CostFunction) algorithms &lt;- get_index_names_as_array(d$Algorithm) Lets preview a sample of the data set kable(dplyr::sample_n(d, size = 10), &quot;html&quot;, booktabs = T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithm AlgorithmID CostFunction CostFunctionID SD budget N y RandomSearch1 6 Damavandi 5 0 20 10 0 NelderMead 4 Schwefel2d26N6 19 3 100000 10 0 RandomSearch1 6 RosenbrockRotatedN6 14 0 100000 10 0 PSO 5 Schwefel2d4N6 20 0 10000 10 4 NelderMead 4 SalomonN2 15 3 20 10 0 RandomSearch2 7 Trefethen 25 0 20 10 0 RandomSearch2 7 XinSheYang2N2 29 3 100 10 0 RandomSearch2 7 PinterN6 11 0 100 10 0 RandomSearch1 6 ThreeHumpCamelBack 24 0 100 10 7 CMAES 1 Schwefel2d21N6 17 0 100000 10 10 3.2 Stan model The Stan model is specified in the file: './stanmodels/probsuccess.stan' print_stan_code(&quot;./stanmodels/probsuccess.stan&quot;) // Probability of success model // Author: David Issa Mattos // Date: 16 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size int y[N_total]; // Result of the binomial int N_draw[N_total]; // Number of draws in the binomial real x_noise[N_total];//predictor for noise //To model each algorithm independently int &lt;lower=1&gt; N_algorithm; // Number of algorithms int algorithm_id[N_total]; //vector that has the id of each algorithm //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { //Fixed effect real a_alg[N_algorithm];//the mean effect given by the algorithms real b_noise[N_algorithm];//slope for the noise // //Random effect. The effect of the benchmarks real a_bm_norm[N_bm];//the mean effect given by the base class type real&lt;lower=0&gt; s;//std for the random effects } model { real p[N_total]; //Fixed effect a_alg ~ normal(0,1); b_noise ~ normal(0,1); // //Random effects s ~ exponential(1); a_bm_norm ~ normal(0,1); for (i in 1:N_total) { p[i] = a_alg[algorithm_id[i]]+ a_bm_norm[bm_id[i]]*s + b_noise[algorithm_id[i]] * x_noise[i]; } //Equivalent to: y~binomial(N, inverse_logit(a+bx=alpha)) y ~ binomial_logit(N_draw,p); } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/probsuccsess-data.RDS\" standata &lt;- list(N_total = nrow(d), y = d$y, N_draw = d$N, x_noise = d$SD, N_algorithm = length(algorithms), algorithm_id = d$AlgorithmID, N_bm = length(bm), bm_id = d$CostFunctionID) saveRDS(standata, file = &quot;./data/probsuccsess-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately standata &lt;- readRDS(&quot;./data/probsuccsess-data.RDS&quot;) probsuccess.fit &lt;- stan(file = &quot;./stanmodels/probsuccess.stan&quot;, data = standata, chains = 4, warmup = 200, iter = 2000) saveRDS(probsuccess.fit, file = &quot;./data/probsuccsess-fit.RDS&quot;) 3.3 Diagnosis The first step is to evaluate the convergence of the chains. We will look now only for the slopes, algorithms intercept and the standard deviation of the random effects a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;, &quot;a_alg[5]&quot;, &quot;a_alg[6]&quot;, &quot;a_alg[7]&quot;, &quot;a_alg[8]&quot;) b_noise &lt;- c(&quot;b_noise[1]&quot;, &quot;b_noise[2]&quot;, &quot;b_noise[3]&quot;, &quot;b_noise[4]&quot;, &quot;b_noise[5]&quot;, &quot;b_noise[6]&quot;, &quot;b_noise[7]&quot;, &quot;b_noise[8]&quot;) rstan::traceplot(probsuccess.fit, pars = a_alg) rstan::traceplot(probsuccess.fit, pars = b_noise) rstan::traceplot(probsuccess.fit, pars = c(&quot;s&quot;)) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.01 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(probsuccess.fit)$summary) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] 0.9946071 0.0148192 0.3290572 0.3618872 0.7721788 0.9866326 1.2147485 1.6573538 493.0550 1.0036977 a_alg[2] -1.4321112 0.0148480 0.3340389 -2.0771512 -1.6564318 -1.4395342 -1.2113320 -0.7558657 506.1224 1.0034682 a_alg[3] 0.8838772 0.0147919 0.3291742 0.2551902 0.6568310 0.8759522 1.1036079 1.5423992 495.2249 1.0033757 a_alg[4] -3.1850027 0.0145669 0.3454132 -3.8454788 -3.4179437 -3.1915344 -2.9594851 -2.4831115 562.2713 1.0028309 a_alg[5] 0.7028661 0.0149930 0.3305516 0.0671596 0.4751224 0.6970125 0.9255912 1.3781179 486.0700 1.0036165 a_alg[6] -1.4422315 0.0148736 0.3342055 -2.0748216 -1.6725905 -1.4474044 -1.2200929 -0.7810854 504.8892 1.0030701 a_alg[7] -0.8948662 0.0147322 0.3306491 -1.5328720 -1.1188970 -0.9022392 -0.6729730 -0.2203865 503.7352 1.0030168 a_alg[8] -1.1009767 0.0147796 0.3326966 -1.7430722 -1.3272842 -1.1045184 -0.8799628 -0.4279465 506.7227 1.0034475 b_noise[1] -1.2633976 0.0005300 0.0459951 -1.3541275 -1.2943863 -1.2629590 -1.2316701 -1.1753833 7530.4952 0.9997559 b_noise[2] -0.8099152 0.0007618 0.0596258 -0.9301217 -0.8503491 -0.8098407 -0.7689690 -0.6947710 6126.8076 0.9998526 b_noise[3] -1.3428084 0.0006179 0.0504481 -1.4425540 -1.3763821 -1.3417232 -1.3093450 -1.2463675 6665.2114 1.0005741 b_noise[4] -0.3988165 0.0008430 0.0713629 -0.5403377 -0.4468691 -0.3988481 -0.3495032 -0.2631669 7165.6523 0.9996902 b_noise[5] -1.1391536 0.0005661 0.0468626 -1.2323387 -1.1698545 -1.1391343 -1.1064296 -1.0489285 6851.9886 1.0001047 b_noise[6] -0.7920796 0.0007000 0.0585343 -0.9083656 -0.8306712 -0.7911704 -0.7529028 -0.6788131 6992.1559 0.9996895 b_noise[7] -0.9597663 0.0006727 0.0565742 -1.0723302 -0.9982250 -0.9584665 -0.9211403 -0.8522974 7072.0237 0.9998860 b_noise[8] -0.7202239 0.0005731 0.0512429 -0.8212971 -0.7539867 -0.7204663 -0.6863093 -0.6199107 7995.2269 1.0000129 a_bm_norm[1] -0.9527650 0.0058266 0.1640450 -1.2986550 -1.0574763 -0.9449164 -0.8405614 -0.6556160 792.6796 1.0007526 a_bm_norm[2] -1.7380809 0.0092187 0.2825468 -2.3365043 -1.9177435 -1.7267402 -1.5394417 -1.2274041 939.3784 1.0007376 a_bm_norm[3] -1.0105804 0.0059796 0.1701593 -1.3624826 -1.1232509 -1.0009202 -0.8910774 -0.6980189 809.7860 1.0007038 a_bm_norm[4] 0.2285718 0.0071118 0.1511781 -0.0474446 0.1236682 0.2220277 0.3239097 0.5546319 451.8765 1.0052771 a_bm_norm[5] -1.2979137 0.0071315 0.2068445 -1.7327485 -1.4332758 -1.2871085 -1.1531152 -0.9203510 841.2615 1.0010337 a_bm_norm[6] -0.5507772 0.0051477 0.1329175 -0.8274771 -0.6393456 -0.5449455 -0.4606890 -0.2952643 666.7008 1.0006213 a_bm_norm[7] 1.5473018 0.0145426 0.3063036 0.9970505 1.3284599 1.5297401 1.7409851 2.2044622 443.6269 1.0066939 a_bm_norm[8] 0.9056046 0.0106608 0.2225983 0.5103041 0.7481791 0.8917652 1.0480799 1.3821662 435.9790 1.0067902 a_bm_norm[9] -2.5626916 0.0098359 0.4670434 -3.5955508 -2.8496381 -2.5244053 -2.2364610 -1.7583805 2254.6803 1.0004011 a_bm_norm[10] -0.3950607 0.0051346 0.1278234 -0.6601937 -0.4780462 -0.3914228 -0.3109385 -0.1480920 619.7451 1.0014863 a_bm_norm[11] -0.8740592 0.0055704 0.1554762 -1.1997542 -0.9759484 -0.8679696 -0.7659328 -0.5848577 779.0311 1.0001963 a_bm_norm[12] 0.4838904 0.0083772 0.1750863 0.1753386 0.3569926 0.4736147 0.5964012 0.8564897 436.8238 1.0064032 a_bm_norm[13] -0.3262289 0.0052969 0.1280675 -0.5837855 -0.4113456 -0.3231585 -0.2420871 -0.0702349 584.5571 1.0013819 a_bm_norm[14] -0.8278555 0.0055364 0.1519790 -1.1457247 -0.9256599 -0.8196604 -0.7241520 -0.5501589 753.5365 1.0001462 a_bm_norm[15] -0.6318079 0.0050920 0.1383457 -0.9195265 -0.7194981 -0.6271263 -0.5391648 -0.3673415 738.1523 1.0002242 a_bm_norm[16] -0.3773009 0.0051681 0.1279449 -0.6353464 -0.4607384 -0.3736860 -0.2931876 -0.1271949 612.9012 1.0015522 a_bm_norm[17] -0.5210612 0.0052287 0.1336631 -0.7950268 -0.6089748 -0.5165403 -0.4308585 -0.2673398 653.4800 1.0008857 a_bm_norm[18] -0.0956786 0.0058456 0.1323205 -0.3513172 -0.1820726 -0.0995839 -0.0109864 0.1746857 512.3876 1.0034467 a_bm_norm[19] -1.7358133 0.0093561 0.2823701 -2.3252919 -1.9162240 -1.7227131 -1.5343894 -1.2305982 910.8451 1.0014322 a_bm_norm[20] -0.6333188 0.0052434 0.1368810 -0.9181768 -0.7203772 -0.6283925 -0.5403273 -0.3761509 681.4834 1.0003449 a_bm_norm[21] -0.3957515 0.0052231 0.1284832 -0.6575543 -0.4778521 -0.3929083 -0.3111569 -0.1480475 605.1188 1.0012406 a_bm_norm[22] -0.0292625 0.0060526 0.1344119 -0.2860116 -0.1204862 -0.0327993 0.0576141 0.2439762 493.1711 1.0033417 a_bm_norm[23] -0.6411656 0.0051955 0.1380644 -0.9247816 -0.7307958 -0.6353042 -0.5463581 -0.3839083 706.1671 1.0002261 a_bm_norm[24] 0.4336104 0.0081215 0.1702392 0.1342273 0.3126054 0.4207068 0.5429854 0.8043065 439.3813 1.0059156 a_bm_norm[25] -0.8869217 0.0057197 0.1574404 -1.2162575 -0.9882524 -0.8796199 -0.7783999 -0.6002996 757.6807 1.0002479 a_bm_norm[26] 1.2805502 0.0128771 0.2704397 0.7980406 1.0887761 1.2651200 1.4519296 1.8662452 441.0660 1.0069973 a_bm_norm[27] -0.8046715 0.0054749 0.1504996 -1.1256752 -0.9031270 -0.7973055 -0.7013677 -0.5251334 755.6355 1.0001749 a_bm_norm[28] -1.9196463 0.0103139 0.3206570 -2.6137884 -2.1209950 -1.8987504 -1.6968301 -1.3506281 966.5842 1.0008691 a_bm_norm[29] 0.0145819 0.0061994 0.1365363 -0.2396681 -0.0764639 0.0072865 0.1033810 0.2964588 485.0618 1.0038566 a_bm_norm[30] 0.3032353 0.0073839 0.1574109 0.0243495 0.1901417 0.2937836 0.4037032 0.6397283 454.4676 1.0054226 s 2.5722159 0.0166277 0.3933513 1.9291118 2.2970962 2.5255790 2.8119336 3.4566604 559.6244 1.0044481 lp__ -5616.6387287 0.2391257 6.3573450 -5630.2796222 -5620.6002220 -5616.1926901 -5612.2277348 -5605.2607227 706.8042 1.0037792 3.4 Results and Plots First lets get the HPDI of every parameter. Then we restrict to the algorithms, them to the slopes, then to the hpdi &lt;- get_HPDI_from_stanfit(probsuccess.fit) hpdi_oddsratio &lt;- hpdi hpdi_oddsratio$Mean &lt;- exp(hpdi$Mean) hpdi_oddsratio$HPDI.lower &lt;- exp(hpdi$HPDI.lower) hpdi_oddsratio$HPDI.higher &lt;- exp(hpdi$HPDI.higher) hpdi_oddsratio_algorithm &lt;- hpdi_oddsratio %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter = algorithms) #Changing to the algorithms labels hpdi_oddsratio_b_noise &lt;- hpdi_oddsratio %&gt;% dplyr::filter(str_detect(Parameter, &quot;b_noise\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter = algorithms) #Changing to the algorithms labels hpdi_s &lt;- hpdi %&gt;% dplyr::filter(Parameter == &quot;s&quot;) p_alg &lt;- ggplot(data = hpdi_oddsratio_algorithm, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(title = &quot;Algorithm intercept&quot;, y = &quot;Odds ratio&quot;, x = &quot;Algorithm&quot;) + coord_flip() p_alg + plot_annotation(title = &quot;HPDI interval for the algorithms odd ratio&quot;) p_noise &lt;- ggplot(data = hpdi_oddsratio_b_noise, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(title = &quot;Noise coefficient&quot;, y = &quot;Odds ratio&quot;, x = &quot;&quot;) + coord_flip() + theme(axis.text.y = element_blank()) p_noise + plot_annotation(title = &quot;HPDI interval for the noise coefficients odd ratio&quot;) p_s &lt;- ggplot(data = hpdi_s, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(title = &quot;Std benchmarks&quot;, y = &quot;Estimate&quot;, x = &quot;Parameter&quot;) + coord_flip() p_s + plot_annotation(title = &quot;HPDI interval for s&quot;) "],
["relative-improvement.html", "Chapter 4 Relative improvement 4.1 Data preparation 4.2 Stan model 4.3 Diagnosis 4.4 Results and Plots", " Chapter 4 Relative improvement Our next, model deals with relative improvement of the algorithms over a baseline in noiseless functions. This model is based on a normal linear regression. RQ What is the expected improvement of these algorithms against the Random Search in noiseless benchmark functions? For this question we will consider the Euclidean distance to the location of the closest global minima, instead of the final reward difference. 4.1 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&quot;./data/statscomp.csv&quot;) Let’s select only the columns that interests us, in this case the Euclidean distance d &lt;- dataset %&gt;% dplyr::select(Algorithm, CostFunction, SD, Budget = MaxFevalPerDimensions, simNumber, EuclideanDistance, OptimizationSuccessful) %&gt;% dplyr::filter(OptimizationSuccessful &amp; SD == 0) %&gt;% dplyr::select(-SD, -OptimizationSuccessful) Let’s first make this a wide data set based on the algorithm to make it easier to compute the relative improvement over the Random Search. We are also dropping the RandomSearch2 since there is no noise in the benchmark functions There are several ways that can be used to compute a relative improvement (and they will affect the result). The way we are using is to compare against the mean of distance of the 10 samples of the Random Search in each cost function for a specific budget. The way we are comparing is we divide the distance of each algorithm by the average distance of the random search. If this ratio is greater than 1 then random search is better, if smaller than 1 then the algorithm is better relativeImprovement &lt;- function(x, rs) { # x is the column rs is the random search column ri &lt;- (rs - x)/rs ri &lt;- ifelse(ri &lt; -1, -1, ri) ri &lt;- ifelse(ri &gt; 1, 1, ri) return(ri) } d_wide &lt;- d %&gt;% tidyr::pivot_wider(names_from = Algorithm, values_from = EuclideanDistance) %&gt;% dplyr::select(-RandomSearch2) %&gt;% dplyr::group_by(CostFunction, Budget) %&gt;% dplyr::mutate(avgRandomSearch = mean(RandomSearch1)) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate_at(c(&quot;NelderMead&quot;, &quot;PSO&quot;, &quot;SimulatedAnnealing&quot;, &quot;CuckooSearch&quot;, &quot;DifferentialEvolution&quot;, &quot;CMAES&quot;), ~relativeImprovement(.x, rs = avgRandomSearch)) After we compute our metric we drop the Random Search column and we pivot_longer again to make the inference d_final &lt;- d_wide %&gt;% dplyr::select(-RandomSearch1, -avgRandomSearch) %&gt;% tidyr::pivot_longer(cols = c(&quot;NelderMead&quot;, &quot;PSO&quot;, &quot;SimulatedAnnealing&quot;, &quot;CuckooSearch&quot;, &quot;DifferentialEvolution&quot;, &quot;CMAES&quot;), names_to = &quot;Algorithm&quot;, values_to = &quot;y&quot;) %&gt;% dplyr::select(-simNumber) %&gt;% dplyr::mutate(AlgorithmID = create_index(Algorithm), CostFunctionID = create_index(CostFunction)) %&gt;% dplyr::select(Algorithm, AlgorithmID, CostFunction, CostFunctionID, Budget, y) # checking if there is any na -&gt; stan does not accept that find.na &lt;- d_final %&gt;% dplyr::filter(is.na(y)) bm &lt;- get_index_names_as_array(d_final$CostFunction) algorithms &lt;- get_index_names_as_array(d_final$Algorithm) Now we have our final dataset to use with Stan. Lets preview a sample of the data set kable(dplyr::sample_n(d_final, size = 10), &quot;html&quot;, booktabs = T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithm AlgorithmID CostFunction CostFunctionID Budget y NelderMead 4 RosenbrockRotatedN6 14 1000 -1.000 CMAES 1 Schwefel2d20N2 16 1000 1.000 CuckooSearch 2 ChenV 3 10000 0.210 NelderMead 4 StrechedVSineWave2N 23 20 -1.000 DifferentialEvolution 3 ChenBird 2 100000 -1.000 NelderMead 4 ExponentialN2 7 100000 -0.664 PSO 5 Tripod 27 100 0.897 CuckooSearch 2 LunacekBiRastriginN6 9 10000 -0.683 NelderMead 4 ThreeHumpCamelBack 24 100 -0.687 DifferentialEvolution 3 Schwefel2d21N6 17 10000 1.000 4.2 Stan model The Stan model is specified in the file: './stanmodels/relativeimprovement.stan' print_stan_code(&quot;./stanmodels/relativeimprovement.stan&quot;) // Relative improvement model // Author: David Issa Mattos // Date: 17 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size real y[N_total]; // relative improvement variable //To model each algorithm independently int &lt;lower=1&gt; N_algorithm; // Number of algorithms int algorithm_id[N_total]; //vector that has the id of each algorithm //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { real &lt;lower=0&gt; sigma;//std for the normal //Fixed effect real a_alg[N_algorithm];//the mean effect given by the algorithms // //Random effect. The effect of the benchmarks real a_bm_norm[N_bm];//the mean effect given by the base class type real&lt;lower=0&gt; s;//std for the random effects } model { real mu[N_total]; sigma ~ exponential(1); //Fixed effect a_alg ~ normal(0,1); // //Random effects s ~ exponential(1); a_bm_norm ~ normal(0,1); for (i in 1:N_total) { mu[i] = a_alg[algorithm_id[i]] + a_bm_norm[bm_id[i]]*s; } y ~ normal(mu, sigma); } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/relativeimprovement-data.RDS\" standata &lt;- list(N_total = nrow(d_final), y = d_final$y, N_algorithm = length(algorithms), algorithm_id = d_final$AlgorithmID, N_bm = length(bm), bm_id = d_final$CostFunctionID) saveRDS(standata, file = &quot;./data/relativeimprovement-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately standata &lt;- readRDS(&quot;./data/relativeimprovement-data.RDS&quot;) relativeimprovement.fit &lt;- stan(file = &quot;./stanmodels/relativeimprovement.stan&quot;, data = standata, chains = 4, warmup = 200, iter = 2000) saveRDS(relativeimprovement.fit, file = &quot;./data/relativeimprovement-fit.RDS&quot;) 4.3 Diagnosis a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;, &quot;a_alg[5]&quot;, &quot;a_alg[6]&quot;) rstan::traceplot(relativeimprovement.fit, pars = a_alg) rstan::traceplot(relativeimprovement.fit, pars = c(&quot;s&quot;, &quot;sigma&quot;)) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.01 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(relativeimprovement.fit)$summary) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat sigma 0.6367136 0.0000457 0.0047811 0.6274032 0.6335385 0.6366453 0.6398819 0.6462761 10932.6669 0.9997383 a_alg[1] 0.1528724 0.0011168 0.0323725 0.0882285 0.1315695 0.1537696 0.1743541 0.2147639 840.2495 1.0058980 a_alg[2] -0.3757817 0.0011411 0.0324419 -0.4395244 -0.3972668 -0.3758854 -0.3542998 -0.3120183 808.3258 1.0073515 a_alg[3] 0.3040463 0.0010979 0.0321423 0.2413520 0.2826737 0.3040526 0.3258091 0.3661104 857.1476 1.0068383 a_alg[4] -0.6389961 0.0011438 0.0323862 -0.7034160 -0.6603248 -0.6383835 -0.6171854 -0.5766122 801.6916 1.0059957 a_alg[5] 0.3221519 0.0011488 0.0321185 0.2586531 0.3006443 0.3220550 0.3437193 0.3849371 781.6795 1.0053256 a_alg[6] -0.5685966 0.0011482 0.0325380 -0.6323306 -0.5898365 -0.5682307 -0.5467721 -0.5056506 803.0586 1.0063050 a_bm_norm[1] -0.5528284 0.0083841 0.3167051 -1.1996023 -0.7578486 -0.5461023 -0.3393093 0.0522052 1426.9037 1.0038626 a_bm_norm[2] -2.1898502 0.0113977 0.4119069 -3.0368446 -2.4563185 -2.1700033 -1.9085664 -1.4215276 1306.0717 1.0041856 a_bm_norm[3] 0.8438667 0.0080493 0.3315706 0.2222905 0.6137846 0.8362845 1.0601660 1.5239791 1696.8102 1.0023274 a_bm_norm[4] 0.1326418 0.0076929 0.3098639 -0.4611369 -0.0784186 0.1286970 0.3417231 0.7405153 1622.4335 1.0027501 a_bm_norm[5] -1.1574387 0.0089213 0.3436102 -1.8558549 -1.3858565 -1.1460847 -0.9203579 -0.5105154 1483.4530 1.0032709 a_bm_norm[6] 0.8473595 0.0077491 0.3310120 0.2198905 0.6176796 0.8400744 1.0664735 1.5069308 1824.6630 1.0020777 a_bm_norm[7] -0.0610561 0.0085946 0.3144614 -0.6810709 -0.2755458 -0.0583310 0.1466462 0.5439426 1338.7012 1.0049663 a_bm_norm[8] 1.1217552 0.0080232 0.3420421 0.4738809 0.8903396 1.1093751 1.3444527 1.8245162 1817.4716 1.0023594 a_bm_norm[9] 0.4668407 0.0078682 0.3195200 -0.1380094 0.2474110 0.4566927 0.6766766 1.1142890 1649.0941 1.0023276 a_bm_norm[10] 0.3172916 0.0077608 0.3189086 -0.2935497 0.0990584 0.3126447 0.5271420 0.9585043 1688.5619 1.0021347 a_bm_norm[11] 0.8617476 0.0082667 0.3297373 0.2358761 0.6354238 0.8532483 1.0858978 1.5127949 1591.0008 1.0032300 a_bm_norm[12] 0.6566616 0.0078074 0.3235731 0.0477075 0.4366450 0.6465615 0.8677998 1.3043995 1717.6193 1.0024749 a_bm_norm[13] -1.7662785 0.0105775 0.3755665 -2.5312320 -2.0139812 -1.7539172 -1.5087779 -1.0539527 1260.6829 1.0043260 a_bm_norm[14] 0.8799962 0.0080830 0.3285285 0.2494554 0.6609048 0.8717198 1.0949806 1.5441091 1651.9503 1.0028887 a_bm_norm[15] -1.0860694 0.0089953 0.3355051 -1.7640085 -1.3054263 -1.0771789 -0.8600993 -0.4451459 1391.1255 1.0035691 a_bm_norm[16] 0.0960651 0.0072150 0.3106989 -0.4885144 -0.1189339 0.0918543 0.3064252 0.7194358 1854.4275 1.0022760 a_bm_norm[17] 0.8120032 0.0078637 0.3262759 0.1982053 0.5919244 0.8021110 1.0258392 1.4730756 1721.5552 1.0032659 a_bm_norm[18] 0.7555947 0.0083187 0.3336851 0.1286890 0.5315805 0.7432168 0.9694233 1.4395282 1609.0170 1.0024624 a_bm_norm[19] -0.5861159 0.0080184 0.3212106 -1.2205327 -0.8040274 -0.5858222 -0.3705707 0.0440855 1604.7485 1.0026585 a_bm_norm[20] 0.0731471 0.0079307 0.3164780 -0.5347566 -0.1350642 0.0726553 0.2799041 0.7101974 1592.4387 1.0026327 a_bm_norm[21] -0.5713165 0.0083904 0.3161689 -1.1905242 -0.7827417 -0.5701083 -0.3607944 0.0425663 1419.9437 1.0041760 a_bm_norm[22] 1.1639801 0.0081842 0.3445592 0.5154897 0.9237753 1.1561923 1.3978981 1.8615953 1772.4412 1.0028106 a_bm_norm[23] -1.2825067 0.0095313 0.3498452 -1.9944274 -1.5092287 -1.2707083 -1.0428518 -0.6255937 1347.2624 1.0046445 a_bm_norm[24] 0.2527552 0.0076225 0.3157048 -0.3578683 0.0430652 0.2507233 0.4669514 0.8864676 1715.3991 1.0028007 a_bm_norm[25] -0.7524686 0.0084671 0.3275641 -1.4141219 -0.9697545 -0.7476515 -0.5245629 -0.1250794 1496.6702 1.0027062 a_bm_norm[26] 1.3912052 0.0091255 0.3635091 0.7033080 1.1367450 1.3827850 1.6295467 2.1351611 1586.7690 1.0023171 a_bm_norm[27] -1.0996624 0.0090897 0.3404926 -1.7931082 -1.3260065 -1.0905193 -0.8685259 -0.4454558 1403.1947 1.0035552 a_bm_norm[28] -0.3881069 0.0077362 0.3130439 -1.0123018 -0.5938400 -0.3877528 -0.1802449 0.2239530 1637.3854 1.0030598 a_bm_norm[29] -0.5438003 0.0078397 0.3152240 -1.1690190 -0.7533013 -0.5436276 -0.3304766 0.0660344 1616.7290 1.0026129 a_bm_norm[30] 1.0874127 0.0085530 0.3444976 0.4601920 0.8502100 1.0742699 1.3177185 1.8022325 1622.3188 1.0033795 s 0.1450795 0.0006307 0.0215598 0.1099735 0.1298466 0.1424160 0.1578436 0.1937342 1168.6901 1.0044734 lp__ -454.4377641 0.1720569 5.8632921 -467.1570738 -458.1997270 -454.0645231 -450.4068103 -444.0162036 1161.2856 1.0050157 4.4 Results and Plots First lets get the HPDI of every parameter. Then we restrict to the algorithms, them to the slopes, then to the hpdi &lt;- get_HPDI_from_stanfit(relativeimprovement.fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter = algorithms) #Changing to the algorithms labels hpdi_other_parameters &lt;- hpdi %&gt;% dplyr::filter(Parameter == &quot;s&quot; | Parameter == &quot;sigma&quot;) p_alg &lt;- ggplot(data = hpdi_algorithm, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(y = &quot;Estimate&quot;, x = &quot;Algorithm&quot;) + coord_flip() p_alg + plot_annotation(title = &quot;HPDI interval for the algorithms&quot;) p_others &lt;- ggplot(data = hpdi_other_parameters, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(y = &quot;Estimate&quot;, x = &quot;Parameter&quot;) + coord_flip() p_others + plot_annotation(title = &quot;HPDI interval&quot;) "],
["ranking.html", "Chapter 5 Ranking 5.1 Data preparation 5.2 Stan model 5.3 Diagnosis 5.4 Results and Plots", " Chapter 5 Ranking In this section, we will consider the Bradley-Terry Model for ranking algorithms in the fixed budget of 10,000 function evaluations per dimension and controlling for noise and the effect of benchmark functions How can we rank algorithm different optimization algorithms given a budget of 10,000 evaluations per dimension in noisy benchmarks? 5.1 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&quot;./data/statscomp.csv&quot;) The BT model formulation that we use has a specific data format, where we have one column with algo_0 (with index of each algorithm) another column with algo_1 and a third column with who won (algo 0 or algo 1), First lets select only the data that we are interested and create ranking by the each run in each group (by the simNumber). To avoid ties (dealing with those on next session) we will rank ties randomly d1 &lt;- dataset %&gt;% dplyr::select(Algorithm, CostFunction, SD, Budget = MaxFevalPerDimensions, simNumber, TrueRewardDifference, OptimizationSuccessful) %&gt;% dplyr::filter(OptimizationSuccessful &amp; Budget == 10000 &amp; SD == 3) %&gt;% dplyr::select(-Budget, -OptimizationSuccessful, -SD) %&gt;% dplyr::group_by(CostFunction, simNumber) %&gt;% dplyr::mutate(rankReward = rank(TrueRewardDifference, ties.method = &quot;random&quot;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-TrueRewardDifference) kable(dplyr::sample_n(d1, size = 10), booktabs = T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithm CostFunction simNumber rankReward CMAES Giunta 0 4 CMAES ThreeHumpCamelBack 3 4 PSO SalomonN2 2 4 CMAES DiscusN2 7 7 RandomSearch1 ExponentialN2 7 2 RandomSearch2 Trigonometric1N6 7 1 DifferentialEvolution Schwefel2d26N6 8 1 SimulatedAnnealing Schwefel2d21N6 3 7 CuckooSearch Schwefel2d20N2 8 2 RandomSearch1 Giunta 8 3 Now to compare the ranks we need to pivot wider the data frame and based on that we will expand to the dataset in the appropriated format d1_wide &lt;- d1 %&gt;% tidyr::pivot_wider(names_from = Algorithm, values_from = rankReward) kable(dplyr::sample_n(d1_wide, size = 10), booktabs = T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) CostFunction simNumber NelderMead PSO SimulatedAnnealing CuckooSearch DifferentialEvolution RandomSearch1 RandomSearch2 CMAES Tripod 5 7 2 6 3 1 5 4 8 StrechedVSineWave2N 9 7 5 1 6 3 4 8 2 SphereN6 1 7 1 8 6 2 5 4 3 BentCigarN6 8 8 3 7 5 2 6 4 1 LunacekBiRastriginN6 3 8 1 7 6 2 5 4 3 LunacekBiRastriginN6 8 8 2 7 4 3 5 6 1 ChenV 0 4 1 5 6 2 3 8 7 SphereN6 2 8 1 6 7 2 5 4 3 Damavandi 9 8 6 7 1 2 4 3 5 QingN2 3 7 2 8 1 4 5 6 3 Now we need to modify this data set and expand it so we have the pairwise comparisons First let’s get the number of algorithms and create combination of all possible 2 by 2 comparisons without repeating algorithms &lt;- get_index_names_as_array(d1$Algorithm) n_algorithms &lt;- length(algorithms) comb &lt;- gtools::combinations(n = n_algorithms, r = 2, v = seq(1:n_algorithms), repeats.allowed = F) The pairs combinations looks like this (for algo_0 and algo_1): kable(comb, booktabs = T) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) 1 2 1 3 1 4 1 5 1 6 1 7 1 8 2 3 2 4 2 5 2 6 2 7 2 8 3 4 3 5 3 6 3 7 3 8 4 5 4 6 4 7 4 8 5 6 5 7 5 8 6 7 6 8 7 8 Note that each row of d_wide will be expanded into 28 rows. Giving a dataset with a total of 8400 rows. The following code can a bit slow to run due to the double for loops (there is probably a way to vectorize this and make it run faster), but for building this appendix we will not run, instead we will run it once, save this data, and load it when needed. It takes a couple of minutes but if you have a lot of data and algorithms it can easily go for hours We will use a progress bar to follow the data frame creation. 1- We initialize a tibble data frame 2- First we loop through the wide data frame d1_wide row by row 3- For each row we will loop through the different combinations in the comb variable to create the rows of the data frame. We add each row to the initial dataframe pb &lt;- progress::progress_bar$new(format = &quot;[:bar] :current/:total (:percent)&quot;, total = nrow(d1_wide)) df_out &lt;- dplyr::tribble(~algo0_name, ~algo0, ~algo1_name, ~algo1, ~y, ~simNumber, ~CostFunction) for (i in 1:nrow(d1_wide)) { current_row &lt;- d1_wide[i, ] for (j in 1:nrow(comb)) { comb_row &lt;- comb[j, ] algo0_name &lt;- algorithms[comb_row[1]] algo0 &lt;- comb_row[1] algo0_rank &lt;- current_row[[1, algo0_name]] algo1_name &lt;- algorithms[comb_row[2]] algo1 &lt;- comb_row[2] algo1_rank &lt;- current_row[[1, algo1_name]] diff_rank &lt;- algo1_rank - algo0_rank y &lt;- ifelse(diff_rank &lt; 0, 1, 0) df_out &lt;- add_row(df_out, algo0_name = algo0_name, algo0 = algo0, algo1_name = algo1_name, algo1 = algo1, y = y, simNumber = current_row$simNumber, CostFunction = current_row$CostFunction) } pb$tick() } saveRDS(df_out, file = &quot;./data/ranking.RDS&quot;) Visualizing how the data frame looks like df_out &lt;- readRDS(&quot;./data/ranking.RDS&quot;) kable(dplyr::sample_n(df_out, size = 10), &quot;html&quot;, booktabs = T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) algo0_name algo0 algo1_name algo1 y simNumber CostFunction CMAES 1 RandomSearch2 7 1 1 ChenBird CuckooSearch 2 RandomSearch1 6 0 5 Damavandi RandomSearch2 7 SimulatedAnnealing 8 1 0 Price1 CuckooSearch 2 RandomSearch2 7 1 5 Schwefel2d4N6 CuckooSearch 2 PSO 5 1 3 XinSheYang2N2 CuckooSearch 2 DifferentialEvolution 3 1 1 ExponentialN2 PSO 5 SimulatedAnnealing 8 0 0 Tripod DifferentialEvolution 3 PSO 5 1 4 XinSheYang2N2 NelderMead 4 PSO 5 1 5 PinterN6 DifferentialEvolution 3 RandomSearch2 7 0 4 Price1 5.2 Stan model The Stan model is specified in the file: './stanmodels/rankingmodel.stan' print_stan_code(&quot;./stanmodels/rankingmodel.stan&quot;) // Relative improvement model // Author: David Issa Mattos // Date: 22 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size int y[N_total]; //variable that indicates which one wins algo0 oor algo 1 int &lt;lower=1&gt; N_algorithm; // Number of algorithms int &lt;lower=1&gt; algo0[N_total]; int &lt;lower=1&gt; algo1[N_total]; // //To model the influence of each benchmark // int &lt;lower=1&gt; N_bm; // int bm_id[N_total]; } parameters { real alpha[N_algorithm]; //Latent variable that represents the strength value of each algorithm } model { real p[N_total]; alpha ~ normal(0,1); for (i in 1:N_total) { p[i] = alpha[algo1[i]] - alpha[algo0[i]]; } y ~ bernoulli_logit(p); } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/rankingmodel-data.RDS\" standata &lt;- list(N_total = nrow(df_out), y = as.integer(df_out$y), N_algorithm = length(algorithms)) saveRDS(standata, file = &quot;./data/rankingmodel-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately standata &lt;- readRDS(&quot;./data/rankingmodel-data.RDS&quot;) ranking.fit &lt;- stan(file = &quot;./stanmodels/rankingmodel.stan&quot;, data = standata, chains = 4, warmup = 200, iter = 2000) saveRDS(ranking.fit, file = &quot;./data/ranking-fit.RDS&quot;) 5.3 Diagnosis rstan::traceplot(ranking.fit) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.01 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(ranking.fit)$summary) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat alpha[1] 0.3626756 0.0151637 0.3641695 -0.3629823 0.1389186 0.3488488 0.5912687 1.0824663 576.7647 1.003887 alpha[2] -0.2224683 0.0151268 0.3639187 -0.9477361 -0.4489886 -0.2382599 0.0118836 0.5074189 578.7779 1.003920 alpha[3] 0.9617232 0.0151831 0.3643322 0.2364690 0.7334038 0.9439163 1.1947268 1.6845083 575.8033 1.004121 alpha[4] -1.5770602 0.0151162 0.3654682 -2.3031634 -1.8088471 -1.5899079 -1.3410112 -0.8420079 584.5399 1.003991 alpha[5] 0.8573993 0.0151242 0.3638385 0.1291004 0.6320486 0.8404011 1.0899170 1.5817985 578.7264 1.004126 alpha[6] 0.2022339 0.0151529 0.3645409 -0.5289829 -0.0256225 0.1887022 0.4332110 0.9226121 578.7643 1.004032 alpha[7] 0.2251329 0.0151701 0.3641272 -0.4947415 -0.0043264 0.2085596 0.4596052 0.9483425 576.1434 1.004048 alpha[8] -0.7457437 0.0151858 0.3647564 -1.4661950 -0.9763557 -0.7634038 -0.5141994 -0.0245150 576.9371 1.004337 lp__ -4763.4052788 0.0508498 2.0014643 -4768.2690741 -4764.4978775 -4763.0723136 -4761.9486324 -4760.4853205 1549.2377 1.009191 5.4 Results and Plots First let’s get the HPDI interval for the “strength” parameters. Then we will sample the posterior and rank them and present the ranks with their respective posteriors. hpdi &lt;- get_HPDI_from_stanfit(ranking.fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;alpha\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter = algorithms) #Changing to the algorithms labels p_alg &lt;- ggplot(data = hpdi_algorithm, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(y = &quot;Estimate&quot;, x = &quot;Algorithm&quot;) + coord_flip() p_alg + plot_annotation(title = &quot;HPDI interval for the algorithms strength&quot;) Computing the ranks posterior &lt;- rstan::extract(ranking.fit) alpha &lt;- as_tibble(posterior$alpha) colnames(alpha) &lt;- algorithms # sampling from the posterior s &lt;- dplyr::sample_n(alpha, size = 1000, replace = T) s &lt;- dplyr::mutate(s, rown = row_number()) wide_s &lt;- tidyr::pivot_longer(s, cols = all_of(algorithms), names_to = &quot;Algorithm&quot;, values_to = &quot;Alpha&quot;) rank_df &lt;- wide_s %&gt;% dplyr::group_by(rown) %&gt;% dplyr::mutate(Rank = rank(-Alpha, ties.method = &quot;random&quot;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-Alpha) %&gt;% dplyr::group_by(Algorithm) %&gt;% dplyr::summarise(MedianRank = median(Rank), VarianceRank = var(Rank)) %&gt;% dplyr::arrange(MedianRank) rank_df_table &lt;- rank_df colnames(rank_df_table) &lt;- c(&quot;Algorithm&quot;, &quot;Median Rank&quot;, &quot;Variance of the Rank&quot;) kable(rank_df_table, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithm Median Rank Variance of the Rank DifferentialEvolution 1 0.0457417 PSO 2 0.0457417 CMAES 3 0.0099099 RandomSearch2 4 0.2360120 RandomSearch1 5 0.2312673 CuckooSearch 6 0.0000000 SimulatedAnnealing 7 0.0000000 NelderMead 8 0.0000000 "],
["time-to-complete.html", "Chapter 6 Time to complete 6.1 Data preparation 6.2 Stan model 6.3 Diagnosis 6.4 Results and Plots", " Chapter 6 Time to complete In this section, we will consider the Cox’s Proportional Hazard model for analyzing the time to converge to a a solution (in number of iterations). *RQ1: **What is the average number of function evaluations that it takes for an algorithm to converge to a solution at a precision of \\(\\epsilon=0.1\\)? Here we will utilize a window of 100,000 function evaluations per dimension (the budget). RQ2: What is the impact of noise in the number of function evaluations? 6.1 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&quot;./data/statscomp.csv&quot;) Filtering the data that we want and applying some transformations d &lt;- dataset %&gt;% dplyr::filter(OptimizationSuccessful == TRUE &amp; MaxFevalPerDimensions == 1e+05 &amp; (Algorithm == &quot;PSO&quot; | Algorithm == &quot;CMAES&quot; | Algorithm == &quot;DifferentialEvolution&quot; | Algorithm == &quot;RandomSearch2&quot;)) %&gt;% dplyr::select(Algorithm, CostFunction, Event = &quot;SolveAt1e-1&quot;, simNumber, Ndimensions, SD, SolvedAtIteration = &quot;SolveEarlierAt1e-1&quot;) %&gt;% dplyr::mutate(y = SolvedAtIteration/Ndimensions, Event = as.integer(Event), CostFunctionID = create_index(CostFunction), AlgorithmID = create_index(Algorithm)) %&gt;% dplyr::select(Algorithm, AlgorithmID, CostFunction, CostFunctionID, SD, Event, y, -simNumber, -SolvedAtIteration, -Ndimensions) algorithms &lt;- get_index_names_as_array(d$Algorithm) bm &lt;- get_index_names_as_array(d$CostFunction) The data should look like this: kable(dplyr::sample_n(d, size = 10), &quot;html&quot;, booktabs = T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithm AlgorithmID CostFunction CostFunctionID SD Event y DifferentialEvolution 2 ThreeHumpCamelBack 24 3 1 112.0 PSO 3 Damavandi 5 0 0 NA CMAES 1 Tripod 27 0 0 NA CMAES 1 Giunta 8 3 0 5.5 DifferentialEvolution 2 QingN2 13 3 0 NA PSO 3 StrechedVSineWave2N 23 0 1 1764.5 PSO 3 Damavandi 5 3 0 NA CMAES 1 PinterN6 11 3 0 NA RandomSearch2 4 StrechedVSineWave2N 23 0 0 NA RandomSearch2 4 Schwefel2d23N6 18 0 0 NA 6.2 Stan model The Stan model is specified in the file: './stanmodels/timetoconverge.stan' print_stan_code(&quot;./stanmodels/timetoconverge.stan&quot;) // Time to converge, Cox regression model // Author: David Issa Mattos // Date: 23 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size real y[N_total]; // iteration where it was solved int event[N_total]; // Indicates if the event occured or not //To model each algorithm independently int &lt;lower=1&gt; N_algorithm; // Number of algorithms int algorithm_id[N_total]; //vector that has the id of each algorithm //To model the influence of the noise real x_noise[N_total]; //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { //Fixed effect real a_alg[N_algorithm];//the mean effect given by the algorithms real b_noise[N_algorithm];//effect of noise // //Random effect. The effect of the benchmarks real a_bm_norm[N_bm];//the mean effect given by the base class type real&lt;lower=0&gt; s;//std for the random effects } model { //Fixed effect a_alg ~ normal(0,2); // //Random effects s ~ exponential(1); a_bm_norm ~ normal(0,2); for (i in 1:N_total) { //uncensored data if(event[i]==1) target += exponential_lpdf(y[i] | exp(a_alg[algorithm_id[i]] + s*a_bm_norm[bm_id[i]] + b_noise[algorithm_id[i]]*x_noise[i])); //censored data if(event[i]==0) target += exponential_lccdf(y[i] | exp(a_alg[algorithm_id[i]] + s*a_bm_norm[bm_id[i]] + b_noise[algorithm_id[i]]*x_noise[i])); } } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/timetoconverge-data.RDS\" Note that stan does not support NA in the data, so we have two options… We either replace NA for a value and add conditionals in the model (note that this value will not be used). Or we separate the data frame in two parts, censored and not not-censored. We will do the first approach replacing the NA by 0. dstan &lt;- d %&gt;% dplyr::mutate(y = replace_na(y, 0)) standata &lt;- list(N_total = nrow(dstan), y = dstan$y, event = dstan$Event, x_noise = d$SD, N_algorithm = length(algorithms), algorithm_id = dstan$AlgorithmID, N_bm = length(bm), bm_id = d$CostFunctionID) saveRDS(standata, file = &quot;./data/timetoconverge-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately standata &lt;- readRDS(&quot;./data/timetoconverge-data.RDS&quot;) timetoconverge_fit &lt;- stan(file = &quot;./stanmodels/timetoconverge.stan&quot;, data = standata, chains = 4, warmup = 200, iter = 2000) saveRDS(timetoconverge_fit, file = &quot;./data/timetoconverge-fit.RDS&quot;) 6.3 Diagnosis a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;) b_noise &lt;- c(&quot;b_noise[1]&quot;, &quot;b_noise[2]&quot;, &quot;b_noise[3]&quot;, &quot;b_noise[4]&quot;) rstan::traceplot(timetoconverge_fit, pars = a_alg) rstan::traceplot(timetoconverge_fit, pars = b_noise) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.01 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(timetoconverge_fit)$summary) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] -3.6879352 0.0260611 0.6434465 -4.7795130 -4.1479033 -3.7444910 -3.2703575 -2.3373023 609.5925 1.0078610 a_alg[2] -4.9665204 0.0259792 0.6423193 -6.0472139 -5.4229311 -5.0252305 -4.5469025 -3.6189649 611.2939 1.0081670 a_alg[3] -4.8981783 0.0261812 0.6437062 -5.9955040 -5.3567107 -4.9539610 -4.4724221 -3.5636479 604.4983 1.0080710 a_alg[4] -7.5308727 0.0262661 0.6518322 -8.6484905 -7.9969732 -7.5880972 -7.0989728 -6.1706267 615.8569 1.0078940 b_noise[1] -0.7873949 0.0008281 0.0693267 -0.9258806 -0.8331142 -0.7861314 -0.7399842 -0.6534489 7008.2558 0.9998908 b_noise[2] -0.9623720 0.0007871 0.0666718 -1.0985159 -1.0065167 -0.9610198 -0.9159089 -0.8386055 7175.6285 0.9997769 b_noise[3] -0.6815599 0.0007611 0.0616257 -0.8061436 -0.7216233 -0.6810177 -0.6402679 -0.5630298 6556.0923 0.9995395 b_noise[4] -0.4157817 0.0008708 0.0704633 -0.5561798 -0.4634531 -0.4142294 -0.3682190 -0.2811375 6547.7656 1.0004995 a_bm_norm[1] -2.2068927 0.0138048 0.4274942 -3.1017957 -2.4900536 -2.1899142 -1.9077986 -1.4165560 958.9637 1.0020081 a_bm_norm[2] -1.9418029 0.0156010 0.7601267 -3.6352772 -2.4018179 -1.8748259 -1.4037041 -0.6298932 2373.9250 1.0016730 a_bm_norm[3] -2.0962038 0.0138489 0.4085403 -2.9049098 -2.3720660 -2.0867449 -1.8124092 -1.3356092 870.2427 1.0032686 a_bm_norm[4] -0.9188171 0.0149801 0.3858492 -1.6656350 -1.1848176 -0.9273221 -0.6567818 -0.1606120 663.4468 1.0069380 a_bm_norm[5] -2.2110704 0.0139838 0.5095674 -3.2718666 -2.5401981 -2.1880026 -1.8572022 -1.2653516 1327.8526 1.0018033 a_bm_norm[6] -2.7140586 0.0149712 0.4329067 -3.5934134 -3.0029193 -2.6973106 -2.4149331 -1.9051682 836.1343 1.0024868 a_bm_norm[7] 3.9107650 0.0382249 1.0162875 2.0143820 3.1720597 3.9075931 4.6106967 5.9460393 706.8727 1.0049193 a_bm_norm[8] 2.1842483 0.0288006 0.7462273 0.8071741 1.6427731 2.1779519 2.6980648 3.6787558 671.3344 1.0056782 a_bm_norm[9] -0.0183041 0.0256579 1.9875436 -3.9108792 -1.3656447 -0.0178273 1.3147861 3.9015139 6000.5355 0.9998576 a_bm_norm[10] -1.7515600 0.0136141 0.3748046 -2.4905996 -2.0029957 -1.7495564 -1.4941955 -1.0428575 757.9354 1.0049120 a_bm_norm[11] -1.9179618 0.0138507 0.4178795 -2.7800509 -2.1977668 -1.9110601 -1.6258559 -1.1348262 910.2443 1.0036066 a_bm_norm[12] 0.6804290 0.0208988 0.5338431 -0.3019114 0.2928043 0.6728829 1.0424419 1.7578355 652.5039 1.0075739 a_bm_norm[13] -1.2329197 0.0144499 0.3863742 -1.9773788 -1.4925028 -1.2357603 -0.9740125 -0.4672595 714.9665 1.0056509 a_bm_norm[14] -1.9813449 0.0137377 0.3985710 -2.7741454 -2.2467841 -1.9750211 -1.7020419 -1.2338100 841.7459 1.0029096 a_bm_norm[15] -2.0244860 0.0138469 0.3978802 -2.8237783 -2.2886983 -2.0137740 -1.7483824 -1.2855645 825.6605 1.0038004 a_bm_norm[16] -1.2447983 0.0145003 0.3805922 -1.9897323 -1.5058430 -1.2394549 -0.9821145 -0.5191411 688.9124 1.0062603 a_bm_norm[17] -2.4526369 0.0143293 0.4082626 -3.2754201 -2.7286151 -2.4430287 -2.1663149 -1.6899662 811.7676 1.0022558 a_bm_norm[18] -0.6057770 0.0159141 0.4048563 -1.3731590 -0.8938669 -0.6173464 -0.3256567 0.1909392 647.1988 1.0074765 a_bm_norm[19] -1.8414474 0.0159610 0.5471594 -2.9809155 -2.1921287 -1.8197649 -1.4708299 -0.8222897 1175.1846 1.0030687 a_bm_norm[20] -1.6896868 0.0139254 0.3895083 -2.4597492 -1.9528991 -1.6816414 -1.4282774 -0.9419728 782.3822 1.0052099 a_bm_norm[21] -2.0289757 0.0138213 0.3852489 -2.7985060 -2.2883522 -2.0256102 -1.7602969 -1.2924934 776.9386 1.0036007 a_bm_norm[22] 0.4854648 0.0201883 0.5142060 -0.4592096 0.1164319 0.4737421 0.8344110 1.5202992 648.7483 1.0075484 a_bm_norm[23] -1.9410607 0.0140284 0.3968524 -2.7507148 -2.2050914 -1.9319710 -1.6639740 -1.1906605 800.2820 1.0038848 a_bm_norm[24] 0.5472608 0.0203586 0.5199840 -0.3892370 0.1667028 0.5379406 0.9051101 1.6039060 652.3543 1.0073756 a_bm_norm[25] -2.8240879 0.0149857 0.4471718 -3.7491880 -3.1158450 -2.8098965 -2.5118785 -1.9938501 890.4142 1.0013487 a_bm_norm[26] 2.8701150 0.0323531 0.8510367 1.2861855 2.2497517 2.8650684 3.4562617 4.5769374 691.9322 1.0053543 a_bm_norm[27] -1.5290088 0.0140347 0.4086175 -2.3528919 -1.8033101 -1.5262928 -1.2427724 -0.7556860 847.6717 1.0049216 a_bm_norm[28] -2.4117939 0.0161124 0.7305654 -3.9858853 -2.8640868 -2.3491364 -1.8878787 -1.1518648 2055.8701 1.0013608 a_bm_norm[29] -0.4067741 0.0167410 0.4299791 -1.2112123 -0.7101342 -0.4242433 -0.1160598 0.4590840 659.6791 1.0077236 a_bm_norm[30] 0.4329136 0.0199368 0.5073487 -0.4988221 0.0666072 0.4198662 0.7798276 1.4490052 647.5905 1.0073069 s 1.4055652 0.0092678 0.2644219 1.0106893 1.2178688 1.3645492 1.5465992 2.0241819 814.0237 1.0026546 lp__ -5484.1013930 0.2128306 6.4247792 -5497.5929141 -5488.2327073 -5483.8242140 -5479.6884137 -5472.1586026 911.2723 1.0024053 6.4 Results and Plots First lets get the HPDI of every parameter. Then we restrict to the algorithms, them to the slopes, then to the parameter s hpdi &lt;- get_HPDI_from_stanfit(timetoconverge_fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter = algorithms) #Changing to the algorithms labels hpdi_noise &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;b_noise\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter = algorithms) #Changing to the algorithms labels hpdi_s &lt;- hpdi %&gt;% dplyr::filter(Parameter == &quot;s&quot;) p_alg &lt;- ggplot(data = hpdi_algorithm, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(y = &quot;a_alg&quot;, x = &quot;Algorithm&quot;) + coord_flip() p_alg + plot_annotation(title = &quot;HPDI interval for the algorithms&quot;) p_noise &lt;- ggplot(data = hpdi_noise, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(y = &quot;b_noise&quot;, x = &quot;Algorithm&quot;) + coord_flip() p_noise + plot_annotation(title = &quot;HPDI interval for noise coefficient&quot;) p_s &lt;- ggplot(data = hpdi_s, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(y = &quot;Estimate&quot;, x = &quot;Parameter&quot;) + coord_flip() p_s + plot_annotation(title = &quot;HPDI interval std of the benchmarks&quot;) Table for the hazard ratio hr_table &lt;- tibble(Algorithm = algorithms, `Baseline Coefficient` = exp(hpdi_algorithm$Mean), `Noise HR` = exp(hpdi_noise$Mean)) kable(hr_table, booktabs = T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithm Baseline Coefficient Noise HR CMAES 0.025 0.455 DifferentialEvolution 0.007 0.382 PSO 0.007 0.506 RandomSearch2 0.001 0.660 "],
["multiple-group-comparison.html", "Chapter 7 Multiple-group comparison 7.1 Data preparation 7.2 Stan model 7.3 Diagnosis 7.4 Results and Plots", " Chapter 7 Multiple-group comparison We present here the Stan version of the BEST (Bayesian Estimation Supersedes the t Test) from John K. Kruschke. We will consider the following research question RQ: 7.1 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&quot;./data/statscomp.csv&quot;) Filtering the data that we want and applying some transformations d &lt;- dataset %&gt;% dplyr::filter(OptimizationSuccessful == TRUE &amp; (Algorithm == &quot;PSO&quot; | Algorithm == &quot;RandomSearch1&quot; | Algorithm == &quot;DifferentialEvolution&quot;)) %&gt;% dplyr::select(Algorithm, CostFunction, TimeToComplete, simNumber, MaxFeval) %&gt;% dplyr::mutate(y = 10000 * TimeToComplete/MaxFeval, CostFunctionID = create_index(CostFunction), AlgorithmID = create_index(Algorithm)) %&gt;% dplyr::select(Algorithm, AlgorithmID, CostFunction, CostFunctionID, y, -simNumber, -MaxFeval) algorithms &lt;- get_index_names_as_array(d$Algorithm) bm &lt;- get_index_names_as_array(d$CostFunction) The data should look like this: kable(dplyr::sample_n(d, size = 10), &quot;html&quot;, booktabs = T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Algorithm AlgorithmID CostFunction CostFunctionID y PSO 2 WhitleyN6 28 1.660 PSO 2 ThreeHumpCamelBack 24 0.381 RandomSearch1 3 PinterN6 11 0.692 PSO 2 Schwefel2d21N6 17 0.362 RandomSearch1 3 StrechedVSineWave2N 23 0.221 RandomSearch1 3 ZakharovN2 30 0.426 RandomSearch1 3 Schwefel2d20N2 16 0.229 PSO 2 BentCigarN6 1 0.663 PSO 2 ChenBird 2 0.567 PSO 2 Trefethen 25 0.627 Some initial visualizations in terms of box-plots p1 &lt;- ggplot(d) + geom_boxplot(aes(x = Algorithm, y = y)) + labs(y = &quot;Time to complete x10,000&quot;) p1 + plot_annotation(title = &quot;Box-plot of the time per evaluation&quot;) lmfit &lt;- lm(y ~ Algorithm, data = d) p2 &lt;- ggplot() + geom_qq(aes(sample = lmfit$residuals)) + geom_qq_line(aes(sample = lmfit$residuals)) + labs(x = &quot;Standard normal quantiles&quot;, y = &quot;Sample quantiles&quot;) p2 + plot_annotation(title = &quot;Q-Q plot for normality analysis&quot;) 7.2 Stan model The Stan model is specified in the file: './stanmodels/multiplegroups.stan' print_stan_code(&quot;./stanmodels/multiplegroups.stan&quot;) // Multiple group comparison // Author: David Issa Mattos // Date: 23 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size real y[N_total]; // time to complete variable //To model each algorithm independently int &lt;lower=1&gt; N_algorithm; // Number of algorithms int algorithm_id[N_total]; //vector that has the id of each algorithm //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { //Fixed effect real a_alg[N_algorithm];//the mean effect given by the algorithms real &lt;lower=0&gt; sigma[N_algorithm];//std for the student t // //Random effect. The effect of the benchmarks real a_bm_norm[N_bm];//the mean effect given by the base class type real&lt;lower=0&gt; s;//std for the random effects real&lt;lower=0&gt; nu;//std for the random effects } model { real mu[N_total]; real sigma_i[N_total]; sigma ~ exponential(1); nu ~ exponential(1.0/30.0); //Fixed effect a_alg ~ normal(0,1); // //Random effects s ~ exponential(1); a_bm_norm ~ normal(0,1); for (i in 1:N_total) { mu[i] = a_alg[algorithm_id[i]] + a_bm_norm[bm_id[i]]*s; sigma_i[i] = sigma[algorithm_id[i]]; } y ~ student_t(nu, mu, sigma_i); } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/multiplegroup-data.RDS\" standata &lt;- list(N_total = nrow(d), y = d$y, N_algorithm = length(algorithms), algorithm_id = d$AlgorithmID, N_bm = length(bm), bm_id = d$CostFunctionID) saveRDS(standata, file = &quot;./data/multiplegroups-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately. Here we increased the maxtreedepth and the number of iterations so we have a higher effective sample for inference. Both of these do not impact the validity of the chain just the computation efficiency. standata&lt;-readRDS(&quot;./data/multiplegroups-data.RDS&quot;) multiplegroup_fit &lt;- stan(file = &#39;./stanmodels/multiplegroups.stan&#39;, data=standata, chains = 4, warmup = 400, iter = 4000, # include = T, # sample_file = &quot;./stanmodels/multiplegroups.csv&quot; control = list(max_treedepth = 15)) saveRDS(multiplegroup_fit, file = &quot;./data/multiplegroups-fit.RDS&quot;) 7.3 Diagnosis a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;) b_noise &lt;- c(&quot;sigma[1]&quot;, &quot;sigma[2]&quot;, &quot;sigma[3]&quot;) rstan::traceplot(multiplegroup_fit, pars = a_alg) rstan::traceplot(multiplegroup_fit, pars = b_noise) rstan::traceplot(multiplegroup_fit, pars = c(&quot;s&quot;, &quot;nu&quot;)) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.01 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(multiplegroup_fit)$summary, &quot;html&quot;, ) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] 1.7815819 0.0024748 0.0566810 1.6721093 1.7445310 1.7807935 1.8199106 1.8926830 524.5383 1.002721 a_alg[2] 0.5696040 0.0024743 0.0566789 0.4603005 0.5326239 0.5687407 0.6077809 0.6804053 524.7432 1.002723 a_alg[3] 0.4428074 0.0024754 0.0566494 0.3332348 0.4059292 0.4419022 0.4810402 0.5535471 523.7002 1.002756 sigma[1] 0.0876911 0.0000311 0.0017075 0.0844449 0.0865150 0.0876638 0.0888157 0.0911495 3013.0294 1.001474 sigma[2] 0.0694382 0.0000276 0.0014904 0.0665632 0.0684023 0.0693989 0.0704513 0.0724145 2913.9665 1.001041 sigma[3] 0.0368466 0.0000150 0.0008085 0.0352774 0.0363041 0.0368220 0.0373747 0.0384569 2897.1475 1.000519 a_bm_norm[1] 0.7192083 0.0086246 0.2094808 0.3161019 0.5779758 0.7220803 0.8677116 1.1162210 589.9497 1.003545 a_bm_norm[2] -0.1814373 0.0079796 0.1837739 -0.5391235 -0.3097406 -0.1810262 -0.0580464 0.1761930 530.4046 1.003210 a_bm_norm[3] -0.5827379 0.0083120 0.1959346 -0.9565364 -0.7190948 -0.5808377 -0.4482490 -0.2036032 555.6618 1.004407 a_bm_norm[4] -0.6089228 0.0083542 0.1970467 -0.9822168 -0.7462298 -0.6064975 -0.4755324 -0.2258659 556.3242 1.004509 a_bm_norm[5] -0.2476612 0.0080067 0.1849829 -0.6059732 -0.3757603 -0.2457094 -0.1225043 0.1104547 533.7774 1.003279 a_bm_norm[6] 0.4750596 0.0082505 0.1959683 0.0961207 0.3410841 0.4779159 0.6141989 0.8520264 564.1714 1.002989 a_bm_norm[7] -0.6420408 0.0083861 0.1987948 -1.0201032 -0.7794661 -0.6404438 -0.5074182 -0.2562647 561.9448 1.004617 a_bm_norm[8] -0.1634319 0.0079914 0.1838506 -0.5206348 -0.2915316 -0.1620165 -0.0398618 0.1912232 529.2798 1.003135 a_bm_norm[9] 2.1154599 0.0123913 0.3386954 1.4588894 1.8888847 2.1236694 2.3444388 2.7697322 747.1082 1.006364 a_bm_norm[10] -0.1804465 0.0079840 0.1839089 -0.5387432 -0.3089380 -0.1785651 -0.0566686 0.1739718 530.6024 1.003236 a_bm_norm[11] 0.9041127 0.0089536 0.2222174 0.4740013 0.7546731 0.9095963 1.0585790 1.3270507 615.9702 1.004018 a_bm_norm[12] -0.7819305 0.0086115 0.2067054 -1.1711710 -0.9250797 -0.7817243 -0.6403822 -0.3839085 576.1671 1.005135 a_bm_norm[13] -0.6772704 0.0084572 0.2008767 -1.0568907 -0.8167815 -0.6756358 -0.5401214 -0.2884239 564.1649 1.004795 a_bm_norm[14] 1.7172228 0.0110972 0.2962334 1.1467413 1.5179348 1.7242375 1.9183202 2.2869387 712.5936 1.005705 a_bm_norm[15] -0.2509447 0.0080060 0.1849612 -0.6096211 -0.3798723 -0.2493476 -0.1265618 0.1081399 533.7393 1.003362 a_bm_norm[16] -0.6887018 0.0084737 0.2014353 -1.0669924 -0.8280677 -0.6876355 -0.5517005 -0.2998266 565.0964 1.004953 a_bm_norm[17] -0.2987615 0.0080353 0.1858974 -0.6575569 -0.4267789 -0.2965663 -0.1724545 0.0592583 535.2275 1.003464 a_bm_norm[18] -0.2057157 0.0079888 0.1842716 -0.5629628 -0.3336642 -0.2054677 -0.0819283 0.1508168 532.0458 1.003351 a_bm_norm[19] -0.1772225 0.0079777 0.1838359 -0.5353252 -0.3051236 -0.1764972 -0.0539611 0.1780432 531.0154 1.003200 a_bm_norm[20] 0.0021975 0.0079578 0.1833495 -0.3579432 -0.1252151 0.0040986 0.1274310 0.3529663 530.8547 1.002951 a_bm_norm[21] -0.0370757 0.0079567 0.1833234 -0.3970028 -0.1650148 -0.0351024 0.0858728 0.3142432 530.8508 1.002966 a_bm_norm[22] -0.2582016 0.0080267 0.1850542 -0.6155364 -0.3865295 -0.2570828 -0.1331500 0.0991284 531.5186 1.003413 a_bm_norm[23] -0.7357537 0.0085331 0.2038511 -1.1186036 -0.8766281 -0.7351464 -0.5982257 -0.3425063 570.7101 1.004962 a_bm_norm[24] -0.8252724 0.0086947 0.2094949 -1.2189848 -0.9708733 -0.8254469 -0.6819852 -0.4221552 580.5534 1.005230 a_bm_norm[25] -0.3359663 0.0080697 0.1868876 -0.6971104 -0.4646943 -0.3327841 -0.2091031 0.0259680 536.3423 1.003643 a_bm_norm[26] 0.3504413 0.0081328 0.1907707 -0.0194125 0.2175444 0.3538897 0.4859020 0.7197615 550.2295 1.002923 a_bm_norm[27] -0.7344475 0.0085396 0.2039208 -1.1172730 -0.8755600 -0.7335636 -0.5964523 -0.3408633 570.2296 1.004995 a_bm_norm[28] 3.7056060 0.0184140 0.5268846 2.6654739 3.3494149 3.7062854 4.0644549 4.7388580 818.7208 1.007627 a_bm_norm[29] -0.4139099 0.0081410 0.1892188 -0.7771179 -0.5447576 -0.4105746 -0.2855314 -0.0474645 540.2168 1.003952 a_bm_norm[30] -0.1763685 0.0079847 0.1837990 -0.5350264 -0.3048733 -0.1757948 -0.0531583 0.1779765 529.8757 1.003222 s 0.3064198 0.0014329 0.0423732 0.2379312 0.2766066 0.3013470 0.3301371 0.4061672 874.5406 1.009378 nu 2.7520111 0.0015060 0.0796144 2.5992295 2.6986152 2.7495934 2.8046633 2.9117775 2794.5987 1.001687 lp__ 14070.3651083 0.1502401 5.7707953 14058.1445159 14066.7032470 14070.6873137 14074.3315285 14080.8165057 1475.3660 1.004358 7.4 Results and Plots First lets get the HPDI of every parameter. Then we restrict to the algorithms, them to the slopes, then to the parameter s hpdi &lt;- get_HPDI_from_stanfit(multiplegroup_fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter = algorithms) #Changing to the algorithms labels hpdi_sigma &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;sigma\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter = algorithms) #Changing to the algorithms labels hpdi_s &lt;- hpdi %&gt;% dplyr::filter(Parameter == &quot;s&quot;) hpdi_nu &lt;- hpdi %&gt;% dplyr::filter(Parameter == &quot;nu&quot;) hpdi_nu_s &lt;- hpdi %&gt;% dplyr::filter(Parameter == &quot;nu&quot; | Parameter == &quot;s&quot;) p_alg &lt;- ggplot(data = hpdi_algorithm, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(y = &quot;a_alg&quot;, x = &quot;Algorithm&quot;) + coord_flip() p_alg + plot_annotation(title = &quot;HPDI interval for the algorithms&quot;) p_sigma &lt;- ggplot(data = hpdi_sigma, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(y = &quot;sigma&quot;, x = &quot;Algorithm&quot;) + coord_flip() p_sigma + plot_annotation(title = &quot;HPDI interval for sigma&quot;) p_s &lt;- ggplot(data = hpdi_s, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(y = &quot;s&quot;, x = &quot;Parameter&quot;) + coord_flip() p_s + plot_annotation(title = &quot;HPDI interval std of the benchmarks&quot;) p_nu &lt;- ggplot(data = hpdi_nu, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(y = &quot;nu&quot;, x = &quot;Parameter&quot;) + coord_flip() p_nu + plot_annotation(title = &quot;HPDI interval of the degree of freedom&quot;) p_nu_s &lt;- ggplot(data = hpdi_nu_s, aes(x = Parameter)) + geom_pointrange(aes(ymin = HPDI.lower, ymax = HPDI.higher, y = Mean)) + labs(y = &quot;Estimate&quot;, x = &quot;Parameter&quot;) + coord_flip() p_nu_s + plot_annotation(title = &quot;HPDI interval&quot;) Now lets get a posterior distribution of the difference posterior &lt;- rstan::extract(multiplegroup_fit) a_alg &lt;- as_tibble(posterior$a_alg) colnames(a_alg) &lt;- algorithms sample_a_alg &lt;- dplyr::sample_n(a_alg, size = 1000, replace = T) %&gt;% dplyr::mutate(PSO_Random = PSO - RandomSearch1, DE_PSO = DifferentialEvolution - PSO, DE_Random = DifferentialEvolution - RandomSearch1) %&gt;% dplyr::select(-DifferentialEvolution, -PSO, -RandomSearch1) # Getting HPDI from a data frame and creating a table instead of plotting... hpdi_diff &lt;- HDInterval::hdi(sample_a_alg, credMass = 0.95) hpdi_diff &lt;- hpdi_diff %&gt;% as_tibble(rownames = &quot;Metric&quot;) %&gt;% tibble::add_row(Metric = &quot;Mean&quot;, PSO_Random = mean(sample_a_alg$PSO_Random), DE_PSO = mean(sample_a_alg$DE_PSO), DE_Random = mean(sample_a_alg$DE_Random)) %&gt;% tidyr::pivot_longer(cols = -Metric, names_to = &quot;AlgorithmDifference&quot;, values_to = &quot;values&quot;) %&gt;% tidyr::pivot_wider(names_from = Metric, values_from = values) %&gt;% dplyr::mutate(Difference = c(&quot;PSO - RandomSearch&quot;, &quot;DifferentialEvolution - PSO&quot;, &quot;DifferentialEvolution - RandomSearch&quot;)) %&gt;% dplyr::select(Difference, Lower = lower, Mean, Upper = upper) kable(hpdi_diff, booktabs = T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) Difference Lower Mean Upper PSO - RandomSearch 0.123 0.127 0.130 DifferentialEvolution - PSO 1.208 1.212 1.218 DifferentialEvolution - RandomSearch 1.335 1.339 1.344 "],
["sensitivity-analysis-and-model-comparison.html", "Chapter 8 Sensitivity analysis and model comparison", " Chapter 8 Sensitivity analysis and model comparison "]
]
