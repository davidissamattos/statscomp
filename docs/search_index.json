[["index.html", "Online Appendix for: Statistical Models for the Analysis of Optimization Algorithms with Benchmark Functions Preface Pre-requisites Source code Compiling this document Software environment", " Online Appendix for: Statistical Models for the Analysis of Optimization Algorithms with Benchmark Functions David Issa Mattos, Jan Bosch, Helena Holmström Olsson 04 March, 2021 Preface This document is an online appendix to the paper “Statistical Models for the Analysis of Optimization Algorithms with Benchmark Functions” by David Issa Mattos, Jan Bosch and Helena Holmström Olsson. It shows the step-by-step process to analyze the data, including data preparation, modeling and plotting for all the models described on the paper. Pre-requisites To follow the code, we assume that the reader has some familiarity with the R environment including packages included of the tidyverse, such as dplyr and ggplot2. The code presented is described and fairly commented to help readers follow the modeling process. Other programming languages such as Python with numpy, pandas, matplotlib etc are capable of performing the same steps, but this is out of the scope of this document. For the Bayesian models, we try to minimize dependency on a specific R package such as brms or rstanarm, and therefore we discuss the model in Stan only, since it has bindings for multiple programming languages. The reader familiar with other languages might be interested in adapting these models and plots to the desired language, Source code The full source code is available in the repository https://github.com/davidissamattos/statscomp. The dataset and the final data for each model (after the described data transformation) is also available for download in the ./data folder. The Stan models are available in the ./stanmodels folder. The utils folder contains some helper functions. Compiling this document This document was created with the bookdown package. To compile it (and run every command to generate the models, figures and etc. ) type: bookdown::render_book(&#39;index.Rmd&#39;, &#39;all&#39;) or alternatively use the custom function from the utils.R file. This function besides compiling the book generate the tables for the paper. We cannot generate latex tables (with correct labels) while compiling to bookdown_site. So this function takes the saved tables and generate them separately compile_book() Software environment The environment used to compile this document is sessionInfo() R version 4.0.3 (2020-10-10) Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Big Sur 10.16 Matrix products: default LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] gtools_3.8.2 progress_1.2.2 ggthemr_1.1.0 viridis_0.5.1 viridisLite_0.3.0 [6] patchwork_1.0.1 coda_0.19-4 rstan_2.21.2 StanHeaders_2.21.0-6 glue_1.4.2 [11] forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 readr_1.3.1 [16] tidyr_1.1.2 tibble_3.0.4 ggplot2_3.3.2 tidyverse_1.3.0 kableExtra_1.2.1 [21] rmdformats_0.3.7 knitr_1.30 loaded via a namespace (and not attached): [1] matrixStats_0.57.0 fs_1.5.0 usethis_1.6.3 lubridate_1.7.9 devtools_2.3.2 [6] webshot_0.5.2 httr_1.4.2 rprojroot_2.0.2 tools_4.0.3 backports_1.2.1 [11] R6_2.5.0 DBI_1.1.0 colorspace_2.0-0 withr_2.3.0 tidyselect_1.1.0 [16] gridExtra_2.3 prettyunits_1.1.1 processx_3.4.5 curl_4.3 compiler_4.0.3 [21] cli_2.2.0 rvest_0.3.6 HDInterval_0.2.2 xml2_1.3.2 desc_1.2.0 [26] labeling_0.4.2 bookdown_0.21 scales_1.1.1 callr_3.5.1 digest_0.6.27 [31] rmarkdown_2.6 pkgconfig_2.0.3 htmltools_0.5.0 sessioninfo_1.1.1 highr_0.8 [36] dbplyr_1.4.4 rlang_0.4.9 readxl_1.3.1 rstudioapi_0.13 farver_2.0.3 [41] generics_0.1.0 jsonlite_1.7.2 inline_0.3.17 magrittr_2.0.1 loo_2.4.1 [46] Rcpp_1.0.5 munsell_0.5.0 fansi_0.4.1 lifecycle_0.2.0 stringi_1.5.3 [51] yaml_2.2.1 pkgbuild_1.1.0 grid_4.0.3 blob_1.2.1 parallel_4.0.3 [56] crayon_1.3.4 lattice_0.20-41 haven_2.3.1 hms_0.5.3 ps_1.5.0 [61] pillar_1.4.7 codetools_0.2-16 stats4_4.0.3 pkgload_1.1.0 reprex_0.3.0 [66] evaluate_0.14 V8_3.4.0 remotes_2.2.0 RcppParallel_5.0.2 modelr_0.1.8 [71] vctrs_0.3.5 testthat_3.0.0 cellranger_1.1.0 gtable_0.3.0 assertthat_0.2.1 [76] xfun_0.19 broom_0.7.0 memoise_1.1.0 ellipsis_0.3.1 "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction This document is based on a single dataset available at ./data/statscomp.csv. With this dataset we ask different research questions that motivates the statistical models developed on the paper. Explanations about the models and how the data was obtained are available in the paper. 1.0.1 Exploring the dataset This dataset follows the principle of tidy data as described in https://r4ds.had.co.nz/tidy-data.html. The key idea is that every variables has its own column, and every observation has its own unique row. Throughout this document, to facilitate our modeling approach, we will modify this dataset in different ways, often resulting in non-tidy data. However every model will start from the same base tidy dataset. This approach will hopefully make it easier for the reader to understand from where we are starting and adopt similar strategies in their own models. Additionally, we recommend, if the reader has the opportunity to influence the data collection process, the choice of tidy data. It is often ideal for exploratory analysis, plotting, is the basis for most models, and easy to transform to be used in different models. d &lt;- readr::read_csv(&#39;./data/statscomp.csv&#39;) Here we are excluding a few columns to simplify our view kable(head(dplyr::select(d, -BestArm, -Continuous, -Differentiability, -Separability, -Scalability, -Modality,-BBOB,-BaseClass, -MaxFeval, -FevalPerDimensions), n=10)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Algorithm CostFunction NumberFunctionEval EuclideanDistance TrueRewardDifference CumulativeRegret TimeToComplete Ndimensions OptimizationSuccessful SD MaxFevalPerDimensions SolveAt1 SolveAt1e-1 SolveAt1e-3 SolveAt1e-6 SolveEarlierAt1 SolveEarlierAt1e-1 SolveEarlierAt1e-3 SolveEarlierAt1e-6 simNumber NelderMead BentCigarN6 600 8.9123708 7.364249e+07 4.926652e+10 0.0300207 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 PSO BentCigarN6 600 0.5605997 1.559497e+05 7.938082e+10 0.0394440 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 SimulatedAnnealing BentCigarN6 600 9.7499527 1.086834e+08 1.208830e+12 0.0424774 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 CuckooSearch BentCigarN6 600 8.0025211 1.200314e+07 1.017438e+13 0.0317579 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 DifferentialEvolution BentCigarN6 600 5.3888603 4.634518e+06 2.399718e+12 0.1168543 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 RandomSearch1 BentCigarN6 600 1.5702536 1.919896e+06 1.983989e+13 0.0399160 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 RandomSearch2 BentCigarN6 599 1.5702536 1.655484e+06 1.929796e+13 0.0356977 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 0 CMAES BentCigarN6 604 0.5744144 3.357865e-01 2.589247e+08 0.1810286 6 TRUE 0 100 TRUE FALSE FALSE FALSE 543 NA NA NA 0 NelderMead BentCigarN6 600 7.9123493 2.152945e+07 2.041479e+11 0.0423668 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 1 PSO BentCigarN6 600 1.0818350 1.853063e+05 7.224343e+10 0.0397996 6 TRUE 0 100 FALSE FALSE FALSE FALSE NA NA NA NA 1 1.0.2 Column definitions of the dataset Algorithms: string Algorithm used in the optimization CostFunction: string Specific cost function used. If the cost function can be instantiated in more than one dimension this name also includes the number of dimensions, e.g. SphereN10 is has the base class Sphere and the N=10 for dimensions. BestArm: string represents the xalgo obtained at the end of the optimization NumberFunctionEval: numeric number of times the functon was evaluated in total EuclideanDistance: numeric ||xalgo - xoptimal||2 TrueRewardDifference: numeric falgo - foptimal CumulativeRegret: numeric total regret TimeToComplete: numeric time taken to complete the optimization Continuous: string function properties from the Jamil and Yang survey 2013 Differentiability: string function properties from the Jamil and Yang survey 2013 Separability: string function properties from the Jamil and Yang survey 2013 Scalability: string function properties from the Jamil and Yang survey 2013 Modality: string function properties from the Jamil and Yang survey 2013 Ndimension: numeric number of dimensions OptimizationSuccessful\" BBOB: boolean is part of the BBOB 2009 functions? BaseClass: string the benchmark function used. E.g. SphereN10 has the base class Sphere SD: numeric gaussian noise added to the benchmark function MaxFeval: numeric maximum number of function evaluations in total MaxFevalPerDimensions: numeric maximum number of function evaluations allowed per dimensions FevalPerDimensions: numeric number of times the benchmark function was evaluated per dimensions (some algorithms might evaluate a bit less than the maximum) SolveAt1: boolean was the problem solved at precision 1 SolveAt1e-1\" boolean was the problem solved at precision 1e-1 SolveAt1e-3\" boolean was the problem solved at precision 1e-3 SolveAt1e-6: boolean was the problem solved at precision 1e-6 SolveEarlierAt1: numeric iteration number where converged to the result at precision 1 SolveEarlierAt1e-1: numeric iteration number where converged to the result at precision 1e-1 SolveEarlierAt1e-3: numeric iteration number where converged to the result at precision 1e-3 SolveEarlierAt1e-6: numeric iteration number where converged to the result at precision 1e-6 simNumber: numeric number of the repeated measures, in the dataset, every algorithm was evaluated 10 times in each benchmark function in each condition, in this case the number goes from 0 to 9 "],["probability-of-success-model.html", "Chapter 2 Probability of success model 2.1 RQ1 Data preparation 2.2 RQ1 Stan model 2.3 RQ1 Diagnosis 2.4 RQ1 Results and Plots", " Chapter 2 Probability of success model Our first model can be used to address problems such as: RQ1-a: What is the probability of each algorithm solving a problem at precision \\(\\epsilon \\leq 0.1\\)? RQ1-b: What is the impact of noise in the probability of success of each algorithm at precision \\(\\epsilon \\leq 0.1\\)? 2.1 RQ1 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&#39;./data/statscomp.csv&#39;) Let’s select only the columns that interests us. Note we use \"\" to select some of the columns because they have “-” in the column name dataset&lt;-dplyr::select(dataset, Algorithm, CostFunction, SD, MaxFevalPerDimensions, simNumber, SolveAt1, &quot;SolveAt1e-1&quot;,&quot;SolveAt1e-3&quot;,&quot;SolveAt1e-6&quot;, OptimizationSuccessful) Let’s do some basic transformation 1 - We select only the cases where the optimization completed 2 - We convert True to 1 and 0 to false 3 - We group by the algorithms, functions, SD, and budget so we can summarize and create aggregated data 4 - We create an index of each algorithm and the cost functions. This is basically creating a map of NelderMead=1, PSO=2 etc… This makes things easier to work in Stan. For that we use the function create_index from the utils.R file 5 - We drop the columns we wont use 6 - Get an array with the names of the benchmark functions and the algorithms (to create nicer plots later with lengend) Since we are only looking at 1e-1 for the precision we comment the other lines d &lt;- dataset %&gt;% dplyr::filter(OptimizationSuccessful==TRUE) %&gt;% dplyr::mutate( solvedAt1e1=as.integer(dataset$&quot;SolveAt1e-1&quot;), budget=MaxFevalPerDimensions) %&gt;% dplyr::group_by(Algorithm, CostFunction, SD, budget) %&gt;% dplyr::summarize( solvedAt1e1=sum(solvedAt1e1), N=n()) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(AlgorithmID=create_index(Algorithm), CostFunctionID=create_index(CostFunction)) %&gt;% dplyr::select(Algorithm,AlgorithmID, CostFunction, CostFunctionID, SD, budget, N, y=solvedAt1e1, ) #List of algorithms bm &lt;- get_index_names_as_array(d$CostFunction) algorithms &lt;- get_index_names_as_array(d$Algorithm) Lets preview a sample of the data set kable(dplyr::sample_n(d,size=10),&quot;html&quot;, booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Algorithm AlgorithmID CostFunction CostFunctionID SD budget N y RandomSearch2 7 ChenV 3 3 20 10 0 PSO 5 WhitleyN6 28 0 20 10 0 SimulatedAnnealing 8 Schwefel2d26N6 19 3 100000 10 0 NelderMead 4 LunacekBiRastriginN6 9 3 1000 10 0 PSO 5 StrechedVSineWave2N 23 3 10000 10 0 PSO 5 Schwefel2d21N6 17 3 100 10 0 CuckooSearch 2 ZakharovN2 30 3 1000 10 0 DifferentialEvolution 3 StrechedVSineWave2N 23 0 10000 10 10 DifferentialEvolution 3 Schwefel2d21N6 17 0 100000 10 10 CMAES 1 Giunta 8 3 1000 10 1 2.2 RQ1 Stan model The Stan model is specified in the file: './stanmodels/probsuccess.stan'. Note that at the end of the model we commented the generated quantities. This block generates the predictive posterior y_rep and the log likelihood, log_lik. These values are useful in diagnosing and validating the model but the end file is extremely large (~1Gb for 2000 iterations) and make many of the following calculations slow. If the reader wants to see these values is just to uncomment and run the stan model again print_stan_code(&#39;./stanmodels/probsuccess.stan&#39;) // Probability of success model // Author: David Issa Mattos // Date: 16 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size int y[N_total]; // Result of the binomial int N_draw[N_total]; // Number of draws in the binomial real x_noise[N_total];//predictor for noise //To model each algorithm independently int &lt;lower=1&gt; N_algorithm; // Number of algorithms int algorithm_id[N_total]; //vector that has the id of each algorithm //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { //Fixed effect real a_alg[N_algorithm];//the mean effect given by the algorithms real b_noise[N_algorithm];//slope for the noise // //Random effect. The effect of the benchmarks real a_bm_norm[N_bm];//the mean effect given by the base class type real&lt;lower=0&gt; s;//std for the random effects } model { real p[N_total]; //Fixed effect a_alg ~ normal(0,5); b_noise ~ normal(0,5); // //Random effects s ~ exponential(0.1); a_bm_norm ~ normal(0,1); for (i in 1:N_total) { p[i] = a_alg[algorithm_id[i]]+ a_bm_norm[bm_id[i]]*s + b_noise[algorithm_id[i]] * x_noise[i]; } //Equivalent to: y~binomial(N, inverse_logit(a+bx=alpha)) y ~ binomial_logit(N_draw,p); } //Uncoment this part to get the posterior predictives and the log likelihood //But note that it takes a lot of space in the final model // generated quantities{ // vector [N_total] y_rep; // vector[N_total] log_lik; // // for(i in 1:N_total){ // real p; // p = a_alg[algorithm_id[i]]+ a_bm_norm[bm_id[i]]*s + b_noise[algorithm_id[i]] * x_noise[i]; // // y_rep[i] = binomial_rng(N_draw[i], inv_logit(p)); // // //Log likelihood // log_lik[i] = binomial_lpmf(y[i] | N_draw[i], inv_logit(p)); // } // } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/probsuccsess-data.RDS\" standata &lt;- list( N_total = nrow(d), y = d$y, N_draw = d$N, x_noise = d$SD, N_algorithm = length(algorithms), algorithm_id =d$AlgorithmID, N_bm = length(bm), bm_id = d$CostFunctionID) saveRDS(standata, file = &quot;./data/probsuccsess-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object in the data folder. However, when we change our model or the data we can just run this chunk separately. standata&lt;-readRDS(&quot;./data/probsuccsess-data.RDS&quot;) probsuccess.fit &lt;- stan(file = &#39;./stanmodels/probsuccess.stan&#39;, data=standata, chains = 4, warmup = 200, iter = 3000) saveRDS(probsuccess.fit, file = &quot;./data/probsuccsess-fit.RDS&quot;) 2.3 RQ1 Diagnosis 2.3.1 RQ1 Chains convergence The first step is to evaluate the convergence of the chains. We will look now only for the slopes, algorithms intercept and the standard deviation of the random effects (and not each intercept of the random effects) a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;, &quot;a_alg[5]&quot;, &quot;a_alg[6]&quot;, &quot;a_alg[7]&quot;, &quot;a_alg[8]&quot;) b_noise &lt;- c(&quot;b_noise[1]&quot;, &quot;b_noise[2]&quot;, &quot;b_noise[3]&quot;, &quot;b_noise[4]&quot;, &quot;b_noise[5]&quot;, &quot;b_noise[6]&quot;, &quot;b_noise[7]&quot;, &quot;b_noise[8]&quot;) rstan::traceplot(probsuccess.fit, pars=a_alg) rstan::traceplot(probsuccess.fit, pars=b_noise) rstan::traceplot(probsuccess.fit, pars=c(&#39;s&#39;)) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.05 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(probsuccess.fit)$summary) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] -0.1016770 0.0141859 0.4511590 -0.9986581 -0.3944800 -0.1079857 0.1956272 0.7748758 1011.456 1.0007354 a_alg[2] -2.5516066 0.0142459 0.4563417 -3.4560617 -2.8523292 -2.5527132 -2.2471997 -1.6730424 1026.124 1.0009415 a_alg[3] -0.2139745 0.0142481 0.4513760 -1.1136926 -0.5092907 -0.2159365 0.0889257 0.6489137 1003.611 1.0007939 a_alg[4] -4.3498335 0.0142878 0.4667423 -5.2926288 -4.6576410 -4.3486295 -4.0362012 -3.4514857 1067.149 1.0007534 a_alg[5] -0.3943603 0.0142332 0.4506022 -1.2874160 -0.6869869 -0.3994464 -0.0884889 0.4724644 1002.264 1.0009414 a_alg[6] -2.0070362 0.0142076 0.4539332 -2.9122092 -2.3013030 -2.0129352 -1.7030975 -1.1352872 1020.806 1.0008781 a_alg[7] -2.2163631 0.0142042 0.4537016 -3.1183583 -2.5133972 -2.2204326 -1.9159819 -1.3441906 1020.248 1.0009555 a_alg[8] -2.5625822 0.0142418 0.4562100 -3.4753740 -2.8617582 -2.5668217 -2.2559160 -1.6908949 1026.123 1.0008100 b_noise[1] -1.2728595 0.0004634 0.0464911 -1.3649835 -1.3042495 -1.2727833 -1.2419386 -1.1808717 10064.607 1.0003304 b_noise[2] -0.8111614 0.0005341 0.0594260 -0.9269817 -0.8512298 -0.8108461 -0.7706404 -0.6960778 12378.857 0.9998278 b_noise[3] -1.3531582 0.0004555 0.0498441 -1.4530193 -1.3862144 -1.3528896 -1.3200088 -1.2545062 11973.466 1.0003042 b_noise[4] -0.3851807 0.0006594 0.0708802 -0.5254143 -0.4334606 -0.3844210 -0.3357693 -0.2486244 11553.895 0.9998575 b_noise[5] -1.1483855 0.0004288 0.0471090 -1.2413071 -1.1803177 -1.1475632 -1.1166922 -1.0569722 12068.645 0.9999617 b_noise[6] -0.9653479 0.0005154 0.0567297 -1.0794613 -1.0029451 -0.9644473 -0.9267742 -0.8573317 12113.240 0.9999536 b_noise[7] -0.7231598 0.0005147 0.0521852 -0.8278536 -0.7579466 -0.7223159 -0.6875584 -0.6217345 10280.685 0.9998671 b_noise[8] -0.7934309 0.0006002 0.0579600 -0.9074668 -0.8320577 -0.7937229 -0.7548121 -0.6780034 9326.707 1.0000572 a_bm_norm[1] -0.5515980 0.0059809 0.2113896 -0.9712815 -0.6922152 -0.5481803 -0.4065674 -0.1432695 1249.219 1.0005773 a_bm_norm[2] -1.3857790 0.0072688 0.3110012 -2.0315335 -1.5876596 -1.3696647 -1.1724448 -0.8177882 1830.642 1.0004815 a_bm_norm[3] -0.6135924 0.0059932 0.2138498 -1.0370821 -0.7578332 -0.6099309 -0.4669450 -0.1978411 1273.226 1.0007982 a_bm_norm[4] 0.6996375 0.0065183 0.2143535 0.2882285 0.5533039 0.6956576 0.8438217 1.1273589 1081.413 1.0011373 a_bm_norm[5] -0.9190532 0.0063871 0.2451850 -1.4221268 -1.0826093 -0.9142283 -0.7500079 -0.4543208 1473.602 1.0004336 a_bm_norm[6] -0.1266989 0.0057661 0.1905645 -0.5002489 -0.2576990 -0.1251113 0.0010672 0.2486333 1092.226 1.0005111 a_bm_norm[7] 2.0929329 0.0098459 0.3562187 1.4022113 1.8483216 2.0869635 2.3308668 2.8104401 1308.946 1.0019514 a_bm_norm[8] 1.4163381 0.0079819 0.2778883 0.8720145 1.2260972 1.4104389 1.6037712 1.9676857 1212.061 1.0016224 a_bm_norm[9] -2.2875170 0.0080868 0.5070058 -3.3784938 -2.6042096 -2.2496205 -1.9305845 -1.4020841 3930.757 1.0003584 a_bm_norm[10] 0.0376740 0.0057984 0.1890303 -0.3267560 -0.0909227 0.0381280 0.1632858 0.4111242 1062.770 1.0008355 a_bm_norm[11] -0.4683589 0.0058892 0.2054966 -0.8814239 -0.6047223 -0.4628744 -0.3299558 -0.0697175 1217.572 1.0004397 a_bm_norm[12] 0.9705429 0.0072309 0.2352270 0.5211149 0.8117004 0.9658386 1.1296289 1.4391891 1058.256 1.0014069 a_bm_norm[13] 0.1111258 0.0058348 0.1895685 -0.2518913 -0.0206515 0.1106819 0.2360902 0.4897085 1055.563 1.0006293 a_bm_norm[14] -0.4185043 0.0058612 0.2020018 -0.8150537 -0.5543865 -0.4170919 -0.2820809 -0.0200522 1187.764 1.0004502 a_bm_norm[15] -0.2120039 0.0057779 0.1931704 -0.5875451 -0.3435861 -0.2108279 -0.0835340 0.1704687 1117.724 1.0004975 a_bm_norm[16] 0.0582553 0.0057948 0.1899707 -0.3051919 -0.0713240 0.0577533 0.1855058 0.4365644 1074.708 1.0007318 a_bm_norm[17] -0.0945615 0.0058098 0.1905650 -0.4616333 -0.2244295 -0.0938981 0.0338356 0.2816415 1075.888 1.0006509 a_bm_norm[18] 0.3553886 0.0060492 0.1962915 -0.0150112 0.2217934 0.3541406 0.4875904 0.7474879 1052.943 1.0007216 a_bm_norm[19] -1.3873544 0.0071574 0.3144653 -2.0341171 -1.5904221 -1.3696656 -1.1671590 -0.8205984 1930.342 1.0006691 a_bm_norm[20] -0.2103035 0.0057874 0.1925856 -0.5825628 -0.3399080 -0.2079499 -0.0814456 0.1685257 1107.335 1.0007190 a_bm_norm[21] 0.0374586 0.0057911 0.1888960 -0.3244013 -0.0918735 0.0378206 0.1633785 0.4096106 1063.953 1.0007178 a_bm_norm[22] 0.4262087 0.0061486 0.1989057 0.0477541 0.2899258 0.4231280 0.5578995 0.8254036 1046.516 1.0011291 a_bm_norm[23] -0.2208615 0.0057839 0.1927261 -0.5929206 -0.3516246 -0.2213745 -0.0911591 0.1589538 1110.306 1.0006586 a_bm_norm[24] 0.9181966 0.0069786 0.2314096 0.4725721 0.7597149 0.9153235 1.0732483 1.3825348 1099.586 1.0014348 a_bm_norm[25] -0.4800747 0.0058778 0.2060577 -0.8903264 -0.6193752 -0.4793265 -0.3426119 -0.0764864 1228.989 1.0007175 a_bm_norm[26] 1.8127547 0.0090502 0.3223532 1.1907146 1.5925470 1.8072705 2.0297038 2.4651831 1268.663 1.0017888 a_bm_norm[27] -0.3950165 0.0058627 0.2010451 -0.7896719 -0.5304012 -0.3915179 -0.2564086 -0.0006536 1175.971 1.0004427 a_bm_norm[28] -1.5823227 0.0078727 0.3510577 -2.3267529 -1.8102101 -1.5614566 -1.3363711 -0.9473444 1988.408 1.0008761 a_bm_norm[29] 0.4734666 0.0062045 0.2008774 0.0930725 0.3343172 0.4734256 0.6092113 0.8729607 1048.224 1.0011553 a_bm_norm[30] 0.7791124 0.0067026 0.2195527 0.3600034 0.6284312 0.7741367 0.9296758 1.2156303 1072.990 1.0013464 s 2.4396697 0.0091963 0.3666749 1.8524855 2.1797734 2.3975453 2.6487166 3.2807386 1589.761 1.0026073 lp__ -5600.1110570 0.1389985 6.2045095 -5613.2707018 -5604.0278224 -5599.7723066 -5595.7819986 -5588.9259626 1992.483 1.0021429 2.4 RQ1 Results and Plots First lets get the HPDI of every parameter. We do this with the helper function from utils.R. But the function is quite simple. It just converts the stanmodel object to an object that the coda package can read (and do some renaming). Alternatively we can use the HDInterval package. Then we restrict to the algorithms, them to the slopes, then to the other parameters. We create different data frames that we use to plot with ggplot pointrange hpdi &lt;- get_HPDI_from_stanfit(probsuccess.fit) hpdi_oddsratio &lt;- hpdi hpdi_oddsratio$Mean &lt;- exp(hpdi$Mean) hpdi_oddsratio$HPDI.lower &lt;- exp(hpdi$HPDI.lower) hpdi_oddsratio$HPDI.higher &lt;- exp(hpdi$HPDI.higher) hpdi_oddsratio_algorithm &lt;- hpdi_oddsratio %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_oddsratio_b_noise &lt;- hpdi_oddsratio %&gt;% dplyr::filter(str_detect(Parameter, &quot;b_noise\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_s &lt;- hpdi %&gt;% dplyr::filter(Parameter==&#39;s&#39;) p_alg&lt;-ggplot(data=hpdi_oddsratio_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Odds ratio for intercept&quot;, x=&quot;Algorithm&quot;)+ coord_flip() p_alg + plot_annotation(title = &#39;HPDI interval for the algorithms OR&#39;) p_noise &lt;- ggplot(data=hpdi_oddsratio_b_noise, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs( y=&quot;Odds ratio for b_noise&quot;, x=&quot;Parameter&quot;)+ coord_flip()+ theme() p_noise + plot_annotation(title = &#39;HPDI interval for the noise coefficients OR&#39;) p_s &lt;- ggplot(data=hpdi_s, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;s&quot;, x=&quot;Parameter&quot;)+ coord_flip() p_s + plot_annotation(title = &#39;HPDI interval for s&#39;) Creating an output table algreduced &lt;- c(&quot;CMAES&quot;, &quot;Cuckoo&quot;, &quot;DiffEvol.&quot;, &quot;NelderM.&quot;, &quot;PSO&quot;, &quot;RandomS1&quot;,&quot;RandomS2&quot;, &quot;SimAnneal&quot;) rename_pars &lt;- c( paste(rep(&#39;a_&#39;,length(algorithms)),algreduced, sep = &quot;&quot;), paste(rep(&#39;b_&#39;,length(algorithms)),algreduced, sep = &quot;&quot;), &#39;s&#39;) t&lt;-create_table_model(probsuccess.fit, pars = c(a_alg, b_noise, &#39;s&#39;), renamepars = rename_pars) t&lt;- t %&gt;% mutate(&#39;OR Mean&#39; = exp(Mean), &#39;OR HPD low&#39; = exp(HPDI.lower), &#39;OR HPD high&#39; = exp(HPDI.higher)) colnames(t)&lt;-c(&quot;Parameter&quot;, &quot;Mean&quot;, &quot;HPD low&quot;, &quot;HPD high&quot;,&#39;OR Mean&#39;,&#39;OR HPD low&#39;,&#39;OR HPD high&#39;) saveRDS(t,&#39;./statscomp-paper/tables/datafortables/probsuccess-par-table.RDS&#39;) "],["relative-improvement.html", "Chapter 3 Relative improvement 3.1 RQ2 Data preparation 3.2 RQ2 Stan model 3.3 RQ2 Diagnosis 3.4 RQ2 Results and Plots", " Chapter 3 Relative improvement Our next, model deals with relative improvement of the algorithms over a baseline in noiseless functions. This model is based on a normal linear regression. RQ2 What is the expected improvement of these algorithms against the Random Search in noiseless benchmark functions in terms of approaching a global minima based in the Euclidean distance to the location of the closest global minima? 3.1 RQ2 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&#39;./data/statscomp.csv&#39;) Let’s select only the columns that interests us, in this case the Euclidean distance d&lt;- dataset %&gt;% dplyr::select(Algorithm, CostFunction, SD, Budget=MaxFevalPerDimensions, simNumber, EuclideanDistance, OptimizationSuccessful) %&gt;% dplyr::filter(OptimizationSuccessful &amp; SD==0) %&gt;% dplyr::select(-SD, -OptimizationSuccessful) Let’s first make this a wide data set based on the algorithm to make it easier to compute the relative improvement over the Random Search. We are also dropping the RandomSearch2 since there is no noise in the benchmark functions There are several ways that can be used to compute a relative improvement (and they will affect the result). The way we are using is to compare against the mean of distance of the 10 samples of the Random Search in each cost function for a specific budget. The way we are comparing is we divide the distance of each algorithm by the average distance of the random search. If this ratio is greater than 1 then random search is better, if smaller than 1 then the algorithm is better relativeImprovement &lt;- function(x, rs){ #x is the column #rs is the random search column ri &lt;- (rs-x)/rs ri&lt;-ifelse(ri &lt; -1, -1, ri) ri&lt;-ifelse(ri &gt; 1, 1, ri) return(ri) } d_wide &lt;- d %&gt;% tidyr::pivot_wider( names_from = Algorithm, values_from = EuclideanDistance) %&gt;% dplyr::select(-RandomSearch2) %&gt;% dplyr::group_by(CostFunction, Budget) %&gt;% dplyr::mutate(avgRandomSearch=mean(RandomSearch1)) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate_at(c(&quot;NelderMead&quot;, &quot;PSO&quot;, &quot;SimulatedAnnealing&quot;,&quot;CuckooSearch&quot;, &quot;DifferentialEvolution&quot;, &quot;CMAES&quot;), ~relativeImprovement(.x,rs=avgRandomSearch)) After we compute our metric we drop the Random Search column and we pivot_longer again to make the inference d_final &lt;- d_wide %&gt;% dplyr::select(-RandomSearch1, -avgRandomSearch) %&gt;% tidyr::pivot_longer( cols = c(&quot;NelderMead&quot;, &quot;PSO&quot;, &quot;SimulatedAnnealing&quot;,&quot;CuckooSearch&quot;, &quot;DifferentialEvolution&quot;, &quot;CMAES&quot;), names_to = &quot;Algorithm&quot;, values_to = &quot;y&quot;) %&gt;% dplyr::select(-simNumber) %&gt;% dplyr::mutate(AlgorithmID=create_index(Algorithm), CostFunctionID=create_index(CostFunction)) %&gt;% dplyr::select(Algorithm, AlgorithmID, CostFunction, CostFunctionID, Budget, y) #checking if there is any na -&gt; stan does not accept that find.na &lt;- d_final %&gt;% dplyr::filter(is.na(y)) bm&lt;-get_index_names_as_array(d_final$CostFunction) saveRDS(bm, &#39;./data/relativeimprovement_bm.RDS&#39;) algorithms &lt;- get_index_names_as_array(d_final$Algorithm) saveRDS(algorithms, &#39;./data/relativeimprovement_algorithms.RDS&#39;) Now we have our final dataset to use with Stan. Lets preview a sample of the data set kable(dplyr::sample_n(d_final,size=10), &quot;html&quot;,booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Algorithm AlgorithmID CostFunction CostFunctionID Budget y CuckooSearch 2 DiscusN2 6 10000 -1.000 NelderMead 4 Schwefel2d20N2 16 100 -1.000 NelderMead 4 Trefethen 25 1000 -1.000 PSO 5 XinSheYang2N2 29 100 0.962 SimulatedAnnealing 6 RosenbrockRotatedN6 14 10000 -1.000 SimulatedAnnealing 6 XinSheYang2N2 29 100 -1.000 DifferentialEvolution 3 ExponentialN2 7 20 -0.372 PSO 5 RosenbrockRotatedN6 14 100000 0.906 CuckooSearch 2 Tripod 27 100000 -1.000 SimulatedAnnealing 6 LunacekBiRastriginN6 9 20 -0.529 3.2 RQ2 Stan model The Stan model is specified in the file: './stanmodels/relativeimprovement.stan'. Note that at the end of the model we commented the generated quantities. This block generates the predictive posterior y_rep and the log likelihood, log_lik. These values are useful in diagnosing and validating the model but the end file is extremely large (~1Gb for 2000 iterations) and make many of the following calculations slow. If the reader wants to see these values is just to uncomment and run the stan model again print_stan_code(&#39;./stanmodels/relativeimprovement.stan&#39;) // Relative improvement model // Author: David Issa Mattos // Date: 17 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size real y[N_total]; // relative improvement variable //To model each algorithm independently int &lt;lower=1&gt; N_algorithm; // Number of algorithms int algorithm_id[N_total]; //vector that has the id of each algorithm //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { real &lt;lower=0&gt; sigma;//std for the normal //Fixed effect real a_alg[N_algorithm];//the mean effect given by the algorithms // //Random effect. The effect of the benchmarks real a_bm_norm[N_bm];//the mean effect given by the base class type real&lt;lower=0&gt; s;//std for the random effects } model { real mu[N_total]; sigma ~ exponential(1); //Fixed effect a_alg ~ normal(0,1); // //Random effects s ~ exponential(0.1); a_bm_norm ~ normal(0,1); for (i in 1:N_total) { mu[i] = a_alg[algorithm_id[i]] + a_bm_norm[bm_id[i]]*s; } y ~ normal(mu, sigma); } //Uncoment this part to get the posterior predictives and the log likelihood //But note that it takes a lot of space in the final model // generated quantities{ // vector [N_total] y_rep; // vector[N_total] log_lik; // for(i in 1:N_total){ // real mu; // mu = a_alg[algorithm_id[i]] + a_bm_norm[bm_id[i]]*s; // y_rep[i]= normal_rng(mu, sigma); // log_lik[i] = normal_lpdf(y[i] | mu, sigma ); // } // } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/relativeimprovement-data.RDS\" standata &lt;- list( N_total=nrow(d_final), y = d_final$y, N_algorithm = length(algorithms), algorithm_id = d_final$AlgorithmID, N_bm = length(bm), bm_id = d_final$CostFunctionID) saveRDS(standata, file = &quot;./data/relativeimprovement-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately standata&lt;-readRDS(&quot;./data/relativeimprovement-data.RDS&quot;) relativeimprovement.fit &lt;- stan(file = &#39;./stanmodels/relativeimprovement.stan&#39;, data=standata, chains = 4, warmup = 200, iter = 2000) saveRDS(relativeimprovement.fit, file = &quot;./data/relativeimprovement-fit.RDS&quot;) 3.3 RQ2 Diagnosis a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;, &quot;a_alg[5]&quot;, &quot;a_alg[6]&quot;) rstan::traceplot(relativeimprovement.fit, pars=a_alg) rstan::traceplot(relativeimprovement.fit, pars=c(&#39;s&#39;,&#39;sigma&#39;)) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.05 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(relativeimprovement.fit)$summary) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat sigma 0.6366100 0.0000428 0.0046356 0.6279357 0.6334462 0.6365653 0.6396818 0.6457408 11737.9589 0.9998479 a_alg[1] 0.1504431 0.0010669 0.0315679 0.0890274 0.1289946 0.1503498 0.1710892 0.2130964 875.5536 1.0020870 a_alg[2] -0.3782017 0.0010608 0.0312129 -0.4384189 -0.3990024 -0.3785456 -0.3569890 -0.3180648 865.8437 1.0022883 a_alg[3] 0.3016706 0.0010386 0.0318547 0.2394596 0.2803897 0.3016050 0.3229036 0.3637406 940.7541 1.0017231 a_alg[4] -0.6416444 0.0010506 0.0316451 -0.7035318 -0.6628445 -0.6415439 -0.6206061 -0.5804023 907.2309 1.0016704 a_alg[5] 0.3197972 0.0010933 0.0316366 0.2567743 0.2984940 0.3203481 0.3410073 0.3804611 837.4044 1.0015712 a_alg[6] -0.5712294 0.0010744 0.0318686 -0.6329672 -0.5926750 -0.5713187 -0.5504153 -0.5080394 879.8954 1.0023406 a_bm_norm[1] -0.5324372 0.0072268 0.3110141 -1.1446108 -0.7392397 -0.5307194 -0.3175314 0.0609326 1852.0990 1.0003273 a_bm_norm[2] -2.1565188 0.0112839 0.4114065 -3.0139485 -2.4225173 -2.1345244 -1.8665108 -1.4204484 1329.3022 1.0009460 a_bm_norm[3] 0.8557922 0.0073074 0.3220833 0.2453745 0.6347019 0.8489550 1.0702666 1.5008964 1942.7468 1.0012470 a_bm_norm[4] 0.1502905 0.0066763 0.3072522 -0.4326836 -0.0593567 0.1441169 0.3552582 0.7645143 2117.9527 1.0002763 a_bm_norm[5] -1.1326181 0.0085411 0.3420580 -1.8346108 -1.3509023 -1.1248777 -0.8996870 -0.4881811 1603.8856 1.0003229 a_bm_norm[6] 0.8558640 0.0072952 0.3236103 0.2370131 0.6320748 0.8505455 1.0739499 1.4909068 1967.7769 1.0011601 a_bm_norm[7] -0.0413356 0.0071274 0.3039265 -0.6351911 -0.2450705 -0.0405354 0.1630906 0.5561242 1818.3338 1.0004550 a_bm_norm[8] 1.1243779 0.0076636 0.3352797 0.4993539 0.8898000 1.1171430 1.3455862 1.8050221 1914.0470 1.0014555 a_bm_norm[9] 0.4787389 0.0070748 0.3140171 -0.1179473 0.2695584 0.4710521 0.6866548 1.1086989 1970.0285 1.0005173 a_bm_norm[10] 0.3282321 0.0071282 0.3096721 -0.2771011 0.1231111 0.3274623 0.5365737 0.9280143 1887.3190 1.0010798 a_bm_norm[11] 0.8736090 0.0080123 0.3259996 0.2503156 0.6479220 0.8653393 1.0896311 1.5221133 1655.4586 1.0009389 a_bm_norm[12] 0.6707052 0.0072055 0.3167687 0.0749724 0.4535551 0.6644335 0.8764376 1.3154337 1932.6408 1.0013575 a_bm_norm[13] -1.7386842 0.0098444 0.3781142 -2.5231052 -1.9816060 -1.7269537 -1.4711017 -1.0392991 1475.2544 1.0006415 a_bm_norm[14] 0.8892515 0.0074731 0.3241643 0.2790440 0.6662089 0.8769090 1.1051927 1.5387143 1881.5965 1.0009292 a_bm_norm[15] -1.0726736 0.0084882 0.3314618 -1.7423574 -1.2824677 -1.0642625 -0.8519186 -0.4486825 1524.8706 1.0007155 a_bm_norm[16] 0.1074685 0.0070940 0.3051650 -0.4907630 -0.0949131 0.1019742 0.3119469 0.7071876 1850.4962 1.0010560 a_bm_norm[17] 0.8224704 0.0072394 0.3262720 0.2010029 0.5977758 0.8164466 1.0357533 1.4659379 2031.1884 1.0005378 a_bm_norm[18] 0.7667547 0.0075036 0.3201146 0.1585328 0.5478409 0.7561602 0.9779430 1.3995464 1819.9792 1.0015047 a_bm_norm[19] -0.5677922 0.0072741 0.3134924 -1.2106486 -0.7726446 -0.5606116 -0.3540789 0.0217070 1857.3671 1.0002155 a_bm_norm[20] 0.0879460 0.0068471 0.3058517 -0.5149716 -0.1169447 0.0908125 0.2943910 0.6794939 1995.2988 1.0007190 a_bm_norm[21] -0.5632527 0.0073789 0.3167070 -1.2067040 -0.7682321 -0.5597471 -0.3499427 0.0516084 1842.1702 1.0005945 a_bm_norm[22] 1.1724562 0.0077659 0.3403050 0.5270203 0.9331018 1.1642364 1.4008483 1.8550582 1920.2270 1.0014713 a_bm_norm[23] -1.2555759 0.0087782 0.3440404 -1.9785188 -1.4762408 -1.2379681 -1.0227795 -0.6103550 1536.0428 1.0003350 a_bm_norm[24] 0.2698611 0.0068884 0.3067795 -0.3418640 0.0682229 0.2669627 0.4770308 0.8782026 1983.4420 1.0004909 a_bm_norm[25] -0.7249007 0.0075140 0.3210720 -1.3799455 -0.9392957 -0.7174527 -0.5078526 -0.1048101 1825.8476 1.0005200 a_bm_norm[26] 1.4013859 0.0081970 0.3558229 0.7488342 1.1587726 1.3878845 1.6302152 2.1367271 1884.3446 1.0012795 a_bm_norm[27] -1.0722893 0.0083682 0.3332402 -1.7443855 -1.2929783 -1.0652907 -0.8431567 -0.4500125 1585.8196 1.0003164 a_bm_norm[28] -0.3734760 0.0069790 0.3079066 -1.0152636 -0.5677904 -0.3693947 -0.1721274 0.2091721 1946.4702 1.0004438 a_bm_norm[29] -0.5220881 0.0076369 0.3137439 -1.1577763 -0.7261937 -0.5122625 -0.3076597 0.0602610 1687.7736 1.0007462 a_bm_norm[30] 1.0909753 0.0079913 0.3376618 0.4643900 0.8590997 1.0825904 1.3123522 1.7709307 1785.3890 1.0013195 s 0.1462747 0.0006600 0.0215428 0.1102501 0.1309906 0.1443111 0.1596464 0.1940269 1065.5445 1.0017016 lp__ -453.8280738 0.1664049 5.9138935 -466.3807087 -457.4939235 -453.3995258 -449.6758589 -443.5918651 1263.0328 1.0020458 3.4 RQ2 Results and Plots First lets get the HPDI of every parameter. Then we restrict to the algorithms, them to the slopes, then to the hpdi &lt;- get_HPDI_from_stanfit(relativeimprovement.fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_other_parameters &lt;- hpdi %&gt;% dplyr::filter(Parameter==&#39;s&#39; | Parameter==&#39;sigma&#39;) p_alg&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate of intercept&quot;, x=&quot;Algorithm&quot;)+ coord_flip() p_alg + plot_annotation(title = &#39;HPDI interval for the algorithms&#39;) p_others &lt;- ggplot(data=hpdi_other_parameters, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate of s and sigma&quot;, x=&quot;Parameter&quot;)+ coord_flip() p_others + plot_annotation(title = &#39;HPDI interval&#39;) Creating an output table rename_pars &lt;- c(&#39;sigma&#39;,paste(rep(&#39;a_&#39;,length(algorithms)), algorithms, sep = &quot;&quot;),&#39;s&#39;) t&lt;-create_table_model(relativeimprovement.fit, c(a_alg, &#39;s&#39;, &#39;sigma&#39;), rename_pars) colnames(t)&lt;-c(&quot;Parameter&quot;, &quot;Mean&quot;, &quot;HPD low&quot;, &quot;HPD high&quot;) saveRDS(t,&#39;./statscomp-paper/tables/datafortables/relativeimprovement-par-table.RDS&#39;) kable(t) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Parameter Mean HPD low HPD high sigma 0.6366100 0.6277880 0.6455934 a_CMAES 0.1504431 0.0913038 0.2148819 a_CuckooSearch -0.3782017 -0.4384310 -0.3180672 a_DifferentialEvolution 0.3016706 0.2414597 0.3649906 a_NelderMead -0.6416444 -0.7048234 -0.5826826 a_PSO 0.3197972 0.2563871 0.3798412 a_SimulatedAnnealing -0.5712294 -0.6317232 -0.5068983 s 0.1462747 0.1085577 0.1907648 "],["ranking.html", "Chapter 4 Ranking 4.1 RQ3 Data preparation 4.2 RQ3 Stan model 4.3 RQ3 Diagnosis 4.4 RQ3 Results and Plots", " Chapter 4 Ranking In this section, we will consider the Bradley-Terry Model for ranking algorithms in the fixed budget of 10,000 function evaluations per dimension and controlling for noise and the effect of benchmark functions RQ3: How can we rank algorithm different optimization algorithms given a budget of 10,000 evaluations per dimension in noisy benchmarks? 4.1 RQ3 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&#39;./data/statscomp.csv&#39;) The BT model formulation that we use has a specific data format, where we have one column with algo_0 (with index of each algorithm) another column with algo_1 and a third column with who won (algo 0 or algo 1), First lets select only the data that we are interested and create ranking by the each run in each group (by the simNumber). To avoid ties (dealing with those on next session) we will rank ties randomly d1 &lt;- dataset %&gt;% dplyr::select(Algorithm, CostFunction, SD, Budget=MaxFevalPerDimensions, simNumber, TrueRewardDifference, OptimizationSuccessful) %&gt;% dplyr::filter(OptimizationSuccessful &amp; Budget==10000 &amp; SD==3.0) %&gt;% dplyr::select(-Budget, -OptimizationSuccessful, -SD) %&gt;% dplyr::group_by(CostFunction, simNumber) %&gt;% dplyr::mutate(rankReward=rank(TrueRewardDifference, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-TrueRewardDifference) kable(dplyr::sample_n(d1,size=10), booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Algorithm CostFunction simNumber rankReward NelderMead Trigonometric1N6 2 1 CMAES DiscusN2 7 7 SimulatedAnnealing ChenV 1 7 RandomSearch1 BentCigarN6 7 4 CMAES Price1 8 6 CMAES ChenV 2 6 CuckooSearch Shubert 8 4 RandomSearch2 LunacekBiRastriginN6 1 5 SimulatedAnnealing Mishra7N6 8 8 CMAES StrechedVSineWave2N 8 3 Now to compare the ranks we need to pivot wider the data frame and based on that we will expand to the dataset in the appropriated format d1_wide &lt;- d1 %&gt;% tidyr::pivot_wider(names_from = Algorithm, values_from=rankReward) kable(dplyr::sample_n(d1_wide,size=10), booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) CostFunction simNumber NelderMead PSO SimulatedAnnealing CuckooSearch DifferentialEvolution RandomSearch1 RandomSearch2 CMAES SphereN6 6 8 1 7 6 2 5 4 3 WhitleyN6 2 8 3 4 7 2 6 5 1 Schwefel2d4N6 8 8 3 7 6 2 4 5 1 Damavandi 9 8 6 7 1 2 4 3 5 Shubert 1 8 3 2 7 4 5 6 1 WhitleyN6 9 8 3 7 6 1 4 5 2 Mishra7N6 0 4 1 8 6 3 5 7 2 Damavandi 8 8 5 4 7 6 3 1 2 XinSheYang2N2 9 6 2 1 4 7 8 3 5 DiscusN2 9 8 3 1 6 4 2 5 7 Now we need to modify this data set and expand it so we have the pairwise comparisons First let’s get the number of algorithms and create combination of all possible 2 by 2 comparisons without repeating algorithms &lt;- get_index_names_as_array(d1$Algorithm) n_algorithms &lt;- length(algorithms) comb &lt;- gtools::combinations(n=n_algorithms, r=2, v=seq(1:n_algorithms), repeats.allowed = F) The pairs combinations looks like this (for algo_0 and algo_1): kable(comb, booktabs=T) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) 1 2 1 3 1 4 1 5 1 6 1 7 1 8 2 3 2 4 2 5 2 6 2 7 2 8 3 4 3 5 3 6 3 7 3 8 4 5 4 6 4 7 4 8 5 6 5 7 5 8 6 7 6 8 7 8 Note that each row of d_wide will be expanded into 28 rows. Giving a dataset with a total of 8400 rows. The following code can a bit slow to run due to the double for loops (there is probably a way to vectorize this and make it run faster), but for building this appendix we will not run, instead we will run it once, save this data, and load it when needed. It takes a couple of minutes but if you have a lot of data and algorithms it can easily go for hours We will use a progress bar to follow the data frame creation. 1- We initialize a tibble data frame 2- First we loop through the wide data frame d1_wide row by row 3- For each row we will loop through the different combinations in the comb variable to create the rows of the data frame. We add each row to the initial dataframe pb &lt;- progress::progress_bar$new(format = &quot;[:bar] :current/:total (:percent)&quot;, total = nrow(d1_wide)) df_out &lt;- dplyr::tribble(~algo0_name, ~algo0, ~algo1_name, ~algo1, ~y, ~simNumber, ~CostFunction) for(i in 1:nrow(d1_wide)) { current_row &lt;- d1_wide[i,] for(j in 1:nrow(comb)){ comb_row &lt;- comb[j,] algo0_name &lt;- algorithms[comb_row[1]] algo0 &lt;- comb_row[1] algo0_rank &lt;- current_row[[1,algo0_name]] algo1_name &lt;- algorithms[comb_row[2]] algo1 &lt;- comb_row[2] algo1_rank &lt;- current_row[[1,algo1_name]] diff_rank &lt;- algo1_rank - algo0_rank y &lt;- ifelse(diff_rank&lt;0, 1, 0) df_out &lt;- add_row(df_out, algo0_name=algo0_name, algo0=algo0, algo1_name=algo1_name, algo1=algo1, y=y, simNumber=current_row$simNumber, CostFunction=current_row$CostFunction) } pb$tick() } saveRDS(df_out, file=&quot;./data/ranking.RDS&quot;) Adding index for the benchmarks df_out &lt;- readRDS(&quot;./data/ranking.RDS&quot;) df_out$CostFunctionId &lt;- create_index(df_out$CostFunction) benchmarks &lt;- get_index_names_as_array(df_out$CostFunction) Visualizing how the data frame looks like kable(dplyr::sample_n(df_out,size=10), &quot;html&quot;, booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) algo0_name algo0 algo1_name algo1 y simNumber CostFunction CostFunctionId RandomSearch1 6 RandomSearch2 7 1 2 Trigonometric1N6 26 CMAES 1 PSO 5 0 2 ExponentialN2 7 CMAES 1 RandomSearch2 7 0 1 Schwefel2d20N2 16 CuckooSearch 2 RandomSearch1 6 1 9 QingN2 13 NelderMead 4 RandomSearch2 7 1 0 ChenBird 2 CMAES 1 PSO 5 1 7 ZakharovN2 30 NelderMead 4 RandomSearch1 6 1 0 ChungReynoldsN2 4 CuckooSearch 2 RandomSearch2 7 1 3 Schwefel2d23N6 18 DifferentialEvolution 3 SimulatedAnnealing 8 0 2 BentCigarN6 1 PSO 5 RandomSearch1 6 0 8 SalomonN2 15 4.2 RQ3 Stan model The Stan model is specified in the file: './stanmodels/rankingmodel_withcluster.stan'. print_stan_code(&#39;./stanmodels/rankingmodel_withcluster.stan&#39;) // Ranking model with cluster data // Author: David Issa Mattos // Date: 1 Oct 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size int y[N_total]; //variable that indicates which one wins algo0 oor algo 1 int &lt;lower=1&gt; N_algorithm; // Number of algorithms int &lt;lower=1&gt; algo0[N_total]; int &lt;lower=1&gt; algo1[N_total]; // //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { real a_alg[N_algorithm]; //Latent variable that represents the strength value of each algorithm real&lt;lower=0&gt; s;//std for the random effects matrix[N_algorithm, N_bm] Uij; //parameters of the random effects for cluster } model { real p[N_total]; a_alg ~ normal(0,2); s ~ exponential(0.1); for (i in 1:N_algorithm) { for(j in 1:N_bm){ Uij[i, j] ~ normal(0, 1); } } for (i in 1:N_total) { p[i] = (a_alg[algo1[i]] + s*Uij[algo1[i], bm_id[i]]) - (a_alg[algo0[i]] + s*Uij[algo0[i], bm_id[i]] ) ; } y ~ bernoulli_logit(p); } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/rankingmodel-withcluster-data.RDS\" For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately standata &lt;- list( N_total=nrow(df_out), y = as.integer(df_out$y), N_algorithm = length(algorithms), algo0=df_out$algo0, algo1=df_out$algo1, bm_id=df_out$CostFunctionId, N_bm=length(benchmarks) ) saveRDS(standata, file = &quot;./data/rankingmodel-withcluster-data.RDS&quot;) standata&lt;-readRDS(&quot;./data/rankingmodel-withcluster-data.RDS&quot;) ranking.fit &lt;- stan(file = &#39;./stanmodels/rankingmodel_withcluster.stan&#39;, data=standata, chains = 4, warmup = 200, iter = 2000) saveRDS(ranking.fit, file = &quot;./data/ranking-with-cluster-fit.RDS&quot;) ranking.fit &lt;-readRDS(&quot;./data/ranking-with-cluster-fit.RDS&quot;) a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;, &quot;a_alg[5]&quot;, &quot;a_alg[6]&quot;, &quot;a_alg[7]&quot;, &quot;a_alg[8]&quot;) 4.3 RQ3 Diagnosis rstan::traceplot(ranking.fit, pars=c(a_alg,&#39;s&#39;)) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.05 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the posteriors. Note that we have several random effects parameter estimates. kable(summary(ranking.fit)$summary) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] 1.0350990 0.0099375 0.7818263 -0.4889451 0.5245045 1.0272608 1.5465781 2.5859266 6189.692 0.9998847 a_alg[2] -0.4086096 0.0097442 0.7778534 -1.9583610 -0.9293837 -0.4082742 0.1144884 1.1325767 6372.434 0.9996527 a_alg[3] 1.9855326 0.0101607 0.7833351 0.4453243 1.4707039 1.9837454 2.5001817 3.5373887 5943.627 0.9997423 a_alg[4] -2.9812765 0.0103527 0.7967958 -4.5108464 -3.5157775 -2.9724022 -2.4480854 -1.4303523 5923.626 1.0001171 a_alg[5] 1.5786103 0.0100043 0.7871080 0.0462318 1.0553736 1.5831165 2.0952817 3.1108493 6190.105 0.9997322 a_alg[6] 0.2820188 0.0099938 0.7906221 -1.2662524 -0.2510642 0.2702926 0.8240528 1.8092083 6258.569 0.9998661 a_alg[7] 0.2511935 0.0098370 0.7876031 -1.2844233 -0.2782489 0.2510030 0.7890238 1.7830516 6410.446 0.9998134 a_alg[8] -1.6873176 0.0100228 0.7843961 -3.2354333 -2.2125968 -1.6899210 -1.1695586 -0.1369124 6124.853 0.9997644 s 1.8186475 0.0026735 0.1185007 1.5974726 1.7367581 1.8133418 1.8958155 2.0619577 1964.622 1.0014523 Uij[1,1] 2.7568246 0.0068488 0.6168957 1.5905147 2.3314248 2.7379874 3.1716803 4.0020500 8113.306 0.9999433 Uij[1,2] -2.0117811 0.0054712 0.4613464 -2.9433107 -2.3196997 -1.9983697 -1.6997434 -1.1434574 7110.256 0.9998827 Uij[1,3] -1.0160761 0.0048771 0.4231998 -1.8436816 -1.2967851 -1.0130569 -0.7369512 -0.1841409 7529.647 1.0000163 Uij[1,4] 0.3563913 0.0048953 0.4326111 -0.4691134 0.0613761 0.3507387 0.6504564 1.2140202 7809.887 0.9998143 Uij[1,5] -0.1472215 0.0046975 0.4254996 -0.9748673 -0.4345510 -0.1545260 0.1406149 0.6839556 8204.801 1.0001625 Uij[1,6] -2.3973034 0.0057218 0.5129544 -3.4075592 -2.7420730 -2.3964285 -2.0419191 -1.4212910 8037.052 1.0008644 Uij[1,7] -1.4078985 0.0049489 0.4284437 -2.2523282 -1.6954565 -1.4018171 -1.1196755 -0.5791312 7495.086 1.0005379 Uij[1,8] -0.7674291 0.0050325 0.4228531 -1.6081325 -1.0460228 -0.7639979 -0.4808151 0.0586314 7060.226 0.9997273 Uij[1,9] 1.1122770 0.0052789 0.4955415 0.1426358 0.7797746 1.1032002 1.4400893 2.1038175 8812.076 1.0002491 Uij[1,10] 1.3826008 0.0056059 0.4816300 0.4669824 1.0563422 1.3832680 1.7059654 2.3426203 7381.241 1.0003584 Uij[1,11] 1.0280591 0.0051854 0.4890837 0.0807931 0.6962308 1.0149521 1.3558423 2.0016199 8896.151 0.9998362 Uij[1,12] -0.3671331 0.0049811 0.4098012 -1.1727494 -0.6382965 -0.3626950 -0.0948469 0.4406861 6768.464 1.0000148 Uij[1,13] 0.7401377 0.0051266 0.4382564 -0.0975171 0.4425075 0.7363724 1.0347060 1.6099385 7307.848 1.0004634 Uij[1,14] 1.5977910 0.0051860 0.4924602 0.6419273 1.2637216 1.5975200 1.9360789 2.5583447 9017.419 0.9998956 Uij[1,15] -2.8218921 0.0064887 0.5298789 -3.8887420 -3.1737296 -2.8081934 -2.4572131 -1.8102215 6668.585 1.0001931 Uij[1,16] -0.3449782 0.0049085 0.4315929 -1.1923280 -0.6397734 -0.3394316 -0.0544866 0.4954096 7731.414 1.0001765 Uij[1,17] -0.2023738 0.0048264 0.4312314 -1.0474327 -0.4889775 -0.1996496 0.0878083 0.6298177 7983.114 1.0000599 Uij[1,18] 1.4790742 0.0051569 0.4860841 0.5489460 1.1450392 1.4778527 1.8072492 2.4212830 8884.640 0.9996503 Uij[1,19] 1.1583333 0.0055574 0.4782449 0.2307281 0.8415235 1.1501705 1.4766778 2.0871754 7405.594 0.9997612 Uij[1,20] 2.4816319 0.0067180 0.5918140 1.3702240 2.0746444 2.4577437 2.8679558 3.6880249 7760.453 1.0000875 Uij[1,21] -0.1058211 0.0046657 0.4166140 -0.9158034 -0.3822645 -0.1072440 0.1680613 0.7142994 7973.206 0.9996953 Uij[1,22] 0.7798184 0.0050579 0.4805069 -0.1575611 0.4546586 0.7763659 1.1039509 1.7378154 9025.204 0.9999745 Uij[1,23] -0.1487943 0.0049382 0.4198536 -0.9823316 -0.4372502 -0.1484105 0.1368763 0.6776679 7228.541 0.9996262 Uij[1,24] -0.6146686 0.0046842 0.4090193 -1.4306410 -0.8911070 -0.6109428 -0.3324223 0.1675987 7624.646 1.0001753 Uij[1,25] -0.8634452 0.0050265 0.4174748 -1.6889816 -1.1475820 -0.8666918 -0.5766713 -0.0342703 6898.128 0.9999936 Uij[1,26] -0.5845752 0.0046152 0.4126612 -1.3717355 -0.8738679 -0.5869563 -0.3024878 0.2312067 7994.861 1.0003718 Uij[1,27] -1.5679692 0.0051020 0.4514158 -2.4771515 -1.8722151 -1.5655713 -1.2647471 -0.7010626 7828.501 1.0005969 Uij[1,28] 1.6374628 0.0054645 0.5310866 0.6053352 1.2810035 1.6363277 1.9869884 2.7069031 9445.681 0.9998314 Uij[1,29] -0.3994496 0.0046884 0.4095300 -1.2048221 -0.6695395 -0.4006865 -0.1184461 0.4011749 7629.896 0.9998467 Uij[1,30] -0.2747694 0.0049231 0.4199378 -1.0941249 -0.5581011 -0.2793798 0.0090081 0.5555291 7275.939 0.9997294 Uij[2,1] -0.9118255 0.0050621 0.4614396 -1.8303036 -1.2250132 -0.9132414 -0.5948358 -0.0307307 8309.262 1.0002471 Uij[2,2] 0.7360394 0.0048665 0.4189589 -0.0820554 0.4524802 0.7333332 1.0246713 1.5382982 7411.590 0.9995565 Uij[2,3] 0.1855127 0.0048088 0.4110061 -0.6349756 -0.0859329 0.1803901 0.4523463 1.0080215 7305.198 0.9995848 Uij[2,4] -0.1561007 0.0052024 0.4522417 -1.0472026 -0.4601719 -0.1544331 0.1495431 0.7313382 7556.670 1.0000256 Uij[2,5] 0.5686377 0.0046033 0.4241218 -0.2657993 0.2837517 0.5759036 0.8515573 1.3920251 8488.576 0.9999016 Uij[2,6] -0.2454198 0.0049806 0.4431801 -1.0994024 -0.5538180 -0.2392124 0.0499348 0.6258264 7917.615 1.0002062 Uij[2,7] 0.3005368 0.0047756 0.4116191 -0.5117091 0.0234118 0.3024190 0.5774570 1.1091651 7429.163 1.0007859 Uij[2,8] 0.1038937 0.0049971 0.4161380 -0.7240727 -0.1731505 0.1094736 0.3813164 0.9073294 6934.755 1.0001131 Uij[2,9] -0.6147997 0.0049292 0.4569832 -1.5164804 -0.9174888 -0.6101217 -0.3046565 0.2881951 8595.067 1.0001000 Uij[2,10] -0.0839954 0.0047354 0.4267774 -0.9161901 -0.3756604 -0.0886199 0.2093155 0.7524626 8122.392 1.0000538 Uij[2,11] -0.4093252 0.0049963 0.4525534 -1.3046255 -0.7126732 -0.4082073 -0.1030230 0.4635166 8204.379 1.0000034 Uij[2,12] 0.3225484 0.0048765 0.4059902 -0.4764681 0.0408683 0.3262363 0.6025226 1.0987600 6931.354 1.0005904 Uij[2,13] 0.3573765 0.0051299 0.4326365 -0.4866159 0.0707737 0.3597662 0.6501994 1.1939192 7112.531 1.0000188 Uij[2,14] -0.2901255 0.0048619 0.4497415 -1.1704280 -0.6003112 -0.2920998 0.0169833 0.5864769 8556.787 1.0000544 Uij[2,15] 0.7022558 0.0049771 0.4222572 -0.1150877 0.4195670 0.7010846 0.9836285 1.5317844 7197.845 0.9997664 Uij[2,16] 0.6945807 0.0049894 0.4272260 -0.1328749 0.4047755 0.6916793 0.9795240 1.5469742 7331.952 0.9998481 Uij[2,17] -0.2185615 0.0048720 0.4382104 -1.0984592 -0.5134980 -0.2113982 0.0781627 0.6276812 8090.155 1.0001446 Uij[2,18] -0.3206384 0.0049597 0.4692944 -1.2375007 -0.6393195 -0.3115535 -0.0046453 0.6043356 8953.384 0.9997829 Uij[2,19] 0.4651950 0.0046032 0.4258835 -0.3668718 0.1747166 0.4626164 0.7540787 1.2940660 8559.786 0.9998108 Uij[2,20] -0.9697870 0.0048787 0.4669374 -1.9028781 -1.2808106 -0.9648317 -0.6616987 -0.0596379 9160.448 1.0001722 Uij[2,21] 0.0559637 0.0047342 0.4247027 -0.7753249 -0.2333481 0.0550506 0.3469145 0.8794904 8047.959 0.9996142 Uij[2,22] -1.3275595 0.0051314 0.4840651 -2.2914464 -1.6502408 -1.3208356 -1.0067086 -0.3925727 8899.026 0.9995716 Uij[2,23] 0.0839835 0.0049526 0.4195062 -0.7455468 -0.2031012 0.0874359 0.3604104 0.9212649 7174.795 1.0000113 Uij[2,24] 0.1323553 0.0048279 0.4103965 -0.6649977 -0.1476288 0.1315250 0.4118975 0.9341050 7225.840 1.0000912 Uij[2,25] 0.3719959 0.0049697 0.4207036 -0.4631670 0.0958649 0.3710583 0.6513204 1.1997728 7166.234 0.9997759 Uij[2,26] -0.2964895 0.0045362 0.4157191 -1.1284417 -0.5798734 -0.3012985 -0.0138295 0.5166549 8398.689 0.9999439 Uij[2,27] 0.8768972 0.0049258 0.4350635 0.0285952 0.5838022 0.8779145 1.1687862 1.7280956 7801.049 1.0003535 Uij[2,28] -0.8093466 0.0049757 0.4614552 -1.7218763 -1.1187694 -0.8034172 -0.4947902 0.0785906 8601.011 0.9998047 Uij[2,29] 0.0758406 0.0046521 0.4096718 -0.7413744 -0.1903349 0.0764793 0.3478485 0.8696867 7754.897 1.0005194 Uij[2,30] 0.4084189 0.0048212 0.4170506 -0.4126051 0.1222951 0.4151572 0.6837595 1.2171550 7482.812 0.9996785 Uij[3,1] 1.0073477 0.0056024 0.5272701 -0.0070510 0.6585322 0.9983542 1.3568953 2.0481410 8857.636 1.0004760 Uij[3,2] 0.4494440 0.0055275 0.4612767 -0.4445557 0.1406110 0.4484923 0.7448691 1.3775233 6964.002 0.9998257 Uij[3,3] -1.6023314 0.0050154 0.4282851 -2.4575609 -1.8902213 -1.6014886 -1.3103747 -0.7596825 7292.256 0.9996174 Uij[3,4] 0.3180840 0.0047771 0.4399761 -0.5296537 0.0255057 0.3126647 0.6131122 1.1813983 8482.623 1.0000645 Uij[3,5] -0.6641079 0.0045801 0.4269222 -1.4850328 -0.9579079 -0.6660101 -0.3689882 0.1608834 8688.658 0.9999617 Uij[3,6] 0.2326513 0.0050891 0.4413233 -0.6207068 -0.0653579 0.2305346 0.5307241 1.1156567 7520.371 1.0001218 Uij[3,7] -0.9653100 0.0047645 0.4100695 -1.7632959 -1.2479738 -0.9651005 -0.6912685 -0.1526064 7407.724 1.0001151 Uij[3,8] -1.2842741 0.0052116 0.4172326 -2.1196463 -1.5598092 -1.2809310 -1.0027277 -0.4882449 6409.460 0.9996239 Uij[3,9] 0.5335109 0.0051922 0.4905067 -0.4254207 0.2079019 0.5362266 0.8650988 1.4927805 8924.692 0.9996950 Uij[3,10] 0.1541278 0.0050599 0.4510499 -0.7386530 -0.1543444 0.1553414 0.4548776 1.0559184 7946.308 1.0004800 Uij[3,11] 1.6121375 0.0058877 0.5383345 0.5858994 1.2456417 1.5952324 1.9730634 2.7112403 8360.260 1.0000395 Uij[3,12] -1.0101996 0.0048170 0.4106617 -1.8339218 -1.2856831 -1.0116214 -0.7348951 -0.2074507 7268.152 1.0000251 Uij[3,13] -0.5352153 0.0051829 0.4322857 -1.3682473 -0.8216571 -0.5326458 -0.2435968 0.3136118 6956.658 1.0005153 Uij[3,14] 1.2543606 0.0054120 0.4888469 0.3119591 0.9204890 1.2494931 1.5864753 2.2339286 8158.870 1.0001832 Uij[3,15] -0.2721445 0.0049936 0.4223608 -1.0855721 -0.5648840 -0.2729688 0.0089531 0.5674364 7153.844 1.0003756 Uij[3,16] -0.0410410 0.0047780 0.4303627 -0.8665422 -0.3387241 -0.0507189 0.2497476 0.8206513 8112.979 0.9998610 Uij[3,17] 1.2865672 0.0062544 0.5398753 0.2606436 0.9166113 1.2718928 1.6461927 2.3826536 7451.016 0.9995780 Uij[3,18] 1.0438479 0.0053805 0.4852059 0.0991268 0.7090153 1.0447925 1.3705275 1.9849483 8132.276 0.9999111 Uij[3,19] 1.0255994 0.0056218 0.4974867 0.0639425 0.6872134 1.0176544 1.3561308 1.9942321 7830.821 1.0000818 Uij[3,20] 0.5705030 0.0051968 0.4763676 -0.3519639 0.2448030 0.5655583 0.8977213 1.5109338 8402.703 0.9998861 Uij[3,21] -0.0469374 0.0050064 0.4346849 -0.8891189 -0.3387192 -0.0530623 0.2465612 0.8252608 7538.760 0.9997205 Uij[3,22] 0.8907837 0.0054451 0.4955141 -0.0686155 0.5476846 0.8889436 1.2243206 1.8669988 8281.294 0.9996579 Uij[3,23] -0.8606064 0.0048879 0.4111195 -1.6729920 -1.1410176 -0.8582076 -0.5856793 -0.0586748 7074.423 0.9998325 Uij[3,24] -0.9155754 0.0047352 0.4078674 -1.7183378 -1.1856843 -0.9162233 -0.6417720 -0.1151063 7419.371 1.0001275 Uij[3,25] -0.7202299 0.0050933 0.4212868 -1.5601059 -1.0034157 -0.7171780 -0.4375285 0.1005430 6841.499 0.9999412 Uij[3,26] -0.9790499 0.0045789 0.4110545 -1.7859790 -1.2523830 -0.9781484 -0.7031598 -0.1634507 8058.986 1.0000878 Uij[3,27] 0.2145017 0.0048992 0.4430642 -0.6478430 -0.0804793 0.2087423 0.5071635 1.0929758 8178.624 1.0004294 Uij[3,28] 2.0605100 0.0062671 0.5785991 0.9638991 1.6584069 2.0490457 2.4419682 3.2332636 8523.600 0.9999511 Uij[3,29] -1.3502330 0.0047122 0.4112604 -2.1525129 -1.6270620 -1.3519898 -1.0796666 -0.5433599 7616.959 0.9999308 Uij[3,30] -0.5482071 0.0048715 0.4214139 -1.3656972 -0.8377365 -0.5534726 -0.2540713 0.2871019 7483.374 0.9998348 Uij[4,1] -1.0547976 0.0057939 0.5258410 -2.1423295 -1.4061579 -1.0444071 -0.7014771 -0.0599811 8237.012 0.9997875 Uij[4,2] 1.4152369 0.0050122 0.4222379 0.5847297 1.1320818 1.4190580 1.7011842 2.2436516 7096.794 0.9996116 Uij[4,3] 1.2888832 0.0051489 0.4264335 0.4681648 0.9979293 1.2831645 1.5828093 2.1267815 6859.112 0.9995560 Uij[4,4] -0.2175562 0.0055988 0.5128904 -1.2512756 -0.5593706 -0.2155310 0.1363785 0.7734667 8391.771 0.9999658 Uij[4,5] -0.9999722 0.0082125 0.6873597 -2.4298974 -1.4415019 -0.9586839 -0.5131495 0.2527938 7005.226 0.9998558 Uij[4,6] -0.3850099 0.0054081 0.5058200 -1.3978102 -0.7159066 -0.3739554 -0.0426153 0.5630651 8747.817 0.9997977 Uij[4,7] 2.2690834 0.0051886 0.4365670 1.4157615 1.9724727 2.2711768 2.5615590 3.1315005 7079.385 1.0000048 Uij[4,8] 1.1511079 0.0051838 0.4241700 0.3032800 0.8721010 1.1546695 1.4351798 1.9730284 6695.441 1.0006260 Uij[4,9] -1.4868073 0.0079324 0.6467099 -2.8417259 -1.8958611 -1.4631879 -1.0382503 -0.3124344 6646.675 0.9999836 Uij[4,10] 0.8863267 0.0052023 0.4409030 0.0126595 0.5910444 0.8893569 1.1766049 1.7544285 7182.722 1.0005518 Uij[4,11] -1.5339463 0.0077762 0.6502043 -2.9039419 -1.9486211 -1.5010588 -1.0846510 -0.3422617 6991.480 1.0001814 Uij[4,12] 0.2032391 0.0055015 0.4590260 -0.6980710 -0.1016079 0.2084355 0.5165290 1.0649153 6961.560 1.0001354 Uij[4,13] -0.3925842 0.0060215 0.5415076 -1.4888802 -0.7477202 -0.3743811 -0.0213750 0.6247565 8087.330 1.0003910 Uij[4,14] -1.0686745 0.0057736 0.5331029 -2.1423804 -1.4144230 -1.0586174 -0.7113862 -0.0299121 8525.592 1.0000143 Uij[4,15] 0.5335171 0.0050969 0.4502252 -0.3564267 0.2312813 0.5306487 0.8325837 1.4213166 7802.747 1.0003882 Uij[4,16] -0.5016088 0.0057299 0.5521439 -1.6211547 -0.8615124 -0.4936333 -0.1214575 0.5467288 9285.621 0.9999972 Uij[4,17] 0.1705628 0.0054261 0.4751121 -0.7500838 -0.1530714 0.1707272 0.5002412 1.0745915 7666.979 0.9995502 Uij[4,18] -0.5736415 0.0055927 0.5318950 -1.6265162 -0.9343312 -0.5703938 -0.2000789 0.4496235 9044.941 0.9998754 Uij[4,19] 0.2321728 0.0049765 0.4541364 -0.6816344 -0.0712835 0.2388763 0.5432059 1.1044883 8327.613 0.9997770 Uij[4,20] -1.6276982 0.0070903 0.6298337 -2.9424722 -2.0267361 -1.6110177 -1.1951056 -0.4638435 7890.771 1.0001433 Uij[4,21] -1.0568413 0.0082121 0.6954488 -2.5205506 -1.4852024 -1.0267179 -0.5783466 0.2099793 7171.753 0.9999914 Uij[4,22] -1.6012507 0.0063108 0.5688484 -2.7643341 -1.9668914 -1.5925988 -1.2223617 -0.5185397 8125.108 0.9997419 Uij[4,23] 0.8565832 0.0050033 0.4290898 0.0053646 0.5724052 0.8574249 1.1540984 1.6820064 7355.129 1.0002642 Uij[4,24] 0.5869816 0.0051309 0.4326763 -0.2741647 0.2885961 0.5883067 0.8834859 1.4277965 7111.227 0.9999143 Uij[4,25] 0.3390502 0.0052386 0.4531598 -0.5562876 0.0367418 0.3431145 0.6411253 1.2120223 7482.901 0.9996849 Uij[4,26] 1.4815727 0.0049017 0.4226446 0.6749818 1.1930830 1.4713979 1.7658538 2.3244273 7434.608 1.0003777 Uij[4,27] 0.1659433 0.0051940 0.4600652 -0.7582899 -0.1361634 0.1733586 0.4785447 1.0384514 7845.866 1.0000199 Uij[4,28] -1.2806149 0.0065112 0.5706343 -2.4592248 -1.6526341 -1.2604375 -0.8855718 -0.1926687 7680.498 1.0004478 Uij[4,29] 0.8940605 0.0048851 0.4256570 0.0509766 0.6162184 0.8931302 1.1736058 1.7256979 7592.286 1.0009941 Uij[4,30] -0.0123213 0.0056647 0.4860611 -0.9555250 -0.3465494 -0.0018815 0.3236089 0.9059362 7362.537 0.9997726 Uij[5,1] 0.1610819 0.0054275 0.4909976 -0.8066471 -0.1726553 0.1590742 0.4973047 1.1202438 8183.943 1.0000650 Uij[5,2] -0.6879136 0.0049420 0.4074034 -1.4862566 -0.9626568 -0.6935778 -0.4132975 0.1100455 6795.886 0.9996164 Uij[5,3] -0.0763661 0.0049768 0.4293301 -0.9076048 -0.3660849 -0.0797143 0.2124873 0.7678842 7441.954 0.9997382 Uij[5,4] 0.2008752 0.0049600 0.4412798 -0.6542883 -0.1016702 0.2010888 0.4999876 1.0749466 7915.213 1.0001994 Uij[5,5] -0.3791894 0.0043556 0.4237388 -1.2022060 -0.6605014 -0.3786616 -0.0887175 0.4374964 9464.666 0.9997955 Uij[5,6] -0.1396903 0.0048631 0.4298054 -0.9736264 -0.4308501 -0.1412236 0.1514906 0.7022374 7811.186 1.0004841 Uij[5,7] -1.1925048 0.0049356 0.4194588 -2.0305877 -1.4782757 -1.1897950 -0.9138555 -0.3823593 7222.731 1.0006622 Uij[5,8] -0.8229707 0.0049344 0.4154215 -1.6529174 -1.0997631 -0.8125020 -0.5490555 -0.0065503 7087.623 0.9999104 Uij[5,9] 1.6620525 0.0055731 0.5287226 0.6613757 1.2985690 1.6572067 2.0100333 2.7178656 9000.311 1.0000451 Uij[5,10] 0.5745866 0.0053482 0.4564669 -0.3198793 0.2746407 0.5730956 0.8819339 1.4688604 7284.481 0.9997806 Uij[5,11] 0.7421610 0.0051307 0.4834058 -0.2162685 0.4145210 0.7401900 1.0622076 1.7063505 8877.106 0.9997100 Uij[5,12] -0.4984991 0.0048439 0.4109265 -1.3099250 -0.7759348 -0.4925025 -0.2199153 0.2952362 7196.885 1.0002638 Uij[5,13] 0.0104689 0.0050218 0.4268828 -0.8355194 -0.2790088 0.0134855 0.3011566 0.8343938 7225.920 1.0001855 Uij[5,14] 0.7068502 0.0051046 0.4768913 -0.2217838 0.3906000 0.6988112 1.0216474 1.6436442 8728.075 1.0000220 Uij[5,15] 0.3028525 0.0049623 0.4293768 -0.5440520 0.0162137 0.3035211 0.5943020 1.1275868 7486.961 0.9997411 Uij[5,16] 0.6871776 0.0049884 0.4377777 -0.1735442 0.3843984 0.6889344 0.9892943 1.5299438 7701.531 0.9997284 Uij[5,17] -0.2144385 0.0048584 0.4336255 -1.0704499 -0.5107113 -0.2090975 0.0789814 0.6316863 7965.900 1.0001001 Uij[5,18] 1.1159882 0.0049244 0.4791961 0.1927222 0.7980919 1.1176990 1.4388016 2.0453534 9469.342 0.9997376 Uij[5,19] -1.2582857 0.0048452 0.4314994 -2.1236219 -1.5442036 -1.2615177 -0.9687659 -0.4378307 7931.232 0.9998592 Uij[5,20] -0.2284588 0.0045918 0.4491880 -1.1084821 -0.5317220 -0.2309842 0.0753132 0.6530863 9569.604 0.9998917 Uij[5,21] -0.2624154 0.0046866 0.4258832 -1.0855067 -0.5432064 -0.2705560 0.0211492 0.5852048 8257.903 0.9997635 Uij[5,22] 2.5212396 0.0070576 0.6089231 1.3708082 2.1078268 2.5018016 2.9109950 3.7519946 7444.068 0.9998780 Uij[5,23] -0.4108674 0.0049068 0.4091119 -1.2175339 -0.6914275 -0.4069610 -0.1268971 0.3721501 6951.642 0.9995629 Uij[5,24] -0.7564929 0.0045936 0.4066680 -1.5624618 -1.0303299 -0.7567261 -0.4871905 0.0455760 7837.313 0.9998797 Uij[5,25] -0.3547424 0.0050576 0.4265396 -1.1826898 -0.6438791 -0.3546845 -0.0677634 0.4885426 7112.606 0.9997734 Uij[5,26] -0.5484119 0.0046012 0.4103649 -1.3405153 -0.8286629 -0.5513117 -0.2747002 0.2700166 7954.267 1.0006088 Uij[5,27] 0.2238965 0.0051198 0.4396453 -0.6290873 -0.0772777 0.2267578 0.5168140 1.1079665 7373.867 1.0004717 Uij[5,28] 0.2166104 0.0051487 0.4884776 -0.7501314 -0.1182536 0.2132065 0.5396021 1.1746844 9001.187 0.9998080 Uij[5,29] -0.2949915 0.0046411 0.4090049 -1.1099320 -0.5640623 -0.2939212 -0.0247612 0.5016446 7766.342 0.9999574 Uij[5,30] -0.2160241 0.0048692 0.4178932 -1.0203039 -0.5016691 -0.2152328 0.0640756 0.5903540 7365.686 1.0002223 Uij[6,1] -0.6247991 0.0050568 0.4536326 -1.5268210 -0.9293660 -0.6256297 -0.3168351 0.2622814 8047.318 1.0000593 Uij[6,2] -0.4187735 0.0048960 0.4103144 -1.2329170 -0.6884198 -0.4175403 -0.1399352 0.3985669 7023.360 0.9996310 Uij[6,3] 0.0985780 0.0051401 0.4209715 -0.7269067 -0.1840676 0.1003228 0.3809159 0.9276387 6707.564 0.9995862 Uij[6,4] 0.2659752 0.0049877 0.4436955 -0.6081839 -0.0374348 0.2662519 0.5762164 1.1168504 7913.396 1.0002142 Uij[6,5] 0.0299219 0.0046911 0.4241500 -0.7971327 -0.2598001 0.0291514 0.3223569 0.8465939 8175.147 1.0002137 Uij[6,6] 0.6451811 0.0048321 0.4292219 -0.1860810 0.3565973 0.6461714 0.9270589 1.4933868 7890.377 1.0003926 Uij[6,7] 0.2538168 0.0046767 0.4086395 -0.5461182 -0.0309398 0.2551826 0.5362982 1.0505904 7634.826 1.0003590 Uij[6,8] 0.4274660 0.0049731 0.4179161 -0.4025759 0.1493242 0.4315904 0.7056309 1.2356792 7061.842 1.0000938 Uij[6,9] -0.5395962 0.0046649 0.4501384 -1.4029731 -0.8487781 -0.5398695 -0.2310737 0.3404310 9311.042 1.0002101 Uij[6,10] -0.8084559 0.0048717 0.4339794 -1.6550548 -1.1022997 -0.8044388 -0.5155709 0.0343890 7935.489 1.0003847 Uij[6,11] -0.6109773 0.0048729 0.4495825 -1.4976911 -0.9182983 -0.6088736 -0.3072446 0.2606366 8512.265 0.9999330 Uij[6,12] -0.1475828 0.0049173 0.4079584 -0.9443651 -0.4288301 -0.1429433 0.1330495 0.6301994 6882.900 0.9998486 Uij[6,13] 0.7110351 0.0050460 0.4275339 -0.1339759 0.4235922 0.7136160 0.9927999 1.5569792 7178.797 1.0007566 Uij[6,14] -0.1825220 0.0049111 0.4478515 -1.0738590 -0.4814001 -0.1794772 0.1207994 0.6942991 8316.060 1.0008052 Uij[6,15] 0.8600299 0.0052773 0.4319325 0.0020161 0.5668183 0.8583955 1.1470909 1.7159696 6698.982 0.9998806 Uij[6,16] 0.5314289 0.0049489 0.4213566 -0.2937868 0.2466523 0.5299128 0.8109240 1.3717170 7249.108 0.9998220 Uij[6,17] 0.1982816 0.0047830 0.4284962 -0.6233876 -0.0986099 0.1980872 0.4916411 1.0310962 8025.852 1.0001748 Uij[6,18] -0.0586963 0.0049996 0.4614932 -0.9639492 -0.3712287 -0.0581867 0.2532450 0.8373281 8520.296 0.9996152 Uij[6,19] -0.0080057 0.0049024 0.4306798 -0.8471121 -0.2934495 -0.0079689 0.2831372 0.8383824 7717.910 1.0000249 Uij[6,20] 0.2211151 0.0048568 0.4510694 -0.6568711 -0.0825393 0.2210293 0.5232522 1.0989568 8625.567 0.9999425 Uij[6,21] -0.0495386 0.0047408 0.4168294 -0.8652290 -0.3299256 -0.0516559 0.2319948 0.7689865 7730.764 0.9997201 Uij[6,22] -0.3717223 0.0053212 0.4680513 -1.2991666 -0.6835655 -0.3701338 -0.0622979 0.5494174 7737.028 0.9996392 Uij[6,23] -0.3540491 0.0048657 0.4145042 -1.1688097 -0.6318058 -0.3490797 -0.0734128 0.4408301 7257.065 0.9999383 Uij[6,24] 0.1959043 0.0048074 0.4080546 -0.6184873 -0.0689125 0.1926992 0.4701189 1.0018415 7204.835 0.9996239 Uij[6,25] 0.4137847 0.0050166 0.4231668 -0.4038988 0.1272633 0.4125788 0.7014688 1.2340569 7115.622 0.9996148 Uij[6,26] -0.2363742 0.0045652 0.4101442 -1.0430403 -0.5183638 -0.2328827 0.0350588 0.5735567 8071.441 1.0000106 Uij[6,27] 0.5553174 0.0049240 0.4363689 -0.2960677 0.2593486 0.5495635 0.8525548 1.4199907 7853.639 1.0000431 Uij[6,28] -0.4486609 0.0048792 0.4598944 -1.3344911 -0.7507847 -0.4533277 -0.1409905 0.4652987 8884.061 0.9996815 Uij[6,29] -0.2704751 0.0047704 0.4091097 -1.0781167 -0.5409605 -0.2680438 -0.0000097 0.5355939 7354.713 1.0005713 Uij[6,30] -0.1030006 0.0048923 0.4204442 -0.9290630 -0.3878017 -0.1069345 0.1813979 0.7205199 7385.697 0.9998357 Uij[7,1] -0.6686367 0.0050088 0.4557680 -1.5845243 -0.9699501 -0.6700753 -0.3655302 0.2316460 8279.956 1.0004251 Uij[7,2] -0.2566409 0.0049552 0.4096727 -1.0817867 -0.5276101 -0.2558151 0.0157505 0.5481599 6835.313 0.9997654 Uij[7,3] 0.2504599 0.0049710 0.4217731 -0.5675008 -0.0370700 0.2473867 0.5333353 1.0803985 7199.055 0.9998415 Uij[7,4] 0.6859212 0.0050999 0.4419246 -0.1685289 0.3834376 0.6864736 0.9898366 1.5408539 7508.894 0.9999479 Uij[7,5] 0.5750226 0.0045292 0.4292228 -0.2560126 0.2836859 0.5738421 0.8653033 1.4180462 8980.797 1.0000379 Uij[7,6] 0.5765717 0.0049524 0.4323608 -0.2676764 0.2743627 0.5737352 0.8682920 1.4216896 7621.919 1.0007105 Uij[7,7] 0.1360901 0.0049709 0.4130857 -0.6627489 -0.1475640 0.1364560 0.4186504 0.9491536 6905.786 1.0008082 Uij[7,8] 0.1086326 0.0050418 0.4155035 -0.7096056 -0.1769173 0.1150681 0.3988567 0.9075658 6791.593 0.9996560 Uij[7,9] -0.6912170 0.0049313 0.4570350 -1.5732257 -1.0027622 -0.6937236 -0.3799759 0.1878556 8589.708 1.0003142 Uij[7,10] -1.2988127 0.0051996 0.4439050 -2.1663917 -1.6026462 -1.2937276 -0.9931531 -0.4350331 7288.674 1.0002294 Uij[7,11] -0.6481199 0.0051177 0.4543147 -1.5596388 -0.9505406 -0.6381087 -0.3428891 0.2293004 7880.840 1.0000041 Uij[7,12] 0.2787110 0.0048952 0.4131514 -0.5265008 -0.0000545 0.2783038 0.5566697 1.0958326 7123.327 1.0001954 Uij[7,13] 0.6035767 0.0051274 0.4296069 -0.2423933 0.3187230 0.6027932 0.8956915 1.4443235 7020.208 1.0003450 Uij[7,14] -0.7790838 0.0049209 0.4487950 -1.6773649 -1.0776840 -0.7779394 -0.4793608 0.1081412 8317.766 1.0002652 Uij[7,15] 0.6962269 0.0049467 0.4255781 -0.1138881 0.4092854 0.6967696 0.9822225 1.5231681 7401.711 0.9999640 Uij[7,16] 0.3818410 0.0048435 0.4306669 -0.4638307 0.0867805 0.3875423 0.6719341 1.2334873 7906.206 0.9999178 Uij[7,17] 0.3570449 0.0048365 0.4263812 -0.4960780 0.0792652 0.3549124 0.6429293 1.1736216 7771.964 1.0005065 Uij[7,18] -0.4615351 0.0048152 0.4633419 -1.3737511 -0.7695673 -0.4553099 -0.1494646 0.4327476 9259.286 0.9995530 Uij[7,19] -0.2435080 0.0047296 0.4275416 -1.0939372 -0.5311344 -0.2456423 0.0485947 0.5775747 8171.696 0.9998685 Uij[7,20] -0.0021774 0.0048818 0.4498383 -0.8823389 -0.3053032 0.0004164 0.2954453 0.8920188 8490.945 0.9999999 Uij[7,21] 0.2828488 0.0048261 0.4227466 -0.5486289 -0.0020694 0.2827153 0.5625054 1.1015473 7673.025 0.9998777 Uij[7,22] 0.2111218 0.0053773 0.4739329 -0.7270469 -0.1077460 0.2117789 0.5240689 1.1405523 7768.035 0.9999473 Uij[7,23] -0.4348333 0.0051215 0.4176412 -1.2611891 -0.7182458 -0.4295870 -0.1606398 0.3867412 6649.742 0.9997878 Uij[7,24] 0.1139629 0.0047761 0.4086911 -0.6701003 -0.1686072 0.1173195 0.3945152 0.9236310 7322.104 1.0003798 Uij[7,25] 0.3257470 0.0048997 0.4168573 -0.5012675 0.0457719 0.3306800 0.6062877 1.1562440 7238.422 1.0002241 Uij[7,26] 0.1063559 0.0045177 0.4060904 -0.6927307 -0.1628985 0.1005658 0.3767784 0.9125101 8079.949 0.9998306 Uij[7,27] 0.2417210 0.0047309 0.4304883 -0.6090570 -0.0453813 0.2394792 0.5293926 1.0927608 8280.224 1.0002968 Uij[7,28] -0.8042228 0.0049591 0.4580414 -1.7158913 -1.1046225 -0.8117571 -0.4960766 0.0870590 8531.002 0.9998705 Uij[7,29] 0.2501043 0.0044318 0.4036534 -0.5421777 -0.0243821 0.2531225 0.5273927 1.0249491 8295.709 0.9999089 Uij[7,30] 0.1830485 0.0049280 0.4130757 -0.6115548 -0.0974593 0.1832852 0.4598386 0.9832646 7026.119 0.9999477 Uij[8,1] -0.6797147 0.0052015 0.4675453 -1.5951908 -0.9912328 -0.6836537 -0.3690497 0.2368825 8079.467 1.0000451 Uij[8,2] 0.7530444 0.0048968 0.4133600 -0.0688056 0.4750426 0.7585286 1.0313103 1.5524937 7125.768 0.9998760 Uij[8,3] 0.8767076 0.0049982 0.4194564 0.0467125 0.5999791 0.8764537 1.1592419 1.7065229 7042.932 0.9996744 Uij[8,4] -1.3931510 0.0060144 0.5399612 -2.4761996 -1.7511404 -1.3889283 -1.0223754 -0.3514750 8060.139 0.9997169 Uij[8,5] 1.0293903 0.0045726 0.4264227 0.1928708 0.7396803 1.0308027 1.3155476 1.8587793 8696.731 0.9998040 Uij[8,6] 1.6629825 0.0052373 0.4351741 0.7973662 1.3748512 1.6663331 1.9563794 2.5070325 6904.170 1.0001921 Uij[8,7] 0.6377627 0.0048004 0.4135980 -0.1742209 0.3574055 0.6422804 0.9095237 1.4354207 7423.358 1.0006607 Uij[8,8] 1.1000020 0.0051864 0.4224096 0.2546472 0.8194431 1.1004458 1.3877358 1.9241408 6633.460 1.0002896 Uij[8,9] 0.0173745 0.0049549 0.4526369 -0.8638235 -0.2838410 0.0200288 0.3198568 0.8929469 8345.180 0.9998040 Uij[8,10] -0.8083359 0.0050599 0.4564115 -1.7030567 -1.1153930 -0.8080988 -0.4964228 0.0680808 8136.347 0.9997124 Uij[8,11] -0.2558014 0.0050491 0.4605078 -1.1509798 -0.5615123 -0.2536856 0.0486054 0.6454935 8318.568 0.9997518 Uij[8,12] 1.2360673 0.0051452 0.4127775 0.4359177 0.9547951 1.2324319 1.5174841 2.0433249 6436.101 1.0001881 Uij[8,13] -1.4924469 0.0062529 0.5559413 -2.6339068 -1.8553862 -1.4735235 -1.1110078 -0.4568472 7904.775 0.9999526 Uij[8,14] -1.2421309 0.0056253 0.5036012 -2.2304790 -1.5726332 -1.2420751 -0.9088601 -0.2481939 8014.580 1.0007184 Uij[8,15] -0.0076941 0.0053476 0.4471726 -0.8754602 -0.3123613 -0.0002802 0.2946612 0.8626898 6992.616 0.9998336 Uij[8,16] -1.3804106 0.0059410 0.5642882 -2.5352336 -1.7493490 -1.3675735 -1.0004769 -0.3081336 9021.487 0.9997923 Uij[8,17] -1.3944819 0.0062225 0.5364612 -2.4952343 -1.7546625 -1.3827286 -1.0274493 -0.3913429 7432.617 1.0001927 Uij[8,18] -2.1729524 0.0063300 0.5818025 -3.3246618 -2.5606298 -2.1556583 -1.7737270 -1.0943774 8447.919 0.9998806 Uij[8,19] -1.3740754 0.0058964 0.5164762 -2.4153540 -1.7141067 -1.3672165 -1.0261003 -0.3724640 7672.202 1.0001024 Uij[8,20] -0.4537292 0.0049502 0.4750314 -1.3906823 -0.7768226 -0.4501857 -0.1315285 0.4734747 9208.560 1.0002069 Uij[8,21] 1.1933456 0.0049073 0.4256895 0.3661585 0.9043413 1.1891295 1.4818523 2.0227081 7524.770 0.9995517 Uij[8,22] -1.0947476 0.0054924 0.4983996 -2.0852497 -1.4272500 -1.0819927 -0.7559473 -0.1399502 8234.507 0.9995982 Uij[8,23] 1.3536038 0.0050467 0.4211611 0.5327337 1.0705955 1.3657610 1.6351796 2.1770506 6964.331 0.9998251 Uij[8,24] 1.2921909 0.0051902 0.4152882 0.4850221 1.0101123 1.2892075 1.5709785 2.1062766 6402.122 1.0001962 Uij[8,25] 0.4860198 0.0049657 0.4251098 -0.3635677 0.2099469 0.4869041 0.7721711 1.3240835 7328.959 0.9998540 Uij[8,26] 1.0710632 0.0047021 0.4094161 0.2780966 0.7864531 1.0680716 1.3409294 1.8947021 7581.408 1.0002360 Uij[8,27] -0.7347508 0.0053484 0.4636911 -1.6525125 -1.0457051 -0.7355525 -0.4167170 0.1814844 7516.329 1.0003472 Uij[8,28] -0.5760778 0.0049569 0.4655035 -1.4891086 -0.8932855 -0.5756722 -0.2580407 0.3286495 8819.219 0.9999394 Uij[8,29] 1.0755167 0.0047433 0.4125635 0.2759515 0.7979133 1.0762203 1.3591285 1.8652500 7565.156 0.9997998 Uij[8,30] 0.5604330 0.0049169 0.4190213 -0.2687412 0.2728515 0.5647204 0.8447049 1.3685136 7262.553 0.9999104 lp__ -3349.4802784 0.4482605 15.6277068 -3380.9917464 -3359.5467733 -3349.4478159 -3338.7833075 -3319.5832264 1215.429 1.0031396 4.4 RQ3 Results and Plots First let’s get the HPDI interval for the “strength” parameters. Then we will sample the posterior and rank them and present the ranks with their respective posteriors. hpdi &lt;- get_HPDI_from_stanfit(ranking.fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels p_alg&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate&quot;, x=&quot;Algorithm&quot;, title = &quot;HPDI interval of the strength of the algorithms&quot;)+ coord_flip() p_alg #+ plot_annotation(title = &#39;HPDI interval for the algorithms strength&#39;) Computing the ranks posterior &lt;- rstan::extract(ranking.fit) a_alg &lt;- as_tibble(posterior$a_alg) colnames(a_alg) &lt;- algorithms #sampling from the posterior s &lt;- dplyr::sample_n(a_alg, size = 1000, replace=T) s &lt;- dplyr::mutate(s, rown = row_number()) wide_s &lt;- tidyr::pivot_longer(s, cols=all_of(algorithms), names_to = &quot;Algorithm&quot;, values_to = &quot;a_alg&quot;) rank_df &lt;- wide_s %&gt;% dplyr::group_by(rown) %&gt;% dplyr::mutate(Rank = rank(-a_alg, ties.method = &#39;random&#39;)) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-a_alg) %&gt;% dplyr::group_by(Algorithm) %&gt;% dplyr::summarise(MedianRank = median(Rank), VarianceRank = var(Rank)) %&gt;% dplyr::arrange(MedianRank) Probability of CMAES to beat Random Search and probability of Differential Evolution beating random search inv_logit &lt;- function(x){ y&lt;-exp(x)/(1+exp(x)) return(y) } p_cmaes_beat_rs &lt;- as.data.frame(inv_logit(s$CMAES-s$RandomSearch1)) colnames(p_cmaes_beat_rs) &lt;- c(&#39;x&#39;) quantile(p_cmaes_beat_rs$x, 0.05) 5% 0.497772 quantile(p_cmaes_beat_rs$x, 0.95) 95% 0.819959 quantile(p_cmaes_beat_rs$x, 0.5) 50% 0.6769183 #raw data draw &lt;- df_out %&gt;% dplyr::filter(algo0_name==&#39;CMAES&#39; &amp; algo1_name==&#39;RandomSearch1&#39;) (nrow(draw)-sum(draw$y))/nrow(draw) #average of the data [1] 0.5833333 # # p_de_beat_rs &lt;- as.data.frame(inv_logit(s$DifferentialEvolution-s$RandomSearch1)) # colnames(p_de_beat_rs) &lt;- c(&#39;x&#39;) # quantile(p_de_beat_rs$x, 0.05) # quantile(p_de_beat_rs$x, 0.95) # quantile(p_de_beat_rs$x, 0.5) we can see that in this case the probability of CMAES beating RS is between 0.50-0.82 with average of 0.67 rank_df_table &lt;- rank_df colnames(rank_df_table) &lt;- c(&quot;Algorithm&quot;,&quot;Median Rank&quot;, &quot;Variance of the Rank&quot;) kable(rank_df_table, &quot;html&quot;) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Algorithm Median Rank Variance of the Rank DifferentialEvolution 1 0.1961962 PSO 2 0.3008569 CMAES 3 0.3364004 RandomSearch1 4 0.4719429 RandomSearch2 5 0.5139530 CuckooSearch 6 0.2099339 SimulatedAnnealing 7 0.0069980 NelderMead 8 0.0049800 a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;, &quot;a_alg[5]&quot;, &quot;a_alg[6]&quot;, &quot;a_alg[7]&quot;, &quot;a_alg[8]&quot;) rename_pars &lt;- c(paste(rep(&#39;a_&#39;,length(algorithms)),algorithms, sep = &quot;&quot;),&#39;s&#39;) t&lt;-create_table_model(ranking.fit, pars = c(a_alg, &#39;s&#39;), renamepars = rename_pars) colnames(t)&lt;-c(&quot;Parameter&quot;, &quot;Mean&quot;, &quot;HPD low&quot;, &quot;HPD high&quot;) saveRDS(t,&#39;./statscomp-paper/tables/datafortables/ranking-par-table.RDS&#39;) "],["function-evaluation-to-converge.html", "Chapter 5 Function Evaluation to converge 5.1 RQ4 Data preparation 5.2 RQ4 Stan model 5.3 RQ4 Diagnosis 5.4 RQ4 Results and Plots", " Chapter 5 Function Evaluation to converge In this section, we will consider the Cox’s Proportional Hazard model for analyzing the time to converge to a a solution (in number of iterations). RQ4-a: What is the average number of function evaluations taken by an algorithm to converge to a solution at a precision of \\(\\epsilon=0.1\\), with a budget of 100,000 function evaluations per dimension? RQ4-b: What is the impact of noise in the number of function evaluations taken by an algorithm to converge to a solution at a precision of \\(\\epsilon=0.1\\), with a budget of 100,000 function evaluations per dimension? 5.1 RQ4 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&#39;./data/statscomp.csv&#39;) Filtering the data that we want and applying some transformations. The Event variable will indicate if it was censored or not. d &lt;- dataset %&gt;% dplyr::filter(OptimizationSuccessful==TRUE &amp; MaxFevalPerDimensions==100000 &amp; (Algorithm==&quot;PSO&quot;|Algorithm==&quot;CMAES&quot;|Algorithm==&quot;DifferentialEvolution&quot;|Algorithm==&quot;RandomSearch1&quot;)) %&gt;% dplyr::select(Algorithm, CostFunction, Event=&quot;SolveAt1e-1&quot;, simNumber, Ndimensions, SD, SolvedAtIteration=&quot;SolveEarlierAt1e-1&quot;) %&gt;% dplyr::mutate(y=SolvedAtIteration/Ndimensions, Event=as.integer(Event), CostFunctionID=create_index(CostFunction), AlgorithmID=create_index(Algorithm)) %&gt;% dplyr::select(Algorithm, AlgorithmID, CostFunction, CostFunctionID, SD, Event, y,-simNumber,-SolvedAtIteration, -Ndimensions) algorithms&lt;-get_index_names_as_array(d$Algorithm) bm &lt;- get_index_names_as_array(d$CostFunction) The data should look like this: kable(dplyr::sample_n(d,size=10),&quot;html&quot;, booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Algorithm AlgorithmID CostFunction CostFunctionID SD Event y PSO 3 DiscusN2 6 0 1 44850.000 RandomSearch1 4 Schwefel2d26N6 19 0 0 NA DifferentialEvolution 2 ExponentialN2 7 3 0 0.500 CMAES 1 DiscusN2 6 0 0 NA RandomSearch1 4 Tripod 27 3 0 NA CMAES 1 SphereN6 22 0 1 40.333 PSO 3 WhitleyN6 28 3 0 NA CMAES 1 PinterN6 11 3 0 NA CMAES 1 SphereN6 22 3 0 46.667 PSO 3 Schwefel2d23N6 18 3 0 237.833 5.2 RQ4 Stan model The Stan model is specified in the file: './stanmodels/timetoconverge.stan'. Note that at the end of the model we commented the generated quantities. This block generates the predictive posterior y_rep and the log likelihood, log_lik. These values are useful in diagnosing and validating the model but the end file is extremely large (~1Gb for 2000 iterations) and make many of the following calculations slow. If the reader wants to see these values is just to uncomment and run the stan model again. Note also the the predictive posterior calculates for censored and non censored data. We can in r restrict and compare the predictive only to the non censored data or even censor it if the prediction is above the budget. print_stan_code(&#39;./stanmodels/timetoconverge.stan&#39;) // Time to converge, Cox regression model // Author: David Issa Mattos // Date: 23 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size real y[N_total]; // iteration where it was solved int event[N_total]; // Indicates if the event occured or not //To model each algorithm independently int &lt;lower=1&gt; N_algorithm; // Number of algorithms int algorithm_id[N_total]; //vector that has the id of each algorithm //To model the influence of the noise real x_noise[N_total]; //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { //Fixed effect real a_alg[N_algorithm];//the mean effect given by the algorithms real b_noise[N_algorithm];//effect of noise // //Random effect. The effect of the benchmarks real a_bm_norm[N_bm];//the mean effect given by the base class type real&lt;lower=0&gt; s;//std for the random effects } model { //Fixed effect a_alg ~ normal(0,10); // //Random effects s ~ exponential(0.1); a_bm_norm ~ normal(0,1); b_noise ~ normal(0,2); for (i in 1:N_total) { //uncensored data if(event[i]==1) target += exponential_lpdf(y[i] | exp(a_alg[algorithm_id[i]] + s*a_bm_norm[bm_id[i]] + b_noise[algorithm_id[i]]*x_noise[i])); //censored data if(event[i]==0) target += exponential_lccdf(y[i] | exp(a_alg[algorithm_id[i]] + s*a_bm_norm[bm_id[i]] + b_noise[algorithm_id[i]]*x_noise[i])); } } //Uncoment this part to get the posterior predictives and the log likelihood //But note that it takes a lot of space in the final model // //Here we suppose that the predictive data will not be censored. // //But if it is above the budget we can censor it later // generated quantities{ // vector [N_total] y_rep; // vector[N_total] log_lik; // // for(i in 1:N_total){ // real mu; // mu = a_alg[algorithm_id[i]] + s*a_bm_norm[bm_id[i]] + b_noise[algorithm_id[i]]*x_noise[i]; // y_rep[i]= exponential_rng(mu); // // //uncensored data // if(event[i]==1) log_lik[i]= exponential_lpdf(y[i] | exp(a_alg[algorithm_id[i]] + s*a_bm_norm[bm_id[i]] + b_noise[algorithm_id[i]]*x_noise[i])); // //censored data // if(event[i]==0) log_lik[i]= exponential_lccdf(y[i] | exp(a_alg[algorithm_id[i]] + s*a_bm_norm[bm_id[i]] + b_noise[algorithm_id[i]]*x_noise[i])); // } // } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/timetoconverge-data.RDS\" Note that stan does not support NA in the data, so we have two options… We either replace NA for a value and add conditionals in the model (note that this value will not be used). Or we separate the data frame in two parts, censored and not not-censored. We will do the first approach replacing the NA by 0. dstan&lt;-d %&gt;% dplyr::mutate(y=replace_na(y,0)) standata &lt;- list( N_total=nrow(dstan), y = dstan$y, event = dstan$Event, x_noise = d$SD, N_algorithm = length(algorithms), algorithm_id = dstan$AlgorithmID, N_bm = length(bm), bm_id = d$CostFunctionID ) saveRDS(standata, file = &quot;./data/timetoconverge-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately standata&lt;-readRDS(&quot;./data/timetoconverge-data.RDS&quot;) timetoconverge_fit &lt;- stan(file = &#39;./stanmodels/timetoconverge.stan&#39;, data=standata, chains = 4, warmup = 200, iter = 3000) saveRDS(timetoconverge_fit, file = &quot;./data/timetoconverge-fit.RDS&quot;) 5.3 RQ4 Diagnosis a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;) b_noise &lt;- c(&quot;b_noise[1]&quot;, &quot;b_noise[2]&quot;, &quot;b_noise[3]&quot;, &quot;b_noise[4]&quot;) rstan::traceplot(timetoconverge_fit, pars=a_alg) rstan::traceplot(timetoconverge_fit, pars=b_noise) rstan::traceplot(timetoconverge_fit, pars=&#39;s&#39;) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.05 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(timetoconverge_fit)$summary) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] -5.0748271 0.0148421 0.4735059 -5.9748027 -5.4004954 -5.0811150 -4.7594485 -4.1269815 1017.792 1.0062787 a_alg[2] -6.3871417 0.0148498 0.4697594 -7.2845576 -6.7112468 -6.3891614 -6.0738266 -5.4345811 1000.714 1.0063466 a_alg[3] -6.2898339 0.0147225 0.4692970 -7.1799064 -6.6112746 -6.2918339 -5.9776548 -5.3393856 1016.088 1.0060641 a_alg[4] -8.6911983 0.0150255 0.4779351 -9.6079250 -9.0188308 -8.6949784 -8.3717172 -7.7361202 1011.760 1.0058845 b_noise[1] -0.7841948 0.0006829 0.0700337 -0.9254156 -0.8292292 -0.7826036 -0.7362581 -0.6519258 10517.588 1.0003918 b_noise[2] -0.9531764 0.0006549 0.0648313 -1.0847994 -0.9960676 -0.9522515 -0.9085118 -0.8291181 9801.285 0.9996793 b_noise[3] -0.6750192 0.0007456 0.0618255 -0.8012824 -0.7157411 -0.6735213 -0.6329539 -0.5578453 6875.241 1.0000335 b_noise[4] -0.5780610 0.0008348 0.0804612 -0.7382183 -0.6322695 -0.5761187 -0.5230502 -0.4264751 9289.680 0.9999791 a_bm_norm[1] -0.6965849 0.0063953 0.2408707 -1.1898297 -0.8556381 -0.6907584 -0.5320683 -0.2328818 1418.538 1.0038465 a_bm_norm[2] -0.5777261 0.0068955 0.4316043 -1.5625677 -0.8336753 -0.5360934 -0.2752100 0.1556809 3917.799 1.0009448 a_bm_norm[3] -0.6236323 0.0063588 0.2271429 -1.0817263 -0.7748316 -0.6180451 -0.4685312 -0.1923110 1275.990 1.0041453 a_bm_norm[4] 0.1265026 0.0058734 0.1951705 -0.2565235 -0.0069494 0.1270324 0.2643450 0.4997121 1104.208 1.0059306 a_bm_norm[5] -0.5759095 0.0060949 0.2554721 -1.0923145 -0.7454205 -0.5713378 -0.3982293 -0.0911667 1756.939 1.0032683 a_bm_norm[6] -0.9766402 0.0067009 0.2393889 -1.4608203 -1.1301936 -0.9717149 -0.8151335 -0.5193319 1276.273 1.0039329 a_bm_norm[7] 2.7370066 0.0101284 0.4209890 1.9367581 2.4502287 2.7263716 3.0244169 3.5721570 1727.664 1.0031417 a_bm_norm[8] 1.7538444 0.0079298 0.3113164 1.1560189 1.5404985 1.7501828 1.9682233 2.3553327 1541.290 1.0039046 a_bm_norm[9] -0.0041220 0.0108223 1.0026192 -1.9926051 -0.6755989 -0.0068539 0.6708510 1.9798202 8582.944 0.9998661 a_bm_norm[10] -0.4315799 0.0061151 0.2038895 -0.8404470 -0.5705033 -0.4306554 -0.2899537 -0.0433997 1111.691 1.0054807 a_bm_norm[11] -0.5259654 0.0062813 0.2320727 -1.0016897 -0.6782325 -0.5212058 -0.3696708 -0.0885248 1365.057 1.0039398 a_bm_norm[12] 0.9068971 0.0064833 0.2329520 0.4544769 0.7455659 0.9050496 1.0665512 1.3565692 1291.032 1.0053887 a_bm_norm[13] -0.4359458 0.0061185 0.2078811 -0.8463867 -0.5740035 -0.4343978 -0.2939291 -0.0382113 1154.354 1.0044905 a_bm_norm[14] -0.5611604 0.0062895 0.2186831 -0.9921228 -0.7077741 -0.5574438 -0.4103387 -0.1426650 1208.918 1.0037847 a_bm_norm[15] -0.6093862 0.0061928 0.2186447 -1.0563642 -0.7537575 -0.6074251 -0.4588930 -0.1877069 1246.542 1.0037959 a_bm_norm[16] -0.1300590 0.0059541 0.1982747 -0.5215199 -0.2645092 -0.1280031 0.0080854 0.2464275 1108.926 1.0054374 a_bm_norm[17] -0.8254504 0.0064065 0.2261639 -1.2866539 -0.9751358 -0.8233464 -0.6698349 -0.3955225 1246.253 1.0034022 a_bm_norm[18] 0.2086358 0.0058512 0.1968964 -0.1784470 0.0744810 0.2075393 0.3447130 0.5823112 1132.377 1.0058857 a_bm_norm[19] -0.4961383 0.0064632 0.3014570 -1.1212569 -0.6881725 -0.4841014 -0.2885630 0.0617583 2175.463 1.0012575 a_bm_norm[20] -0.4030878 0.0061632 0.2137075 -0.8352109 -0.5437513 -0.4023146 -0.2533124 -0.0035589 1202.323 1.0046415 a_bm_norm[21] -0.6078715 0.0062295 0.2106205 -1.0271393 -0.7478032 -0.6053876 -0.4642284 -0.2029004 1143.133 1.0047761 a_bm_norm[22] 0.8147772 0.0063431 0.2273031 0.3741683 0.6582662 0.8148677 0.9724791 1.2506644 1284.123 1.0051303 a_bm_norm[23] -0.5624268 0.0062140 0.2145921 -0.9935462 -0.7043190 -0.5604617 -0.4166895 -0.1503493 1192.559 1.0041953 a_bm_norm[24] 0.8505912 0.0064559 0.2302010 0.4029159 0.6899970 0.8482067 1.0106928 1.2894445 1271.473 1.0056251 a_bm_norm[25] -1.0437646 0.0067559 0.2449729 -1.5429579 -1.2032632 -1.0416170 -0.8773582 -0.5722703 1314.833 1.0035678 a_bm_norm[26] 2.1491961 0.0086831 0.3531701 1.4738920 1.9038174 2.1439157 2.3909719 2.8434322 1654.308 1.0033435 a_bm_norm[27] -0.4074135 0.0061642 0.2200510 -0.8398117 -0.5532337 -0.4047249 -0.2547040 0.0094274 1274.343 1.0043605 a_bm_norm[28] -0.8421851 0.0068159 0.4135339 -1.7664022 -1.0984919 -0.8014230 -0.5507585 -0.1228178 3681.056 1.0007621 a_bm_norm[29] 0.3769791 0.0060082 0.2086409 -0.0361264 0.2336479 0.3784831 0.5230282 0.7762157 1205.901 1.0061095 a_bm_norm[30] 0.8778486 0.0065097 0.2327422 0.4259823 0.7175528 0.8777734 1.0409110 1.3204945 1278.271 1.0052698 s 2.4787526 0.0078300 0.3477918 1.9115061 2.2297659 2.4443030 2.6820143 3.2530978 1972.951 1.0021061 lp__ -5484.5180011 0.1232301 5.6597193 -5496.8727218 -5488.1030994 -5484.1825342 -5480.4879169 -5474.5067482 2109.386 1.0011965 5.4 RQ4 Results and Plots 5.4.1 RQ4 Parameters and plots First lets get the HPDI of every parameter. Then we restrict to the algorithms, them to the slopes, then to the parameter s hpdi &lt;- get_HPDI_from_stanfit(timetoconverge_fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_noise&lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;b_noise\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_s &lt;- hpdi %&gt;% dplyr::filter(Parameter==&#39;s&#39;) p_alg&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;a_alg&quot;, x=&quot;Algorithm&quot;)+ coord_flip() p_alg + plot_annotation(title = &#39;HPDI interval for the algorithms&#39;) p_noise&lt;-ggplot(data=hpdi_noise, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;b_noise&quot;, x=&quot;Algorithm&quot;)+ coord_flip() p_noise + plot_annotation(title = &#39;HPDI interval for noise coefficient&#39;) p_s &lt;- ggplot(data=hpdi_s, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate of s&quot;, x=&quot;Parameter&quot;)+ coord_flip() p_s + plot_annotation(title = &#39;HPDI interval std of the benchmarks&#39;) 5.4.2 Hazard ratio hr_table &lt;- tibble( &quot;Algorithms&quot; = algorithms, &quot;Baseline HR&quot; = exp(hpdi_algorithm$Mean), &quot;Noise HR&quot; = exp(hpdi_noise$Mean)) kable(hr_table, booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Algorithms Baseline HR Noise HR CMAES 0.006 0.456 DifferentialEvolution 0.002 0.386 PSO 0.002 0.509 RandomSearch1 0.000 0.561 5.4.3 Iterations to Converge To obtain the average iteration to converge we first extract samples from the posterior distribution Looking at the average value of the data regardless of the benchmarks d_no_na &lt;- d %&gt;% drop_na(y) d_no_na %&gt;% group_by(Algorithm) %&gt;% summarise(Mean=mean(y)) # A tibble: 4 x 2 Algorithm Mean &lt;chr&gt; &lt;dbl&gt; 1 CMAES 115. 2 DifferentialEvolution 3340. 3 PSO 2406. 4 RandomSearch1 13951. posterior &lt;- rstan::extract(timetoconverge_fit) a &lt;- as_tibble(posterior$a_alg) colnames(a) &lt;- algorithms lambda &lt;- exp(a) mu &lt;- 1/lambda #Creating a HPD table timetoconverge_table&lt;-as_tibble(HDInterval::hdi(mu,credMass=0.95), rownames = &quot;Metric&quot;) %&gt;% tibble::add_row(Metric=&quot;Mean&quot;,CMAES=mean(mu$CMAES), DifferentialEvolution=mean(mu$DifferentialEvolution), PSO=mean(mu$PSO), RandomSearch1=mean(mu$RandomSearch1)) %&gt;% tidyr::pivot_longer(cols=-Metric,names_to = &#39;Algorithms&#39;, values_to=&#39;values&#39;) %&gt;% tidyr::pivot_wider(names_from =Metric , values_from=values) %&gt;% dplyr::rename(Mean=Mean, &#39;HPD low&#39; = lower, &#39;HPD high&#39; = upper) %&gt;% dplyr::relocate(Algorithms, Mean) saveRDS(timetoconverge_table, &#39;./statscomp-paper/tables/datafortables/averagetimetoconverge.RDS&#39;) 5.4.4 Merging hazards and time to converge table 5.4.5 Parameter table rename_pars &lt;- c(paste(rep(&#39;a_&#39;,length(algorithms)),algorithms, sep = &quot;&quot;), paste(rep(&#39;b_&#39;,length(algorithms)),algorithms, sep = &quot;&quot;),&#39;s&#39;) t&lt;-create_table_model(timetoconverge_fit, c(a_alg, b_noise, &#39;s&#39;), rename_pars) colnames(t)&lt;-c(&quot;Parameter&quot;, &quot;Mean&quot;, &quot;HPD low&quot;, &quot;HPD high&quot;) saveRDS(t,&#39;./statscomp-paper/tables/datafortables/timetoconverge-hr-par-table.RDS&#39;) "],["multiple-group-comparison.html", "Chapter 6 Multiple-group comparison 6.1 RQ5 Data preparation 6.2 RQ5 Stan model 6.3 RQ5 Diagnosis 6.4 RQ5 Results and Plots", " Chapter 6 Multiple-group comparison We present here the Stan version of the BEST (Bayesian Estimation Supersedes the t Test) from John K. Kruschke. We will consider the following research question RQ5: Is there a difference in the time taken per function evaluation between the PSO, the RandomSearch1 and the Differential Evolution algorithms? 6.1 RQ5 Data preparation We start importing the dataset dataset &lt;- readr::read_csv(&#39;./data/statscomp.csv&#39;) Filtering the data that we want and applying some transformations d &lt;- dataset %&gt;% dplyr::filter( OptimizationSuccessful==TRUE &amp; (Algorithm==&quot;PSO&quot; | Algorithm==&quot;RandomSearch1&quot; | Algorithm==&quot;DifferentialEvolution&quot;)) %&gt;% dplyr::select(Algorithm, CostFunction, TimeToComplete, simNumber, MaxFeval) %&gt;% dplyr::mutate(y=10000*TimeToComplete/MaxFeval, CostFunctionID=create_index(CostFunction), AlgorithmID=create_index(Algorithm)) %&gt;% dplyr::select(Algorithm, AlgorithmID, CostFunction, CostFunctionID, y,-simNumber, -MaxFeval) algorithms&lt;-get_index_names_as_array(d$Algorithm) bm &lt;- get_index_names_as_array(d$CostFunction) The data should look like this: kable(dplyr::sample_n(d,size=10), &quot;html&quot;,booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Algorithm AlgorithmID CostFunction CostFunctionID y DifferentialEvolution 1 WhitleyN6 28 2.950 DifferentialEvolution 1 ChenBird 2 1.813 PSO 2 Trefethen 25 0.541 DifferentialEvolution 1 WhitleyN6 28 2.957 RandomSearch1 3 DiscusN2 6 0.517 PSO 2 RosenbrockRotatedN6 14 1.747 RandomSearch1 3 Giunta 8 0.379 RandomSearch1 3 QingN2 13 0.190 RandomSearch1 3 Schwefel2d4N6 20 0.436 RandomSearch1 3 Damavandi 5 0.360 Some initial visualizations in terms of box-plots p1&lt;-ggplot(d) + geom_boxplot(aes(x=Algorithm, y=y))+ labs(y=&quot;Time to complete x10,000 (s)&quot;) p1 + plot_annotation(title =&quot;Box-plot of the runtime per function evaluation&quot;) lmfit &lt;- lm(y~Algorithm, data=d) p2&lt;-ggplot()+ geom_qq(aes(sample=lmfit$residuals))+ geom_qq_line(aes(sample=lmfit$residuals))+ labs(x=&quot;Standard normal quantiles&quot;, y=&quot;Sample quantiles&quot;) p2 + plot_annotation(title = &quot;Q-Q plot for normality analysis&quot;) Verifying that the log of the runtime still present high tailed distributions d2&lt;-d d2$y &lt;- log(d2$y) lmfit2 &lt;- lm(y~Algorithm, data=d2) p3&lt;-ggplot()+ geom_qq(aes(sample=lmfit2$residuals))+ geom_qq_line(aes(sample=lmfit2$residuals))+ labs(x=&quot;Standard normal quantiles&quot;, y=&quot;Sample quantiles&quot;) p2 + plot_annotation(title = &quot;Q-Q plot with the log of runtime&quot;) 6.2 RQ5 Stan model The Stan model is specified in the file: './stanmodels/multiplegroups.stan'. Note that at the end of the model we commented the generated quantities. This block generates the predictive posterior y_rep and the log likelihood, log_lik. These values are useful in diagnosing and validating the model but the end file is extremely large (~1Gb for 2000 iterations) and make many of the following calculations slow. If the reader wants to see these values is just to uncomment and run the stan model again. print_stan_code(&#39;./stanmodels/multiplegroups.stan&#39;) // Multiple group comparison // Author: David Issa Mattos // Date: 23 June 2020 // // data { int &lt;lower=1&gt; N_total; // Sample size real y[N_total]; // time to complete variable //To model each algorithm independently int &lt;lower=1&gt; N_algorithm; // Number of algorithms int algorithm_id[N_total]; //vector that has the id of each algorithm //To model the influence of each benchmark int &lt;lower=1&gt; N_bm; int bm_id[N_total]; } parameters { //Fixed effect real a_alg[N_algorithm];//the mean effect given by the algorithms real &lt;lower=0&gt; sigma[N_algorithm];//std for the student t // //Random effect. The effect of the benchmarks real a_bm_norm[N_bm];//the mean effect given by the base class type real&lt;lower=0&gt; s;//std for the random effects real&lt;lower=0&gt; nu;//std for the random effects } model { real mu[N_total]; real sigma_i[N_total]; sigma ~ exponential(1); nu ~ exponential(1.0/30.0); //Fixed effect a_alg ~ normal(0,1); // //Random effects s ~ exponential(1); a_bm_norm ~ normal(0,1); for (i in 1:N_total) { mu[i] = a_alg[algorithm_id[i]] + a_bm_norm[bm_id[i]]*s; sigma_i[i] = sigma[algorithm_id[i]]; } y ~ student_t(nu, mu, sigma_i); } //Uncoment this part to get the posterior predictives and the log likelihood //But note that it takes a lot of space in the final model // generated quantities{ // vector [N_total] y_rep; // vector[N_total] log_lik; // for (i in 1:N_total){ // real mu; // real sigma_i; // mu = a_alg[algorithm_id[i]] + a_bm_norm[bm_id[i]]*s; // sigma_i = sigma[algorithm_id[i]]; // y_rep[i] = student_t_rng(nu,mu,sigma_i); // // //Log likelihood // log_lik[i] = student_t_lpdf(y[i] | nu,mu,sigma_i); // // } // } Let’s compile and start sampling with the Stan function. In the data folder you can find the specific data used to fit the model after all transformations \"./data/multiplegroup-data.RDS\" standata &lt;- list( N_total=nrow(d), y = d$y, N_algorithm = length(algorithms), algorithm_id = d$AlgorithmID, N_bm = length(bm), bm_id = d$CostFunctionID ) saveRDS(standata, file = &quot;./data/multiplegroups-data.RDS&quot;) For computation time sake we are not running this chunk every time we compile this document. From now on we will load from the saved Stan fit object. However, when we change our model or the data we can just run this chunk separately. Here we increased the maxtreedepth and the number of iterations so we have a higher effective sample for inference. Both of these do not impact the validity of the chain just the computation efficiency. standata&lt;-readRDS(&quot;./data/multiplegroups-data.RDS&quot;) multiplegroup_fit &lt;- stan(file = &#39;./stanmodels/multiplegroups.stan&#39;, data=standata, chains = 4, warmup = 400, iter = 4000, control = list(max_treedepth = 15)) saveRDS(multiplegroup_fit, file = &quot;./data/multiplegroups-fit.RDS&quot;) 6.3 RQ5 Diagnosis a_alg_v &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;) sigma_v &lt;- c(&quot;sigma[1]&quot;, &quot;sigma[2]&quot;, &quot;sigma[3]&quot;) rstan::traceplot(multiplegroup_fit, pars=a_alg_v) rstan::traceplot(multiplegroup_fit, pars=sigma_v) rstan::traceplot(multiplegroup_fit, pars=c(&#39;s&#39;, &#39;nu&#39;)) Another diagnosis is to look at the Rhat. If Rhat is greater than 1.05 it indicates a divergence in the chains (they did not mix well). The table below shows a summary of the sampling. kable(summary(multiplegroup_fit)$summary, &quot;html&quot;,) %&gt;% kable_styling(bootstrap_options = c(&#39;striped&#39;,&quot;hover&quot;, &quot;condensed&quot; )) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] 1.7811268 0.0026230 0.0606078 1.6619826 1.7403893 1.7820330 1.8215807 1.8982458 533.8817 1.006126 a_alg[2] 0.5691139 0.0026209 0.0605588 0.4496716 0.5283954 0.5700587 0.6093643 0.6855433 533.8829 1.006094 a_alg[3] 0.4423529 0.0026208 0.0605582 0.3233317 0.4013482 0.4432400 0.4826083 0.5580626 533.9264 1.006091 sigma[1] 0.0877444 0.0000318 0.0017455 0.0843878 0.0865374 0.0877212 0.0889341 0.0911437 3011.1145 1.001623 sigma[2] 0.0694441 0.0000277 0.0015140 0.0664710 0.0684330 0.0694499 0.0704463 0.0724822 2976.8074 1.000836 sigma[3] 0.0368688 0.0000144 0.0008039 0.0352959 0.0363257 0.0368479 0.0374083 0.0384708 3121.5824 1.000719 a_bm_norm[1] 0.7169454 0.0087107 0.2135654 0.2988931 0.5710651 0.7142151 0.8674880 1.1328629 601.1139 1.008025 a_bm_norm[2] -0.1824269 0.0085491 0.1988784 -0.5714764 -0.3176881 -0.1827604 -0.0451221 0.1940826 541.1669 1.005750 a_bm_norm[3] -0.5830931 0.0091159 0.2163121 -1.0160424 -0.7275807 -0.5801526 -0.4404438 -0.1643768 563.0722 1.003927 a_bm_norm[4] -0.6091954 0.0091666 0.2178293 -1.0467759 -0.7534750 -0.6062762 -0.4656718 -0.1855039 564.6959 1.003882 a_bm_norm[5] -0.2485564 0.0086165 0.2008707 -0.6432752 -0.3856466 -0.2485221 -0.1111202 0.1316417 543.4600 1.005561 a_bm_norm[6] 0.4734111 0.0084824 0.2024756 0.0780666 0.3334823 0.4720095 0.6152515 0.8568556 569.7825 1.007887 a_bm_norm[7] -0.6423130 0.0092312 0.2198717 -1.0846795 -0.7887968 -0.6386941 -0.4972334 -0.2139691 567.3092 1.003799 a_bm_norm[8] -0.1644817 0.0085326 0.1985217 -0.5517190 -0.2996106 -0.1657384 -0.0275923 0.2129775 541.3154 1.005963 a_bm_norm[9] 2.1116300 0.0121812 0.3393819 1.4484849 1.8816608 2.1092589 2.3358436 2.7967160 776.2394 1.005745 a_bm_norm[10] -0.1812781 0.0085416 0.1989285 -0.5714155 -0.3176169 -0.1816472 -0.0444326 0.1975518 542.3993 1.005821 a_bm_norm[11] 0.9019238 0.0089863 0.2249558 0.4576177 0.7501431 0.8991386 1.0577316 1.3417398 626.6611 1.007809 a_bm_norm[12] -0.7818005 0.0095262 0.2292960 -1.2427092 -0.9348107 -0.7782093 -0.6287008 -0.3328971 579.3611 1.003212 a_bm_norm[13] -0.6775106 0.0092827 0.2220651 -1.1227791 -0.8255683 -0.6734795 -0.5296425 -0.2434466 572.2849 1.003527 a_bm_norm[14] 1.7137941 0.0109291 0.2967903 1.1337453 1.5123317 1.7108835 1.9123530 2.3102016 737.4474 1.006425 a_bm_norm[15] -0.2518458 0.0086224 0.2009569 -0.6477380 -0.3884656 -0.2520031 -0.1136934 0.1301903 543.1896 1.005519 a_bm_norm[16] -0.6890408 0.0093176 0.2228297 -1.1395233 -0.8382348 -0.6855899 -0.5409571 -0.2539657 571.9248 1.003543 a_bm_norm[17] -0.2997380 0.0086702 0.2027220 -0.6996320 -0.4386068 -0.2983614 -0.1626570 0.0871212 546.6910 1.005343 a_bm_norm[18] -0.2065718 0.0085791 0.1997227 -0.5978602 -0.3420286 -0.2057953 -0.0687583 0.1743771 541.9588 1.005684 a_bm_norm[19] -0.1781351 0.0085411 0.1988323 -0.5689114 -0.3141829 -0.1783365 -0.0407893 0.1991033 541.9402 1.005823 a_bm_norm[20] 0.0013479 0.0084299 0.1959243 -0.3801857 -0.1321950 -0.0011718 0.1384331 0.3748182 540.1669 1.006703 a_bm_norm[21] -0.0381568 0.0084334 0.1961037 -0.4215247 -0.1727083 -0.0400992 0.0998043 0.3345669 540.7160 1.006430 a_bm_norm[22] -0.2591012 0.0086473 0.2014404 -0.6537203 -0.3957154 -0.2589614 -0.1220667 0.1262415 542.6678 1.005472 a_bm_norm[23] -0.7358550 0.0094254 0.2260798 -1.1892931 -0.8858307 -0.7320868 -0.5846833 -0.2939862 575.3385 1.003330 a_bm_norm[24] -0.8254234 0.0096072 0.2324392 -1.2935835 -0.9805076 -0.8213645 -0.6689679 -0.3684101 585.3639 1.003067 a_bm_norm[25] -0.3366898 0.0087284 0.2039811 -0.7369234 -0.4748569 -0.3359966 -0.1992994 0.0536531 546.1521 1.005150 a_bm_norm[26] 0.3488281 0.0084028 0.1985740 -0.0350128 0.2120805 0.3487199 0.4879435 0.7238281 558.4606 1.007803 a_bm_norm[27] -0.7346498 0.0094058 0.2258222 -1.1905483 -0.8847851 -0.7302798 -0.5839647 -0.2927947 576.4249 1.003359 a_bm_norm[28] 3.6998131 0.0182556 0.5324392 2.6893552 3.3338793 3.6905820 4.0513884 4.7696181 850.6446 1.004008 a_bm_norm[29] -0.4143554 0.0088376 0.2074474 -0.8239239 -0.5542043 -0.4118008 -0.2764703 -0.0164877 550.9920 1.004714 a_bm_norm[30] -0.1770084 0.0085576 0.1989495 -0.5672901 -0.3128015 -0.1772755 -0.0397254 0.2004904 540.4818 1.005940 s 0.3072466 0.0014798 0.0441124 0.2352709 0.2768578 0.3021747 0.3322788 0.4069552 888.6323 1.001588 nu 2.7532026 0.0015421 0.0802559 2.6021136 2.6972495 2.7520588 2.8075206 2.9112477 2708.6351 1.001055 lp__ 14070.2753067 0.1671716 5.9172571 14057.6082046 14066.4863632 14070.6208015 14074.4422447 14080.7870357 1252.8982 1.000465 6.4 RQ5 Results and Plots First lets get the HPDI of every parameter. Then we restrict to the algorithms, them to the slopes, then to the parameter s hpdi &lt;- get_HPDI_from_stanfit(multiplegroup_fit) hpdi_algorithm &lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;a_alg\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_sigma&lt;- hpdi %&gt;% dplyr::filter(str_detect(Parameter, &quot;sigma\\\\[&quot;)) %&gt;% dplyr::mutate(Parameter=algorithms) #Changing to the algorithms labels hpdi_s &lt;- hpdi %&gt;% dplyr::filter(Parameter==&#39;s&#39;) hpdi_nu &lt;- hpdi %&gt;% dplyr::filter(Parameter==&#39;nu&#39;) hpdi_nu_s &lt;- hpdi %&gt;% dplyr::filter(Parameter==&#39;nu&#39; | Parameter==&#39;s&#39;) p_alg&lt;-ggplot(data=hpdi_algorithm, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;a_alg&quot;, x=&quot;Algorithm&quot;)+ theme(axis.title.x= element_blank())+ coord_flip() p_alg + plot_annotation(title = &#39;HPDI interval for the algorithms&#39;) p_sigma&lt;-ggplot(data=hpdi_sigma, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;sigma&quot;, x=&quot;Algorithm&quot;)+ theme(axis.title.x= element_blank())+ coord_flip() p_sigma + plot_annotation(title = &#39;HPDI interval for sigma&#39;) p_s &lt;- ggplot(data=hpdi_s, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;s&quot;, x=&quot;Parameter&quot;)+ coord_flip() p_s + plot_annotation(title = &#39;HPDI interval std of the benchmarks&#39;) p_nu &lt;- ggplot(data=hpdi_nu, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;nu&quot;, x=&quot;Parameter&quot;)+ coord_flip() p_nu + plot_annotation(title = &#39;HPDI interval of the degree of freedom&#39;) p_nu_s &lt;- ggplot(data=hpdi_nu_s, aes(x=Parameter))+ geom_pointrange(aes( ymin=HPDI.lower, ymax=HPDI.higher, y=Mean))+ labs(y=&quot;Estimate of s and nu&quot;, x=&quot;Parameter&quot;)+ theme(axis.title.x= element_blank())+ coord_flip() p_nu_s + plot_annotation(title = &#39;HPDI interval&#39;) Now lets get a posterior distribution of the difference posterior &lt;- rstan::extract(multiplegroup_fit) a_alg &lt;- as_tibble(posterior$a_alg) colnames(a_alg) &lt;- algorithms sample_a_alg &lt;- dplyr::sample_n(a_alg, size=1000, replace=T) %&gt;% dplyr::mutate(PSO_Random = PSO-RandomSearch1, DE_PSO= DifferentialEvolution-PSO, DE_Random = DifferentialEvolution-RandomSearch1) %&gt;% dplyr::select(-DifferentialEvolution,-PSO,-RandomSearch1) #Getting HPDI from a data frame and creating a table instead of plotting... hpdi_diff&lt;-HDInterval::hdi(sample_a_alg,credMass=0.95) hpdi_diff&lt;-hpdi_diff %&gt;% as_tibble(rownames = &quot;Metric&quot;) %&gt;% tibble::add_row(Metric=&quot;Mean&quot;, PSO_Random=mean(sample_a_alg$PSO_Random), DE_PSO=mean(sample_a_alg$DE_PSO), DE_Random=mean(sample_a_alg$DE_Random)) %&gt;% tidyr::pivot_longer(cols=-Metric, names_to=&quot;AlgorithmDifference&quot;, values_to=&#39;values&#39;) %&gt;% tidyr::pivot_wider(names_from =Metric , values_from=values) %&gt;% dplyr::mutate(Difference=c(&#39;PSO - RandomSearch&#39;, &#39;DiffEvolution - PSO&#39;, &#39;DiffEvolution - RandomSearch&#39;)) %&gt;% dplyr::select(Difference, Lower=lower, Mean, Upper=upper) kable(hpdi_diff, booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) Difference Lower Mean Upper PSO - RandomSearch 0.123 0.127 0.130 DiffEvolution - PSO 1.207 1.212 1.217 DiffEvolution - RandomSearch 1.334 1.339 1.343 Creating an output table rename_pars &lt;- c( paste(rep(&#39;a_&#39;,length(algorithms)),algorithms, sep = &quot;&quot;), paste(rep(&#39;sigma_&#39;,length(algorithms)),algorithms, sep = &quot;&quot;), &#39;s&#39;, &#39;nu&#39;) t&lt;-create_table_model(multiplegroup_fit, pars=c(a_alg_v, sigma_v, &#39;s&#39;,&#39;nu&#39;),rename_pars) colnames(t)&lt;-c(&quot;Parameter&quot;, &quot;Mean&quot;, &quot;HPD low&quot;, &quot;HPD high&quot;) saveRDS(t,&#39;./statscomp-paper/tables/datafortables/multiplegroupsdifference-par-table.RDS&#39;) "],["sensitivity-analysis-model-comparison-and-posterior-predictive.html", "Chapter 7 Sensitivity analysis, model comparison and posterior predictive 7.1 Compare models with and without clustering 7.2 Sensitivity analysis of priors 7.3 Posterior predictive plots", " Chapter 7 Sensitivity analysis, model comparison and posterior predictive In this chapter, we provide an example on how to do a sensitivity analysis and model comparison. For that we will use the Relative Improvement model of chapter 3. Note that it is important to have a model that calculates the log likelihood to compute the WAIC or the LOO-CV. Here we will show only how to use and interpret the WAIC. Reading the data for the (already prepared data) for the model standata&lt;-readRDS(&quot;./data/relativeimprovement-data.RDS&quot;) algorithms&lt;-readRDS(&quot;./data/relativeimprovement_algorithms.RDS&quot;) bm&lt;-readRDS(&quot;./data/relativeimprovement_bm.RDS&quot;) Here we consider 3 models for the relative improvement. The original model presented on chapter 3 and in the paper. A model without clustering information about the benchmarks (m1) and a model with a different set of priors (m2) In m2 we consider HalfNormal(0,5) for both the s and the sigma parameters (instead of the exponential) We are saving the files in a non-tracked folder because they are too big for github relativeimprovement_fit_original &lt;- stan(file = &#39;./stanmodels/relativeimprovement-original.stan&#39;, data=standata, chains = 4, warmup = 200, iter = 1000) saveRDS(relativeimprovement_fit_original, file = &quot;./data/gitignore/relativeimprovement_fit_original.RDS&quot;) standata_m1 = list( N_total= standata$N_total, y = standata$y, N_algorithm = standata$N_algorithm, algorithm_id = standata$algorithm_id ) relativeimprovement_fit_m1 &lt;- stan(file = &#39;./stanmodels/relativeimprovement-m1.stan&#39;, data=standata_m1, chains = 4, warmup = 200, iter = 1000) saveRDS(relativeimprovement_fit_m1, file = &quot;./data/gitignore/relativeimprovement_fit_m1.RDS&quot;) standata_m2 &lt;- standata relativeimprovement_fit_m2 &lt;- stan(file = &#39;./stanmodels/relativeimprovement-m2.stan&#39;, data=standata, chains = 4, warmup = 200, iter = 1000) saveRDS(relativeimprovement_fit_m2, file = &quot;./data/gitignore/relativeimprovement_fit_m2.RDS&quot;) 7.1 Compare models with and without clustering First we get the log likelihood log_lik_original &lt;- loo::extract_log_lik(relativeimprovement_fit_original, merge_chains = T) log_lik_m1 &lt;- loo::extract_log_lik(relativeimprovement_fit_m1 ,merge_chains = T) Then we compute the waic waic_original&lt;-loo::waic(log_lik_original) waic_m1&lt;-loo::waic(log_lik_m1) Now we use the compare function print(waic_original) Computed from 3200 by 9000 log-likelihood matrix Estimate SE elpd_waic -8723.2 53.7 p_waic 33.7 0.4 waic 17446.3 107.4 print(waic_m1) Computed from 3200 by 9000 log-likelihood matrix Estimate SE elpd_waic -8913.5 52.2 p_waic 6.6 0.1 waic 17827.0 104.4 loo::loo_compare(waic_original, waic_m1) elpd_diff se_diff model1 0.0 0.0 model2 -190.3 20.1 We can see that the WAIC original (with clustering) provides a big improvement over m1 (without clustering). 7.2 Sensitivity analysis of priors log_lik_m2 &lt;- loo::extract_log_lik(relativeimprovement_fit_m2, merge_chains = T) First let’s look at the WAIC waic_m2&lt;-loo::waic(log_lik_m2) print(waic_m2) Computed from 3200 by 9000 log-likelihood matrix Estimate SE elpd_waic -8723.6 53.7 p_waic 34.1 0.4 waic 17447.2 107.5 Comparing the models loo::loo_compare(waic_original, waic_m2) elpd_diff se_diff model1 0.0 0.0 model2 -0.4 0.1 We can see here that there is no significant difference between the models with the two priors. This already indicates some robustness in the estimation parameters regardless of the priors (which is expected since both are weakly informative priors). Comparing the estimates for the intercepts of the algorithms only. Note that since we have a very big stanfit the summary calculations might take a bit longer. a_alg &lt;- c(&quot;a_alg[1]&quot;, &quot;a_alg[2]&quot;, &quot;a_alg[3]&quot;, &quot;a_alg[4]&quot;, &quot;a_alg[5]&quot;, &quot;a_alg[6]&quot;) df_original&lt;-summary(relativeimprovement_fit_original, pars = a_alg)$summary kable(df_original, &quot;html&quot;,booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] 0.154 0.001 0.032 0.090 0.132 0.154 0.175 0.215 470.933 1.007 a_alg[2] -0.374 0.002 0.032 -0.437 -0.396 -0.374 -0.353 -0.312 453.902 1.005 a_alg[3] 0.306 0.002 0.032 0.244 0.284 0.306 0.328 0.368 459.828 1.005 a_alg[4] -0.638 0.001 0.032 -0.700 -0.659 -0.638 -0.617 -0.575 465.164 1.004 a_alg[5] 0.323 0.001 0.032 0.258 0.301 0.324 0.345 0.385 472.724 1.004 a_alg[6] -0.567 0.001 0.032 -0.630 -0.589 -0.567 -0.545 -0.506 472.289 1.005 df_m2&lt;-summary(relativeimprovement_fit_m2, pars = a_alg)$summary kable(df_m2, &quot;html&quot;,booktabs=T, format.args = list(scientific = FALSE), digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) %&gt;% kableExtra::scroll_box(width = &quot;100%&quot;) mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat a_alg[1] 0.148 0.002 0.031 0.083 0.129 0.148 0.169 0.209 423.994 1.016 a_alg[2] -0.380 0.001 0.031 -0.445 -0.400 -0.380 -0.359 -0.322 426.036 1.015 a_alg[3] 0.300 0.002 0.031 0.236 0.280 0.302 0.321 0.360 428.207 1.016 a_alg[4] -0.644 0.002 0.031 -0.709 -0.664 -0.643 -0.622 -0.585 431.340 1.012 a_alg[5] 0.318 0.002 0.031 0.253 0.298 0.319 0.339 0.378 422.176 1.015 a_alg[6] -0.573 0.002 0.031 -0.638 -0.593 -0.573 -0.552 -0.514 417.598 1.012 We can see from both tables that the estimates of the algorithms intercepts are very similar, which starts to indicate a certain robustness of the model in respect to the priors. 7.3 Posterior predictive plots To check for the posterior predictive we will use the original model. First we extract the posterior of the predictive values. We have in this posterior 3200 rows (800 iterations for every chain) and 9000 columns (1 for each point in the dataset). Lets start by resampling to get only 100 estimates for each observation. Then we will create a data frame that has a column for each type of observation. Then we will pivot longer so the 100 observations go to a single column. This will multiply the dataset number of rows by 100 . This will facilitate plotting with ggplot y_rep_posterior &lt;- as_tibble(rstan::extract(relativeimprovement_fit_original, pars=&#39;y_rep&#39;)$y_rep) y_rep &lt;- as_tibble(t(sample_n(y_rep_posterior, size=100))) y &lt;- as_tibble(standata$y) %&gt;% select(y_obs=value) algo&lt;-as_tibble(standata$algorithm_id) %&gt;% select(algo=value) algo$algo&lt;-dplyr::recode(algo$algo, &#39;1&#39;=algorithms[1], &#39;2&#39;=algorithms[2], &#39;3&#39;=algorithms[3], &#39;4&#39;=algorithms[4], &#39;5&#39;=algorithms[5], &#39;6&#39;=algorithms[6]) benchmark &lt;- as_tibble(standata$bm_id) %&gt;% select(benchmark=value) benchmark$benchmark&lt;-dplyr::recode(benchmark$benchmark, &#39;1&#39;=bm[1], &#39;2&#39;=bm[2], &#39;3&#39;=bm[3], &#39;4&#39;=bm[4], &#39;5&#39;=bm[5], &#39;6&#39;=bm[6], &#39;7&#39;=bm[7], &#39;8&#39;=bm[8], &#39;9&#39;=bm[9], &#39;10&#39;=bm[10], &#39;11&#39;=bm[11], &#39;12&#39;=bm[12], &#39;13&#39;=bm[13], &#39;14&#39;=bm[14], &#39;15&#39;=bm[15], &#39;16&#39;=bm[16], &#39;17&#39;=bm[17], &#39;18&#39;=bm[18], &#39;19&#39;=bm[19], &#39;20&#39;=bm[20], &#39;21&#39;=bm[21], &#39;22&#39;=bm[22], &#39;23&#39;=bm[23], &#39;24&#39;=bm[24], &#39;25&#39;=bm[25], &#39;26&#39;=bm[26], &#39;27&#39;=bm[27], &#39;28&#39;=bm[28], &#39;29&#39;=bm[29], &#39;30&#39;=bm[30] ) df &lt;- algo %&gt;% add_column(benchmark) %&gt;% add_column(y) %&gt;% add_column(y_rep) %&gt;% tidyr::pivot_longer(cols=4:103,names_to = &#39;sample&#39;, values_to=&#39;y_rep&#39;) There are multiple ways to plot predictive posterior. One of them is with a histogram plot of the predictions, or lines for th intercept etc.. Here we plot the histogram of each benchmark function for the PSO algorithm. Note that the model predicts better for some benchmark functions and not so well for others, but in average all the observed values are inside the histogram of the predictions ggplot(data=dplyr::filter(df, algo==&#39;PSO&#39;))+ geom_histogram(aes(x=y_rep), fill=&#39;black&#39;, alpha=0.8)+ geom_histogram(aes(x=y_obs), fill=&#39;blue&#39;, alpha=0.8)+ facet_wrap(~benchmark)+ labs(title=&#39;Predictive posterior for the PSO&#39;) "]]
